{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818ecc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "\n",
    "# -- Pipeline functions --\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the components folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../components\"))\n",
    "from pipeline import Pipeline\n",
    "\n",
    "# tqdm \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17be56",
   "metadata": {},
   "source": [
    "The following code is meant to push our data sources into an SQL database. I opted to use Jupyter Notebooks for the convenience of being able to retrieve data from an SQL database immediately after pushing data for QA checks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e405e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ..\\metadata\\tabulated_skills.json has been loaded into the pipeline.\n"
     ]
    }
   ],
   "source": [
    "# We init a new pipeline obj per iteration of calls\n",
    "pipeline = Pipeline()\n",
    "pipeline.get_tabulated_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6b7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.unique_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1c04f",
   "metadata": {},
   "source": [
    "## Pipeline Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6429d",
   "metadata": {},
   "source": [
    "### Salary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3409bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Salary extraction testing\n",
    "\n",
    "# No salary provided in the job description\n",
    "_testData = \"We offer 2 weeks vacation, with $1000 of medical insurance. We require 2-3 years of working experience, and upon hiring; working 2-5 days in office of 40 working hours (minimum) per week.\"\n",
    "_testOut = pipeline.extract_salary_from_job_desc(_testData)\n",
    "_testAvg = pipeline.create_avg_salary(_testOut)\n",
    "\n",
    "print(_testOut)\n",
    "print(_testAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802d97ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "70000.0\n"
     ]
    }
   ],
   "source": [
    "# Salary extraction testing\n",
    "\n",
    "# No salary provided in the job description\n",
    "_testData = \"Salary is $70,000 We offer 2 weeks vacation, with $1000 of medical insurance. We require 2-3 years of working experience, and upon hiring; working 2-5 days in office of 40 working hours (minimum) per week.\"\n",
    "_testOut = pipeline.extract_salary_from_job_desc(_testData)\n",
    "_testAvg = pipeline.create_avg_salary(_testOut)\n",
    "\n",
    "print(_testOut)\n",
    "print(_testAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e97b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000,90000\n",
      "80000.0\n"
     ]
    }
   ],
   "source": [
    "# Salary extraction testing\n",
    "\n",
    "# No salary provided in the job description\n",
    "_testData = \"Salary is $70,000 to $90,000 We offer 2 weeks vacation, with $1000 of medical insurance. We require 2-3 years of working experience, and upon hiring; working 2-5 days in office of 40 working hours (minimum) per week.\"\n",
    "_testOut = pipeline.extract_salary_from_job_desc(_testData)\n",
    "_testAvg = pipeline.create_avg_salary(_testOut)\n",
    "\n",
    "print(_testOut)\n",
    "print(_testAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c1310",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d6552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7945952, -106.5348379\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.get_coordinates('USA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185ae768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.get_coordinates('Anywhere'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d412ad",
   "metadata": {},
   "source": [
    "## AiJobs.net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317cd44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125705 entries, 0 to 125704\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   work_year           125705 non-null  int64 \n",
      " 1   experience_level    125705 non-null  object\n",
      " 2   employment_type     125705 non-null  object\n",
      " 3   job_title           125705 non-null  object\n",
      " 4   salary              125705 non-null  int64 \n",
      " 5   salary_currency     125705 non-null  object\n",
      " 6   salary_in_usd       125705 non-null  int64 \n",
      " 7   employee_residence  125705 non-null  object\n",
      " 8   remote_ratio        125705 non-null  int64 \n",
      " 9   company_location    125705 non-null  object\n",
      " 10  company_size        125705 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 10.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>405000</td>\n",
       "      <td>USD</td>\n",
       "      <td>405000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>255000</td>\n",
       "      <td>USD</td>\n",
       "      <td>255000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>306000</td>\n",
       "      <td>USD</td>\n",
       "      <td>306000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>191000</td>\n",
       "      <td>USD</td>\n",
       "      <td>191000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>175000</td>\n",
       "      <td>USD</td>\n",
       "      <td>175000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type          job_title  salary  \\\n",
       "0       2025               SE              FT           Engineer  405000   \n",
       "1       2025               SE              FT           Engineer  255000   \n",
       "2       2025               SE              FT  Software Engineer  306000   \n",
       "3       2025               SE              FT  Software Engineer  191000   \n",
       "4       2025               MI              FT        AI Engineer  175000   \n",
       "\n",
       "  salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0             USD         405000                 US             0   \n",
       "1             USD         255000                 US             0   \n",
       "2             USD         306000                 US             0   \n",
       "3             USD         191000                 US             0   \n",
       "4             USD         175000                 US           100   \n",
       "\n",
       "  company_location company_size  \n",
       "0               US            M  \n",
       "1               US            M  \n",
       "2               US            M  \n",
       "3               US            M  \n",
       "4               US            M  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aijobs = pd.read_csv(\"../../datasets/aijobs.net/salaries.csv\")\n",
    "aijobs.info()\n",
    "aijobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0488b",
   "metadata": {},
   "source": [
    "## Data Analyst Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99ae1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12894 entries, 0 to 12893\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   job_title       12894 non-null  object\n",
      " 1   company         12894 non-null  object\n",
      " 2   job_location    12894 non-null  object\n",
      " 3   job_link        12894 non-null  object\n",
      " 4   first_seen      12894 non-null  object\n",
      " 5   search_city     12894 non-null  object\n",
      " 6   search_country  12894 non-null  object\n",
      " 7   job level       12894 non-null  object\n",
      " 8   job_type        12894 non-null  object\n",
      " 9   job_summary     12851 non-null  object\n",
      " 10  job_skills      12705 non-null  object\n",
      "dtypes: object(11)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst-SQL, Tableau</td>\n",
       "      <td>Zortech Solutions</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>https://www.linkedin.com/jobs/data-analyst-jobs</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market Research &amp; Insights Analyst</td>\n",
       "      <td>Indiana University Foundation</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/market-rese...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Company Description\\nAre you a high-performer ...</td>\n",
       "      <td>Data analysis, Market research, Survey develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Systems Analyst `1</td>\n",
       "      <td>Cook Medical</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/business-sy...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Overview\\nThe Business Systems Analyst 1 perfo...</td>\n",
       "      <td>Business Analysis, Technical Writing, Software...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior VAT and Indirect Tax Analyst</td>\n",
       "      <td>Epic</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-vat-...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>We're looking for an experienced tax professio...</td>\n",
       "      <td>Accounting, Finance, VAT/GST tax regimes, US a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior HRIS Analyst (Timekeeping and Payroll)</td>\n",
       "      <td>Nordson Corporation</td>\n",
       "      <td>Greater Bloomington Area</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-hris...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Collaboration drives Nordson’s success as a ma...</td>\n",
       "      <td>Workday HCM, UKG Dimensions, Ceridian Dayforce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       job_title  \\\n",
       "0                      Data Analyst-SQL, Tableau   \n",
       "1             Market Research & Insights Analyst   \n",
       "2                    Business Systems Analyst `1   \n",
       "3            Senior VAT and Indirect Tax Analyst   \n",
       "4  Senior HRIS Analyst (Timekeeping and Payroll)   \n",
       "\n",
       "                         company              job_location  \\\n",
       "0              Zortech Solutions         Mountain View, CA   \n",
       "1  Indiana University Foundation           Bloomington, IN   \n",
       "2                   Cook Medical           Bloomington, IN   \n",
       "3                           Epic           Bloomington, IN   \n",
       "4            Nordson Corporation  Greater Bloomington Area   \n",
       "\n",
       "                                            job_link  first_seen  search_city  \\\n",
       "0    https://www.linkedin.com/jobs/data-analyst-jobs  2023-12-20  Bloomington   \n",
       "1  https://www.linkedin.com/jobs/view/market-rese...  2023-12-20  Bloomington   \n",
       "2  https://www.linkedin.com/jobs/view/business-sy...  2023-12-20  Bloomington   \n",
       "3  https://www.linkedin.com/jobs/view/senior-vat-...  2023-12-20  Bloomington   \n",
       "4  https://www.linkedin.com/jobs/view/senior-hris...  2023-12-20  Bloomington   \n",
       "\n",
       "  search_country   job level job_type  \\\n",
       "0  United States   Associate   Onsite   \n",
       "1  United States  Mid senior   Onsite   \n",
       "2  United States  Mid senior   Onsite   \n",
       "3  United States  Mid senior   Onsite   \n",
       "4  United States  Mid senior   Remote   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0                                                NaN   \n",
       "1  Company Description\\nAre you a high-performer ...   \n",
       "2  Overview\\nThe Business Systems Analyst 1 perfo...   \n",
       "3  We're looking for an experienced tax professio...   \n",
       "4  Collaboration drives Nordson’s success as a ma...   \n",
       "\n",
       "                                          job_skills  \n",
       "0                                                NaN  \n",
       "1  Data analysis, Market research, Survey develop...  \n",
       "2  Business Analysis, Technical Writing, Software...  \n",
       "3  Accounting, Finance, VAT/GST tax regimes, US a...  \n",
       "4  Workday HCM, UKG Dimensions, Ceridian Dayforce...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_postings = pd.read_csv('../../datasets/kaggle_asanickza/Data Analyst Job Postings/postings.csv')\n",
    "analyst_postings.info()\n",
    "analyst_postings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e70dd",
   "metadata": {},
   "source": [
    "### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ae3ff",
   "metadata": {},
   "source": [
    "We have to split up column `job_skills`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba531e62",
   "metadata": {},
   "source": [
    "This script confirms that all nan values are parsed through pandas as a float object, which should be skipped when the script formats the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in analyst_postings.itertuples():\n",
    "    if (isinstance(row[11],float)):\n",
    "        print(f\"For {row[11]}, it is considered a null value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14183ce6",
   "metadata": {},
   "source": [
    "Now, go through the values of `job_skills`, which will go through and grab the unique values within the values of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b988d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data analysis': 1,\n",
       " 'market research': 2,\n",
       " 'survey development': 3,\n",
       " 'analytical methods': 4,\n",
       " 'quantitative initiatives': 5,\n",
       " 'qualitative initiatives': 6,\n",
       " 'business solutions': 7,\n",
       " 'project management': 8,\n",
       " 'communication skills': 9,\n",
       " 'customer service': 10,\n",
       " 'statistical analysis': 11,\n",
       " 'data visualization': 12,\n",
       " 'data storytelling': 13,\n",
       " 'stakeholder engagement': 14,\n",
       " 'windows': 15,\n",
       " 'excel': 16,\n",
       " 'word': 17,\n",
       " 'powerpoint': 18,\n",
       " 'market research software': 19,\n",
       " 'marketing': 20,\n",
       " 'analytics': 21,\n",
       " 'psychology': 22,\n",
       " 'data science': 23,\n",
       " 'business analysis': 24,\n",
       " 'technical writing': 25,\n",
       " 'software testing': 26,\n",
       " 'systems development': 27,\n",
       " 'process improvement': 28,\n",
       " 'sdlc': 29,\n",
       " 'web technologies': 30,\n",
       " 'jbase': 31,\n",
       " 'mobile technologies': 32,\n",
       " 'relational databases': 33,\n",
       " 'agile': 34,\n",
       " 'waterfall': 35,\n",
       " 'accounting': 36,\n",
       " 'finance': 37,\n",
       " 'vat/gst tax regimes': 38,\n",
       " 'us and foreign tax laws': 39,\n",
       " 'vat and sales tax issues': 40,\n",
       " 'indirect tax': 41,\n",
       " 'sales and use tax': 42,\n",
       " 'excise tax': 43,\n",
       " 'tax compliance': 44,\n",
       " 'tax audits': 45,\n",
       " 'tax inquiries': 46,\n",
       " 'tax preparation': 47,\n",
       " 'tax analysis': 48,\n",
       " 'tax reporting': 49,\n",
       " 'tax regulations': 50,\n",
       " 'tax codes': 51,\n",
       " 'tax policies': 52,\n",
       " 'tax laws': 53,\n",
       " 'tax treaties': 54,\n",
       " 'tax planning': 55,\n",
       " 'tax accounting': 56,\n",
       " 'tax software': 57,\n",
       " 'tax research': 58,\n",
       " 'tax consulting': 59,\n",
       " 'tax management': 60,\n",
       " 'tax optimization': 61,\n",
       " 'tax minimization': 62,\n",
       " 'erp systems': 63,\n",
       " 'internal auditing': 64,\n",
       " 'financial reporting': 65,\n",
       " 'budgeting': 66,\n",
       " 'forecasting': 67,\n",
       " 'cost control': 68,\n",
       " 'risk management': 69,\n",
       " 'business strategy': 70,\n",
       " 'tax technology': 71,\n",
       " 'workday hcm': 72,\n",
       " 'ukg dimensions': 73,\n",
       " 'ceridian dayforce': 74,\n",
       " 'reporting': 75,\n",
       " 'analytical skills': 76,\n",
       " 'data load abilities (eibs)': 77,\n",
       " 'timekeeping': 78,\n",
       " 'payroll': 79,\n",
       " 'human capital management': 80,\n",
       " 'business process improvements': 81,\n",
       " 'configuration': 82,\n",
       " 'integration': 83,\n",
       " 'troubleshooting': 84,\n",
       " 'user requests': 85,\n",
       " 'interdependencies of system changes': 86,\n",
       " 'best practices': 87,\n",
       " 'organizational skills': 88,\n",
       " 'multitasking': 89,\n",
       " 'team collaboration': 90,\n",
       " 'dynamic environment': 91,\n",
       " 'global mindset': 92,\n",
       " 'diverse partners': 93,\n",
       " 'prioritization': 94,\n",
       " 'english verbal and written communication': 95,\n",
       " 'customer passion': 96,\n",
       " 'sap business objects': 97,\n",
       " 'sql': 98,\n",
       " 'qlik': 99,\n",
       " 'data modeling': 100,\n",
       " 'data warehousing': 101,\n",
       " 'dashboard design': 102,\n",
       " 'business intelligence': 103,\n",
       " 'communication': 104,\n",
       " 'problem solving': 105,\n",
       " 'data integration': 106,\n",
       " 'information technology': 107,\n",
       " 'microsoft office': 108,\n",
       " 'system design': 109,\n",
       " 'leadership': 110,\n",
       " 'negotiation': 111,\n",
       " 'teamwork': 112,\n",
       " 'attention to detail': 113,\n",
       " 'adaptability': 114,\n",
       " 'research': 115,\n",
       " 'root cause analysis': 116,\n",
       " 'workarounds': 117,\n",
       " 'requirements gathering': 118,\n",
       " 'user acceptance testing': 119,\n",
       " 'change impact assessment': 120,\n",
       " 'costbenefit analysis': 121,\n",
       " 'risk assessment': 122,\n",
       " 'data analytics': 123,\n",
       " 'underwriting': 124,\n",
       " 'gis': 125,\n",
       " 'arcgis': 126,\n",
       " 'arcgis pro': 127,\n",
       " 'arcgis hub': 128,\n",
       " 'arcgis portal': 129,\n",
       " 'arcgis online': 130,\n",
       " 'storymaps': 131,\n",
       " 'esri gis': 132,\n",
       " 'spatial analysis': 133,\n",
       " 'tuition educational assistance programs': 134,\n",
       " 'network solutions': 135,\n",
       " 'ddi (dns dhcp ipam)': 136,\n",
       " 'routing': 137,\n",
       " 'switching': 138,\n",
       " 'load balancing': 139,\n",
       " 'bgp': 140,\n",
       " 'ospf': 141,\n",
       " 'tcp/ip': 142,\n",
       " 'cloud infrastructure': 143,\n",
       " 'azure': 144,\n",
       " 'vmware nsx': 145,\n",
       " 'avi': 146,\n",
       " 'network management platforms': 147,\n",
       " 'packet analysis': 148,\n",
       " 'complex network issues': 149,\n",
       " 'microsoft azure': 150,\n",
       " 'software development': 151,\n",
       " 'flowcharting': 152,\n",
       " 'programming logic': 153,\n",
       " 'ms office': 154,\n",
       " 'visio': 155,\n",
       " 'gliffy': 156,\n",
       " 'balsamiq': 157,\n",
       " 'pmipba': 158,\n",
       " 'ireb': 159,\n",
       " 'itil': 160,\n",
       " 'system analysis': 161,\n",
       " 'requirements engineering': 162,\n",
       " 'xml': 163,\n",
       " 'sql (ssrs)': 164,\n",
       " 'java script': 165,\n",
       " 'powershell': 166,\n",
       " 'web services': 167,\n",
       " 'xslt': 168,\n",
       " '.net': 169,\n",
       " 'ms office programs': 170,\n",
       " 'sharepoint workflow': 171,\n",
       " 'electronic workflow interface expertise': 172,\n",
       " 'as400': 173,\n",
       " 'midrange or mainframe computer operating systems': 174,\n",
       " 'database': 175,\n",
       " 'microsoft networking operating systems': 176,\n",
       " 'documents': 177,\n",
       " 'spreadsheets': 178,\n",
       " 'critical thinking': 179,\n",
       " 'reading comprehension': 180,\n",
       " 'systems analysis': 181,\n",
       " 'vendor management': 182,\n",
       " 'decision making': 183,\n",
       " '* network engineering': 184,\n",
       " '* network operations': 185,\n",
       " 'business': 186,\n",
       " 'economics': 187,\n",
       " 'engineering': 188,\n",
       " 'math': 189,\n",
       " 'computer science': 190,\n",
       " 'spreadsheet': 191,\n",
       " 'data management': 192,\n",
       " 'presentation': 193,\n",
       " 'oracle': 194,\n",
       " 'hyperion': 195,\n",
       " 'accounting research': 196,\n",
       " 'regulatory compliance': 197,\n",
       " 'financial analysis': 198,\n",
       " 'control functions': 199,\n",
       " 'audits': 200,\n",
       " \"bachelor's degree in accounting or finance\": 201,\n",
       " 'cpa or active pursuit': 202,\n",
       " 'insurance experience': 203,\n",
       " 'financial reporting experience': 204,\n",
       " 'problemsolving skills': 205,\n",
       " 'ability to prioritize': 206,\n",
       " 'ability to meet deadlines': 207,\n",
       " 'ability to work in an urgent environment': 208,\n",
       " 'ability to selfreview work': 209,\n",
       " 'ability to work outside normal business hours': 210,\n",
       " 'vat/gst': 211,\n",
       " 'sales tax': 212,\n",
       " 'presentation skills': 213,\n",
       " 'collaboration': 214,\n",
       " 'devops': 215,\n",
       " 'scrum': 216,\n",
       " 'kanban': 217,\n",
       " 'software development lifecycle': 218,\n",
       " 'operating systems': 219,\n",
       " 'software testing tools': 220,\n",
       " 'development tools': 221,\n",
       " 'modeling tools': 222,\n",
       " 'technical workflow design': 223,\n",
       " 'time management': 224,\n",
       " 'mysql': 225,\n",
       " 'etl': 226,\n",
       " 'domo': 227,\n",
       " 'access': 228,\n",
       " 'automotive retail': 229,\n",
       " 'service industries': 230,\n",
       " 'business partnering': 231,\n",
       " 'spreadsheet skills': 232,\n",
       " 'microsoft excel': 233,\n",
       " 'legal research': 234,\n",
       " 'legal analytics': 235,\n",
       " 'intranet': 236,\n",
       " 'sydneyenterprise': 237,\n",
       " 'westlaw edge': 238,\n",
       " 'westlaw precision': 239,\n",
       " 'westlaw litigation analytics': 240,\n",
       " 'monitor suite': 241,\n",
       " 'hoovers': 242,\n",
       " 'mergent': 243,\n",
       " 'vitallaw': 244,\n",
       " 'law.com compass': 245,\n",
       " 'pacer': 246,\n",
       " 'pivot tables': 247,\n",
       " 'pivot charts': 248,\n",
       " 'law firm experience': 249,\n",
       " 'legal data analytics': 250,\n",
       " 'mlis': 251,\n",
       " \"bachelor's degree\": 252,\n",
       " 'paralegal studies': 253,\n",
       " 'oracle erp': 254,\n",
       " 'oracle cloud erp': 255,\n",
       " 'programming': 256,\n",
       " 'system development': 257,\n",
       " 'test plan': 258,\n",
       " 'system portfolio': 259,\n",
       " 'problemsolving': 260,\n",
       " 'information systems': 261,\n",
       " 'statistics': 262,\n",
       " 'policy analysis': 263,\n",
       " 'performance evaluation': 264,\n",
       " 'operational management': 265,\n",
       " 'inferential statistics': 266,\n",
       " 'legal compliance': 267,\n",
       " 'data collection': 268,\n",
       " 'study design': 269,\n",
       " 'report writing': 270,\n",
       " 'microsoft office suite': 271,\n",
       " 'statistical software': 272,\n",
       " 'statistical regression': 273,\n",
       " 'sas': 274,\n",
       " 'spss': 275,\n",
       " 'financial administration': 276,\n",
       " 'budget analysis': 277,\n",
       " 'revenue forecasting': 278,\n",
       " 'expenditure forecasting': 279,\n",
       " 'cash flow management': 280,\n",
       " 'internal control': 281,\n",
       " 'budget development': 282,\n",
       " 'financial data analysis': 283,\n",
       " 'longrange planning': 284,\n",
       " 'shortrange planning': 285,\n",
       " 'projections': 286,\n",
       " 'law interpretation': 287,\n",
       " 'policy interpretation': 288,\n",
       " 'rule interpretation': 289,\n",
       " 'regulation interpretation': 290,\n",
       " 'standard interpretation': 291,\n",
       " 'procedure interpretation': 292,\n",
       " 'business administration': 293,\n",
       " 'business management': 294,\n",
       " 'computer information systems': 295,\n",
       " 'cost analysis': 296,\n",
       " 'fiscal management studies': 297,\n",
       " 'narrative report writing': 298,\n",
       " 'federal regulations': 299,\n",
       " 'state regulations': 300,\n",
       " 'policies': 301,\n",
       " 'procedures': 302,\n",
       " 'laws': 303,\n",
       " 'rules': 304,\n",
       " 'revpro': 305,\n",
       " 'cloud gcp': 306,\n",
       " 'coding': 307,\n",
       " 'meetings and data': 308,\n",
       " 'sap': 309,\n",
       " 'sap apo': 310,\n",
       " 'sap pp': 311,\n",
       " 'sap pm': 312,\n",
       " 'sap portal': 313,\n",
       " 'sap mm': 314,\n",
       " 'sap mii': 315,\n",
       " 'sap ariba': 316,\n",
       " 'logility': 317,\n",
       " 'lsmw': 318,\n",
       " 'sap production planning and detailed scheduling (pp/ds)': 319,\n",
       " 'cif': 320,\n",
       " 'sap screen personalization tool': 321,\n",
       " 'make to stock': 322,\n",
       " 'make to order': 323,\n",
       " 'process manufacturing': 324,\n",
       " 'procure to pay': 325,\n",
       " 'preventative maintenance': 326,\n",
       " 'repairs': 327,\n",
       " 'demand and production planning': 328,\n",
       " 'bom': 329,\n",
       " 'routings': 330,\n",
       " 'production orders': 331,\n",
       " 'work orders': 332,\n",
       " 'production costing': 333,\n",
       " 'production reporting': 334,\n",
       " 'integration points': 335,\n",
       " 'multicountry': 336,\n",
       " 'multibusiness model': 337,\n",
       " 'project implementation': 338,\n",
       " 'functional specifications': 339,\n",
       " 'regression testing': 340,\n",
       " 'change management': 341,\n",
       " 'user documentation': 342,\n",
       " 'work instructions': 343,\n",
       " 'business process': 344,\n",
       " 'security / technical requirement': 345,\n",
       " 'data migration': 346,\n",
       " 'customer focus': 347,\n",
       " 'learning on the fly': 348,\n",
       " 'interpersonal savvy': 349,\n",
       " 'drive for results': 350,\n",
       " 'kronos': 351,\n",
       " 'adp': 352,\n",
       " 'eibs': 353,\n",
       " 'server support': 354,\n",
       " 'vmware virtualization': 355,\n",
       " 'configuration management': 356,\n",
       " 'web engineering support': 357,\n",
       " 'linux': 358,\n",
       " 'red hat': 359,\n",
       " 'hp/ux': 360,\n",
       " 'middleware support': 361,\n",
       " 'sftp': 362,\n",
       " 'sharepoint': 363,\n",
       " 'cohesity': 364,\n",
       " 'f5': 365,\n",
       " 'websphere': 366,\n",
       " 'apache': 367,\n",
       " 'iis': 368,\n",
       " 'ansible': 369,\n",
       " 'project leadership': 370,\n",
       " 'creative thinking': 371,\n",
       " 'machine learning': 372,\n",
       " 'artificial intelligence': 373,\n",
       " 'virtual assistants': 374,\n",
       " 'blockchain': 375,\n",
       " 'human resources': 376,\n",
       " 'predictive analytics': 377,\n",
       " 'prescriptive analytics': 378,\n",
       " 'r': 379,\n",
       " 'python': 380,\n",
       " 'matlab': 381,\n",
       " 'mathematics': 382,\n",
       " 'monte carlo simulations': 383,\n",
       " 'simulation': 384,\n",
       " 'catastrophe modeling': 385,\n",
       " 'data mining': 386,\n",
       " 'pl/sql': 387,\n",
       " 'sql*plus': 388,\n",
       " 'oracle forms': 389,\n",
       " 'java': 390,\n",
       " 'css': 391,\n",
       " 'javascript': 392,\n",
       " 'perl': 393,\n",
       " 'html': 394,\n",
       " 'http': 395,\n",
       " 'apis': 396,\n",
       " 'system development life cycle (sdlc)': 397,\n",
       " 'complex principles and procedures of computer systems': 398,\n",
       " 'complex software applications': 399,\n",
       " 'hardware': 400,\n",
       " 'telecommunications': 401,\n",
       " 'networking principles': 402,\n",
       " 'relational database concepts': 403,\n",
       " 'design techniques': 404,\n",
       " 'tools': 405,\n",
       " 'computer file methods': 406,\n",
       " 'structured testing techniques': 407,\n",
       " 'objectoriented software development techniques': 408,\n",
       " 'interpersonal skills': 409,\n",
       " 'training skills': 410,\n",
       " 'ability to work independently': 411,\n",
       " 'ability to work in a team': 412,\n",
       " 'ellucian banner': 413,\n",
       " 'system development life cycles (sdlc)': 414,\n",
       " 'objectoriented software development': 415,\n",
       " 'technical writing skills': 416,\n",
       " 'database design': 417,\n",
       " 'er modeling tools': 418,\n",
       " 'capital projects': 419,\n",
       " 'cost allocation': 420,\n",
       " 'debt financing': 421,\n",
       " 'development impact fees': 422,\n",
       " 'economic analysis': 423,\n",
       " 'fiscal information': 424,\n",
       " 'governmental accounting': 425,\n",
       " 'grant accounting': 426,\n",
       " 'grant management': 427,\n",
       " 'infrastructure funding': 428,\n",
       " 'principales of capital planning': 429,\n",
       " 'public administration': 430,\n",
       " 'supervision': 431,\n",
       " 'team leadership': 432,\n",
       " 'training': 433,\n",
       " 'microleadership': 434,\n",
       " 'service learning': 435,\n",
       " 'multidisciplinary collaboration': 436,\n",
       " 'human resource business partners': 437,\n",
       " 'statistical methods': 438,\n",
       " 'business degree': 439,\n",
       " 'social innovation': 440,\n",
       " 'massively multidisciplinary collaboration': 441,\n",
       " 'insurance': 442,\n",
       " 'technical research': 443,\n",
       " 'environmental research': 444,\n",
       " 'business research': 445,\n",
       " 'insight generation': 446,\n",
       " 'expectations': 447,\n",
       " 'crossfunctional work': 448,\n",
       " 'curiosity': 449,\n",
       " 'creativity': 450,\n",
       " 'independence': 451,\n",
       " 'data identification': 452,\n",
       " 'solution development': 453,\n",
       " 'citation skills': 454,\n",
       " 'project managers': 455,\n",
       " 'biometric data analysis': 456,\n",
       " 'data security': 457,\n",
       " 'data privacy': 458,\n",
       " 'healthcare operations': 459,\n",
       " 'healthcare systems': 460,\n",
       " 'recordkeeping processes': 461,\n",
       " 'data preprocessing': 462,\n",
       " 'model building': 463,\n",
       " 'data presentation': 464,\n",
       " 'budgeting reports': 465,\n",
       " 'data communication': 466,\n",
       " 'datadriven decision making': 467,\n",
       " 'healthcare quality improvement': 468,\n",
       " 'cost reduction': 469,\n",
       " 'enterprise data warehouse (edw)': 470,\n",
       " 'social enterprise': 471,\n",
       " 'systemic change': 472,\n",
       " 'marketing analysis': 473,\n",
       " 'tableau': 474,\n",
       " 'power bi': 475,\n",
       " 'excel bi': 476,\n",
       " 'dashboard development': 477,\n",
       " 'descriptive analytics': 478,\n",
       " 'charting': 479,\n",
       " 'graphing': 480,\n",
       " 'exploratory analysis': 481,\n",
       " 'gis data visualization': 482,\n",
       " 'mapping': 483,\n",
       " 'crowdsourcing': 484,\n",
       " 'project finance': 485,\n",
       " 'budget variance analysis': 486,\n",
       " 'estimate to complete': 487,\n",
       " 'estimate at complete': 488,\n",
       " 'budget baseline analysis': 489,\n",
       " 'sales forecasting': 490,\n",
       " 'contract reconciliation': 491,\n",
       " 'revenue recognition': 492,\n",
       " 'percent complete': 493,\n",
       " 'cdrls': 494,\n",
       " 'microsoft power point': 495,\n",
       " 'microsoft word': 496,\n",
       " 'iso': 497,\n",
       " 'as9100': 498,\n",
       " 'erp/mrp software': 499,\n",
       " 'microsoft power bi': 500,\n",
       " 'dax': 501,\n",
       " 'power query': 502,\n",
       " 'sql server': 503,\n",
       " 'power pi': 504,\n",
       " 'microsoft office (outlook excel powerpoint teams)': 505,\n",
       " 'erp/crm systems': 506,\n",
       " 'business process automation': 507,\n",
       " 'data aggregation': 508,\n",
       " 'data interpretation': 509,\n",
       " 'strategic planning': 510,\n",
       " 'tactical planning': 511,\n",
       " 'qlikview': 512,\n",
       " 'hadoop': 513,\n",
       " 'spark': 514,\n",
       " 'nosql': 515,\n",
       " 'cloud computing': 516,\n",
       " 'agile development': 517,\n",
       " 'microsoft power bi desktop': 518,\n",
       " 'dax expressions and queries': 519,\n",
       " 'power bi (dax m formula languages)': 520,\n",
       " 'english': 521,\n",
       " 'chinesemandarin': 522,\n",
       " 'system reporting tools': 523,\n",
       " 'dashboards': 524,\n",
       " 'power pivot': 525,\n",
       " 'analytical thinking': 526,\n",
       " 'model n': 527,\n",
       " 'salesforce': 528,\n",
       " 'm formula languages': 529,\n",
       " 'api’s': 530,\n",
       " 'sharepoint design': 531,\n",
       " 'salesforce.com': 532,\n",
       " 'teams': 533,\n",
       " 'sales/marketing': 534,\n",
       " 'program development': 535,\n",
       " 'database schema design': 536,\n",
       " 'relational database': 537,\n",
       " 'er': 538,\n",
       " 'structured query language': 539,\n",
       " 'adobe photoshop': 540,\n",
       " 'objectoriented programming': 541,\n",
       " 'database schemas': 542,\n",
       " 'triggers': 543,\n",
       " 'ivr': 544,\n",
       " 'contact center technologies': 545,\n",
       " 'pega': 546,\n",
       " 'jira': 547,\n",
       " 'confluence': 548,\n",
       " 'business requirements': 549,\n",
       " 'business use cases': 550,\n",
       " 'process models': 551,\n",
       " 'business needs analysis': 552,\n",
       " 'conceptual design documents': 553,\n",
       " 'functional design': 554,\n",
       " 'workflow analysis': 555,\n",
       " 'use case analysis': 556,\n",
       " 'process modeling': 557,\n",
       " 'testing discipline': 558,\n",
       " 'test planning': 559,\n",
       " 'incident research and analysis': 560,\n",
       " 'test strategy': 561,\n",
       " 'qa testing': 562,\n",
       " 'data properties': 563,\n",
       " 'feasibility analysis': 564,\n",
       " 'meeting facilitation': 565,\n",
       " 'management and executive reporting': 566,\n",
       " 'change request management': 567,\n",
       " 'issue tracking': 568,\n",
       " 'process improvements': 569,\n",
       " 'project planning': 570,\n",
       " 'issue resolution': 571,\n",
       " 'break and hot fix support': 572,\n",
       " 'triage': 573,\n",
       " 'corrective action': 574,\n",
       " 'sql queries': 575,\n",
       " 'reports': 576,\n",
       " 'strong communication skills': 577,\n",
       " 'strong organizational skills': 578,\n",
       " 'excellence': 579,\n",
       " 'influence': 580,\n",
       " 'team and interpersonal skills': 581,\n",
       " 'process management': 582,\n",
       " 'stakeholder understanding': 583,\n",
       " 'decisionmaking skills': 584,\n",
       " 'predictive insights': 585,\n",
       " 'conflict resolution': 586,\n",
       " 'assumption resolution': 587,\n",
       " 'requirement summarization': 588,\n",
       " 'research skills': 589,\n",
       " 'network infrastructure': 590,\n",
       " 'cisco systems': 591,\n",
       " 'hp/aruba': 592,\n",
       " 'wan/lan': 593,\n",
       " 'routing and switching': 594,\n",
       " 'carrier transmission systems': 595,\n",
       " 'voip': 596,\n",
       " 'wlans/wifi': 597,\n",
       " 'cabling infrastructure': 598,\n",
       " 'business/cost analysis': 599,\n",
       " 'vendor and partner management': 600,\n",
       " 'ip video systems': 601,\n",
       " 'cloud networking infrastructure': 602,\n",
       " 'data integrity': 603,\n",
       " 'data visual': 604,\n",
       " 'program management': 605,\n",
       " 'microsoft access': 606,\n",
       " 'google workspace': 607,\n",
       " 'business operations applications': 608,\n",
       " 'system review': 609,\n",
       " 'technical support': 610,\n",
       " 'process review': 611,\n",
       " 'process redesign': 612,\n",
       " 'agent desktop': 613,\n",
       " 'case management tools': 614,\n",
       " 'use case scenarios': 615,\n",
       " 'business process modeling': 616,\n",
       " 'qa': 617,\n",
       " 'system behavior': 618,\n",
       " 'estimating': 619,\n",
       " 'incident identification and resolution': 620,\n",
       " 'documentation maintenance': 621,\n",
       " 'traceability': 622,\n",
       " 'customer needs and strategies': 623,\n",
       " 'formal and informal written communication': 624,\n",
       " 'meetings and presentations': 625,\n",
       " 'customer experience': 626,\n",
       " 'interpersonal and communication skills': 627,\n",
       " 'resourcefulness': 628,\n",
       " 'change champion': 629,\n",
       " 'process and best practices': 630,\n",
       " 'leading and mentoring': 631,\n",
       " 'project management skills': 632,\n",
       " 'data interpretation skills': 633,\n",
       " 'stakeholder needs': 634,\n",
       " 'innovative solutions': 635,\n",
       " 'system and process enhancements': 636,\n",
       " 'historical trends': 637,\n",
       " 'risk resolution': 638,\n",
       " 'requirements summarization': 639,\n",
       " 'business goals': 640,\n",
       " 'impact analysis': 641,\n",
       " 'dynamics 365 business central': 642,\n",
       " 'microsoft dynamics nav': 643,\n",
       " 'jet reports': 644,\n",
       " 'production processes': 645,\n",
       " 'assembly line processes': 646,\n",
       " 'inventory': 647,\n",
       " 'warehousing': 648,\n",
       " 'case management systems': 649,\n",
       " 'remote assistance tools': 650,\n",
       " 'content management systems (cms)': 651,\n",
       " 'agile methodology': 652,\n",
       " 'ux best practices': 653,\n",
       " 'accessibility': 654,\n",
       " 'search engine marketing (sem)': 655,\n",
       " 'optimization': 656,\n",
       " 'user manuals': 657,\n",
       " 'handbooks': 658,\n",
       " 'workflow diagrams': 659,\n",
       " 'activity diagrams': 660,\n",
       " 'cost benefit analysis': 661,\n",
       " 'risk analysis': 662,\n",
       " 'data gathering': 663,\n",
       " 'research & analytics': 664,\n",
       " 'problem identification': 665,\n",
       " 'solution recommendations': 666,\n",
       " 'it systems': 667,\n",
       " 'business process analytics': 668,\n",
       " 'financial modeling': 669,\n",
       " 'spreadsheet software': 670,\n",
       " 'ms excel': 671,\n",
       " 'presentations': 672,\n",
       " \"master's degree\": 673,\n",
       " 'data profiling': 674,\n",
       " 'data cleansing': 675,\n",
       " 'data manipulation': 676,\n",
       " 'pytest': 677,\n",
       " 'xray': 678,\n",
       " 'zephyr': 679,\n",
       " 'qmetry': 680,\n",
       " 'bitbucket': 681,\n",
       " 'data lineage': 682,\n",
       " 'test management': 683,\n",
       " 'test automation': 684,\n",
       " 'salesforce crm': 685,\n",
       " 'snowflake': 686,\n",
       " 'databricks': 687,\n",
       " 'rihv': 688,\n",
       " 'lean six sigma': 689,\n",
       " 'customer relationship management (crm)': 690,\n",
       " 'financial metrics': 691,\n",
       " 'customer health': 692,\n",
       " 'customer engagement': 693,\n",
       " 'customer adoption': 694,\n",
       " 'revenue growth': 695,\n",
       " 'business process improvement': 696,\n",
       " 'documentation': 697,\n",
       " 'user training': 698,\n",
       " \"bachelor's degree in business economics data analytics statistics\": 699,\n",
       " '4+ years of experience in a business analyst role': 700,\n",
       " 'private equity': 701,\n",
       " 'debt': 702,\n",
       " 'derivatives': 703,\n",
       " 'financial markets': 704,\n",
       " 'pricing': 705,\n",
       " 'valuation': 706,\n",
       " 'models': 707,\n",
       " 'analysis': 708,\n",
       " 'restructurings': 709,\n",
       " 'corporate actions': 710,\n",
       " 'mutual funds': 711,\n",
       " 'cfa level 3': 712,\n",
       " 'investment management': 713,\n",
       " 'investment performance': 714,\n",
       " 'statistical theory': 715,\n",
       " 'financial instruments': 716,\n",
       " 'quantitative techniques': 717,\n",
       " 'mathematical techniques': 718,\n",
       " 'fair valuations': 719,\n",
       " 'risk': 720,\n",
       " 'validation': 721,\n",
       " 'database management': 722,\n",
       " 'bloomberg': 723,\n",
       " 'market data': 724,\n",
       " 'organization': 725,\n",
       " 'accuracy': 726,\n",
       " 'common equity models': 727,\n",
       " 'black scholes': 728,\n",
       " 'binomial trees': 729,\n",
       " 'garch': 730,\n",
       " 'plain vanilla bonds': 731,\n",
       " 'complex securities valuation': 732,\n",
       " 'market data providers': 733,\n",
       " 'sales operations': 734,\n",
       " 'demand forecasting': 735,\n",
       " 'supply allocation': 736,\n",
       " 'demand generation': 737,\n",
       " 'intel foundry services (ifs)': 738,\n",
       " 'market insights': 739,\n",
       " 'strategic marketing': 740,\n",
       " 'planning': 741,\n",
       " 'supply/demand planning': 742,\n",
       " 'forecasting revenue': 743,\n",
       " 'sales enablement': 744,\n",
       " 'operations systems': 745,\n",
       " 'situational analysis': 746,\n",
       " 'organizational management': 747,\n",
       " 'supply chain operations': 748,\n",
       " 'foundry': 749,\n",
       " 'supply planning': 750,\n",
       " 'intel systems': 751,\n",
       " 'demand': 752,\n",
       " 'customer relationship management': 753,\n",
       " 'revenue optimization': 754,\n",
       " 'supply chain management': 755,\n",
       " \"bachelor's degree in business economics or supply chain\": 756,\n",
       " 'surveys': 757,\n",
       " 'legislation': 758,\n",
       " 'filing': 759,\n",
       " 'rate': 760,\n",
       " 'network': 761,\n",
       " 'westlaw': 762,\n",
       " 'insurance terminology': 763,\n",
       " 'proficiency with word excel outlook teams': 764,\n",
       " 'pricing processes': 765,\n",
       " 'compliance': 766,\n",
       " 'contract automation': 767,\n",
       " 'contract management': 768,\n",
       " 'pricing dynamics': 769,\n",
       " 'contract publishing': 770,\n",
       " 'contract tool development': 771,\n",
       " 'user acceptance': 772,\n",
       " 'enterprise systems': 773,\n",
       " 'financial risks': 774,\n",
       " 'process engineering': 775,\n",
       " 'sales and marketing': 776,\n",
       " 'complex sales': 777,\n",
       " 'product roadmaps': 778,\n",
       " 'emerging technology trends': 779,\n",
       " 'us locations': 780,\n",
       " 'requirement analysis': 781,\n",
       " 'business process documentation': 782,\n",
       " 'needs assessment': 783,\n",
       " 'test script mapping': 784,\n",
       " 'professional in business analysis (pmipba)': 785,\n",
       " 'cbap certification': 786,\n",
       " 'pmp': 787,\n",
       " 'data import and data export': 788,\n",
       " 'disaster management': 789,\n",
       " 'cost management': 790,\n",
       " 'emergency management': 791,\n",
       " 'cost recovery': 792,\n",
       " 'cost tracking': 793,\n",
       " 'demand planning': 794,\n",
       " 'process analysis': 795,\n",
       " 'business transformation': 796,\n",
       " 'collaborative process design': 797,\n",
       " 'information design': 798,\n",
       " 'planning domain': 799,\n",
       " 'industrystandard processes': 800,\n",
       " 'software development life cycle': 801,\n",
       " 'process development': 802,\n",
       " 'industry standard process': 803,\n",
       " 'business measures': 804,\n",
       " 'statistical analytics': 805,\n",
       " 'algorithmic analytics': 806,\n",
       " 'time series forecasting': 807,\n",
       " 'blue yonder': 808,\n",
       " 'erp': 809,\n",
       " 'project coordination': 810,\n",
       " 'computer skills': 811,\n",
       " 'data analyst': 812,\n",
       " 'vlookup': 813,\n",
       " 'data models': 814,\n",
       " 'datasets': 815,\n",
       " 'data figures': 816,\n",
       " 'descriptive analysis': 817,\n",
       " 'inferential analysis': 818,\n",
       " 'predictive analysis': 819,\n",
       " 'prescriptive analysis': 820,\n",
       " 'sybase': 821,\n",
       " 'db2': 822,\n",
       " 'ecda': 823,\n",
       " 'database requirements': 824,\n",
       " 'api integrations': 825,\n",
       " 'software systems': 826,\n",
       " 'modeling applications': 827,\n",
       " 'api documentation': 828,\n",
       " 'debugging': 829,\n",
       " 'integration issues': 830,\n",
       " 'industry trends': 831,\n",
       " 'database reporting': 832,\n",
       " 'data discovery': 833,\n",
       " 'decisionmaking': 834,\n",
       " 'crystal reports': 835,\n",
       " 'ssrs': 836,\n",
       " 'data trends': 837,\n",
       " 'data relationships': 838,\n",
       " 'conclusions': 839,\n",
       " 'sas statistical software': 840,\n",
       " 'aws': 841,\n",
       " 'redshift': 842,\n",
       " 's3 bucket': 843,\n",
       " 'machine learning methods': 844,\n",
       " 'data management schemas': 845,\n",
       " 'predictive modeling': 846,\n",
       " 'data quality': 847,\n",
       " 'business value': 848,\n",
       " 'maintenance metrics': 849,\n",
       " 'advanced analytics': 850,\n",
       " 'leadership briefs': 851,\n",
       " 'productivity improvement': 852,\n",
       " 'business processes': 853,\n",
       " 'servicenow': 854,\n",
       " 'it management systems': 855,\n",
       " 'statistical methodologies': 856,\n",
       " 'itil v4 foundations': 857,\n",
       " 'iso 20k': 858,\n",
       " 'text data labeling': 859,\n",
       " 'data labeling': 860,\n",
       " 'basic queries': 861,\n",
       " 'data labeling tools': 862,\n",
       " 'call transcripts': 863,\n",
       " 'text messages': 864,\n",
       " 'retail banking': 865,\n",
       " 'financial services': 866,\n",
       " 'oracle developer suite': 867,\n",
       " 'asp .net': 868,\n",
       " 'coldfusion': 869,\n",
       " 'c#': 870,\n",
       " 'office': 871,\n",
       " 'project': 872,\n",
       " 'adobe captivate': 873,\n",
       " 'lexis casesoft suite': 874,\n",
       " 'microsoft dynamics crm': 875,\n",
       " 'web development': 876,\n",
       " 'web design': 877,\n",
       " 'information architecture': 878,\n",
       " 'user experience': 879,\n",
       " 'sharepoint development': 880,\n",
       " 'office development': 881,\n",
       " 'excel development': 882,\n",
       " 'visio development': 883,\n",
       " 'project development': 884,\n",
       " 'database normalization': 885,\n",
       " 'entityrelationship diagrams': 886,\n",
       " 'data scrubbing': 887,\n",
       " 'intelligence analysis': 888,\n",
       " 'genomics': 889,\n",
       " 'bioinformatics': 890,\n",
       " 'data visualizations': 891,\n",
       " 'phylogenetic': 892,\n",
       " 'c++': 893,\n",
       " 'bash': 894,\n",
       " 'hive': 895,\n",
       " 'pig': 896,\n",
       " 'big data': 897,\n",
       " 'highperformance computing (hpc)': 898,\n",
       " 'algorithm development': 899,\n",
       " 'data editing': 900,\n",
       " 'data auditing': 901,\n",
       " 'data quality control': 902,\n",
       " 'multiomics': 903,\n",
       " 'genome assembly': 904,\n",
       " 'bacterial annotation': 905,\n",
       " 'microbial sequencing': 906,\n",
       " 'publications': 907,\n",
       " 'data representation': 908,\n",
       " 'databases': 909,\n",
       " 'business intelligence (bi)': 910,\n",
       " 'microsoft sql server': 911,\n",
       " 'travel': 912,\n",
       " 'google': 913,\n",
       " 'facebook': 914,\n",
       " 'ctrip': 915,\n",
       " 'trip.com': 916,\n",
       " 'makemytrip': 917,\n",
       " 'grab': 918,\n",
       " 'amazon': 919,\n",
       " 'pandas': 920,\n",
       " 'artificial intelligence (ai)': 921,\n",
       " 'capital one': 922,\n",
       " 'accenture': 923,\n",
       " 'upwork': 924,\n",
       " 'deloitte': 925,\n",
       " 'mckinsey': 926,\n",
       " 'bain': 927,\n",
       " 'microsoft': 928,\n",
       " 'uber': 929,\n",
       " 'lyft': 930,\n",
       " 'gojek': 931,\n",
       " 'lazada': 932,\n",
       " 'alibaba': 933,\n",
       " 'shopify': 934,\n",
       " 'expedia': 935,\n",
       " 'skyscanner': 936,\n",
       " 'allsource analyst': 937,\n",
       " 'opensource media': 938,\n",
       " 'news reports': 939,\n",
       " 'social media': 940,\n",
       " 'classified materials': 941,\n",
       " 'unclassified materials': 942,\n",
       " 'country analysis': 943,\n",
       " 'regional analysis': 944,\n",
       " 'threat assessments': 945,\n",
       " 'strategic assessments': 946,\n",
       " 'intelligence assessments': 947,\n",
       " 'intelligence products': 948,\n",
       " 'collection managers': 949,\n",
       " 'targets': 950,\n",
       " 'policymakers': 951,\n",
       " 'leaders': 952,\n",
       " 'political science': 953,\n",
       " 'public policy': 954,\n",
       " 'cybersecurity': 955,\n",
       " 'national security': 956,\n",
       " 'top secret': 957,\n",
       " 'sci clearance': 958,\n",
       " 'us citizenship': 959,\n",
       " 'pattern recognition': 960,\n",
       " 'trend identification': 961,\n",
       " 'threat identification': 962,\n",
       " 'actionable indicators': 963,\n",
       " 'written assessments': 964,\n",
       " 'oral assessments': 965,\n",
       " 'intelligence community databases': 966,\n",
       " 'intelligence tools': 967,\n",
       " 'intelligence techniques': 968,\n",
       " 'foia xpress': 969,\n",
       " 'federal web based systems': 970,\n",
       " 'foia exemptions': 971,\n",
       " 'foia request processing': 972,\n",
       " 'foia request marking and redactions': 973,\n",
       " 'foia information segregation': 974,\n",
       " 'document redactions': 975,\n",
       " 'information policy’s guidance': 976,\n",
       " 'records assessment': 977,\n",
       " 'foia requests completion': 978,\n",
       " 'foia guidelines': 979,\n",
       " 'frma': 980,\n",
       " 'departmental regulations and issuances': 981,\n",
       " 'organizational and departmental policy statements': 982,\n",
       " 'complex information management statutes': 983,\n",
       " 'foia request data entry': 984,\n",
       " 'draft acknowledgement letters': 985,\n",
       " 'records redaction': 986,\n",
       " 'juniorlevel analyst work knowledge base': 987,\n",
       " 'phylogenetics': 988,\n",
       " 'metagenomics': 989,\n",
       " 'metatranscriptomics': 990,\n",
       " 'bash script': 991,\n",
       " 'hpc': 992,\n",
       " 'pipelines': 993,\n",
       " 'attack the network tool suite': 994,\n",
       " 'regex': 995,\n",
       " 'api requests': 996,\n",
       " 'geoint tools': 997,\n",
       " 'jupyterhub': 998,\n",
       " 'jupyter notebook': 999,\n",
       " 'json': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start of with a python dict. and then convert this dict into a pandas df, or push \n",
    "# directly into a PostgreSQL\n",
    "\n",
    "pipeline.scan_for_new_skills(analyst_postings, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d031ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data analysis': 1,\n",
       " 'market research': 2,\n",
       " 'survey development': 3,\n",
       " 'analytical methods': 4,\n",
       " 'quantitative initiatives': 5,\n",
       " 'qualitative initiatives': 6,\n",
       " 'business solutions': 7,\n",
       " 'project management': 8,\n",
       " 'communication skills': 9,\n",
       " 'customer service': 10,\n",
       " 'statistical analysis': 11,\n",
       " 'data visualization': 12,\n",
       " 'data storytelling': 13,\n",
       " 'stakeholder engagement': 14,\n",
       " 'windows': 15,\n",
       " 'excel': 16,\n",
       " 'word': 17,\n",
       " 'powerpoint': 18,\n",
       " 'market research software': 19,\n",
       " 'marketing': 20,\n",
       " 'analytics': 21,\n",
       " 'psychology': 22,\n",
       " 'data science': 23,\n",
       " 'business analysis': 24,\n",
       " 'technical writing': 25,\n",
       " 'software testing': 26,\n",
       " 'systems development': 27,\n",
       " 'process improvement': 28,\n",
       " 'sdlc': 29,\n",
       " 'web technologies': 30,\n",
       " 'jbase': 31,\n",
       " 'mobile technologies': 32,\n",
       " 'relational databases': 33,\n",
       " 'agile': 34,\n",
       " 'waterfall': 35,\n",
       " 'accounting': 36,\n",
       " 'finance': 37,\n",
       " 'vat/gst tax regimes': 38,\n",
       " 'us and foreign tax laws': 39,\n",
       " 'vat and sales tax issues': 40,\n",
       " 'indirect tax': 41,\n",
       " 'sales and use tax': 42,\n",
       " 'excise tax': 43,\n",
       " 'tax compliance': 44,\n",
       " 'tax audits': 45,\n",
       " 'tax inquiries': 46,\n",
       " 'tax preparation': 47,\n",
       " 'tax analysis': 48,\n",
       " 'tax reporting': 49,\n",
       " 'tax regulations': 50,\n",
       " 'tax codes': 51,\n",
       " 'tax policies': 52,\n",
       " 'tax laws': 53,\n",
       " 'tax treaties': 54,\n",
       " 'tax planning': 55,\n",
       " 'tax accounting': 56,\n",
       " 'tax software': 57,\n",
       " 'tax research': 58,\n",
       " 'tax consulting': 59,\n",
       " 'tax management': 60,\n",
       " 'tax optimization': 61,\n",
       " 'tax minimization': 62,\n",
       " 'erp systems': 63,\n",
       " 'internal auditing': 64,\n",
       " 'financial reporting': 65,\n",
       " 'budgeting': 66,\n",
       " 'forecasting': 67,\n",
       " 'cost control': 68,\n",
       " 'risk management': 69,\n",
       " 'business strategy': 70,\n",
       " 'tax technology': 71,\n",
       " 'workday hcm': 72,\n",
       " 'ukg dimensions': 73,\n",
       " 'ceridian dayforce': 74,\n",
       " 'reporting': 75,\n",
       " 'analytical skills': 76,\n",
       " 'data load abilities (eibs)': 77,\n",
       " 'timekeeping': 78,\n",
       " 'payroll': 79,\n",
       " 'human capital management': 80,\n",
       " 'business process improvements': 81,\n",
       " 'configuration': 82,\n",
       " 'integration': 83,\n",
       " 'troubleshooting': 84,\n",
       " 'user requests': 85,\n",
       " 'interdependencies of system changes': 86,\n",
       " 'best practices': 87,\n",
       " 'organizational skills': 88,\n",
       " 'multitasking': 89,\n",
       " 'team collaboration': 90,\n",
       " 'dynamic environment': 91,\n",
       " 'global mindset': 92,\n",
       " 'diverse partners': 93,\n",
       " 'prioritization': 94,\n",
       " 'english verbal and written communication': 95,\n",
       " 'customer passion': 96,\n",
       " 'sap business objects': 97,\n",
       " 'sql': 98,\n",
       " 'qlik': 99,\n",
       " 'data modeling': 100,\n",
       " 'data warehousing': 101,\n",
       " 'dashboard design': 102,\n",
       " 'business intelligence': 103,\n",
       " 'communication': 104,\n",
       " 'problem solving': 105,\n",
       " 'data integration': 106,\n",
       " 'information technology': 107,\n",
       " 'microsoft office': 108,\n",
       " 'system design': 109,\n",
       " 'leadership': 110,\n",
       " 'negotiation': 111,\n",
       " 'teamwork': 112,\n",
       " 'attention to detail': 113,\n",
       " 'adaptability': 114,\n",
       " 'research': 115,\n",
       " 'root cause analysis': 116,\n",
       " 'workarounds': 117,\n",
       " 'requirements gathering': 118,\n",
       " 'user acceptance testing': 119,\n",
       " 'change impact assessment': 120,\n",
       " 'costbenefit analysis': 121,\n",
       " 'risk assessment': 122,\n",
       " 'data analytics': 123,\n",
       " 'underwriting': 124,\n",
       " 'gis': 125,\n",
       " 'arcgis': 126,\n",
       " 'arcgis pro': 127,\n",
       " 'arcgis hub': 128,\n",
       " 'arcgis portal': 129,\n",
       " 'arcgis online': 130,\n",
       " 'storymaps': 131,\n",
       " 'esri gis': 132,\n",
       " 'spatial analysis': 133,\n",
       " 'tuition educational assistance programs': 134,\n",
       " 'network solutions': 135,\n",
       " 'ddi (dns dhcp ipam)': 136,\n",
       " 'routing': 137,\n",
       " 'switching': 138,\n",
       " 'load balancing': 139,\n",
       " 'bgp': 140,\n",
       " 'ospf': 141,\n",
       " 'tcp/ip': 142,\n",
       " 'cloud infrastructure': 143,\n",
       " 'azure': 144,\n",
       " 'vmware nsx': 145,\n",
       " 'avi': 146,\n",
       " 'network management platforms': 147,\n",
       " 'packet analysis': 148,\n",
       " 'complex network issues': 149,\n",
       " 'microsoft azure': 150,\n",
       " 'software development': 151,\n",
       " 'flowcharting': 152,\n",
       " 'programming logic': 153,\n",
       " 'ms office': 154,\n",
       " 'visio': 155,\n",
       " 'gliffy': 156,\n",
       " 'balsamiq': 157,\n",
       " 'pmipba': 158,\n",
       " 'ireb': 159,\n",
       " 'itil': 160,\n",
       " 'system analysis': 161,\n",
       " 'requirements engineering': 162,\n",
       " 'xml': 163,\n",
       " 'sql (ssrs)': 164,\n",
       " 'java script': 165,\n",
       " 'powershell': 166,\n",
       " 'web services': 167,\n",
       " 'xslt': 168,\n",
       " '.net': 169,\n",
       " 'ms office programs': 170,\n",
       " 'sharepoint workflow': 171,\n",
       " 'electronic workflow interface expertise': 172,\n",
       " 'as400': 173,\n",
       " 'midrange or mainframe computer operating systems': 174,\n",
       " 'database': 175,\n",
       " 'microsoft networking operating systems': 176,\n",
       " 'documents': 177,\n",
       " 'spreadsheets': 178,\n",
       " 'critical thinking': 179,\n",
       " 'reading comprehension': 180,\n",
       " 'systems analysis': 181,\n",
       " 'vendor management': 182,\n",
       " 'decision making': 183,\n",
       " '* network engineering': 184,\n",
       " '* network operations': 185,\n",
       " 'business': 186,\n",
       " 'economics': 187,\n",
       " 'engineering': 188,\n",
       " 'math': 189,\n",
       " 'computer science': 190,\n",
       " 'spreadsheet': 191,\n",
       " 'data management': 192,\n",
       " 'presentation': 193,\n",
       " 'oracle': 194,\n",
       " 'hyperion': 195,\n",
       " 'accounting research': 196,\n",
       " 'regulatory compliance': 197,\n",
       " 'financial analysis': 198,\n",
       " 'control functions': 199,\n",
       " 'audits': 200,\n",
       " \"bachelor's degree in accounting or finance\": 201,\n",
       " 'cpa or active pursuit': 202,\n",
       " 'insurance experience': 203,\n",
       " 'financial reporting experience': 204,\n",
       " 'problemsolving skills': 205,\n",
       " 'ability to prioritize': 206,\n",
       " 'ability to meet deadlines': 207,\n",
       " 'ability to work in an urgent environment': 208,\n",
       " 'ability to selfreview work': 209,\n",
       " 'ability to work outside normal business hours': 210,\n",
       " 'vat/gst': 211,\n",
       " 'sales tax': 212,\n",
       " 'presentation skills': 213,\n",
       " 'collaboration': 214,\n",
       " 'devops': 215,\n",
       " 'scrum': 216,\n",
       " 'kanban': 217,\n",
       " 'software development lifecycle': 218,\n",
       " 'operating systems': 219,\n",
       " 'software testing tools': 220,\n",
       " 'development tools': 221,\n",
       " 'modeling tools': 222,\n",
       " 'technical workflow design': 223,\n",
       " 'time management': 224,\n",
       " 'mysql': 225,\n",
       " 'etl': 226,\n",
       " 'domo': 227,\n",
       " 'access': 228,\n",
       " 'automotive retail': 229,\n",
       " 'service industries': 230,\n",
       " 'business partnering': 231,\n",
       " 'spreadsheet skills': 232,\n",
       " 'microsoft excel': 233,\n",
       " 'legal research': 234,\n",
       " 'legal analytics': 235,\n",
       " 'intranet': 236,\n",
       " 'sydneyenterprise': 237,\n",
       " 'westlaw edge': 238,\n",
       " 'westlaw precision': 239,\n",
       " 'westlaw litigation analytics': 240,\n",
       " 'monitor suite': 241,\n",
       " 'hoovers': 242,\n",
       " 'mergent': 243,\n",
       " 'vitallaw': 244,\n",
       " 'law.com compass': 245,\n",
       " 'pacer': 246,\n",
       " 'pivot tables': 247,\n",
       " 'pivot charts': 248,\n",
       " 'law firm experience': 249,\n",
       " 'legal data analytics': 250,\n",
       " 'mlis': 251,\n",
       " \"bachelor's degree\": 252,\n",
       " 'paralegal studies': 253,\n",
       " 'oracle erp': 254,\n",
       " 'oracle cloud erp': 255,\n",
       " 'programming': 256,\n",
       " 'system development': 257,\n",
       " 'test plan': 258,\n",
       " 'system portfolio': 259,\n",
       " 'problemsolving': 260,\n",
       " 'information systems': 261,\n",
       " 'statistics': 262,\n",
       " 'policy analysis': 263,\n",
       " 'performance evaluation': 264,\n",
       " 'operational management': 265,\n",
       " 'inferential statistics': 266,\n",
       " 'legal compliance': 267,\n",
       " 'data collection': 268,\n",
       " 'study design': 269,\n",
       " 'report writing': 270,\n",
       " 'microsoft office suite': 271,\n",
       " 'statistical software': 272,\n",
       " 'statistical regression': 273,\n",
       " 'sas': 274,\n",
       " 'spss': 275,\n",
       " 'financial administration': 276,\n",
       " 'budget analysis': 277,\n",
       " 'revenue forecasting': 278,\n",
       " 'expenditure forecasting': 279,\n",
       " 'cash flow management': 280,\n",
       " 'internal control': 281,\n",
       " 'budget development': 282,\n",
       " 'financial data analysis': 283,\n",
       " 'longrange planning': 284,\n",
       " 'shortrange planning': 285,\n",
       " 'projections': 286,\n",
       " 'law interpretation': 287,\n",
       " 'policy interpretation': 288,\n",
       " 'rule interpretation': 289,\n",
       " 'regulation interpretation': 290,\n",
       " 'standard interpretation': 291,\n",
       " 'procedure interpretation': 292,\n",
       " 'business administration': 293,\n",
       " 'business management': 294,\n",
       " 'computer information systems': 295,\n",
       " 'cost analysis': 296,\n",
       " 'fiscal management studies': 297,\n",
       " 'narrative report writing': 298,\n",
       " 'federal regulations': 299,\n",
       " 'state regulations': 300,\n",
       " 'policies': 301,\n",
       " 'procedures': 302,\n",
       " 'laws': 303,\n",
       " 'rules': 304,\n",
       " 'revpro': 305,\n",
       " 'cloud gcp': 306,\n",
       " 'coding': 307,\n",
       " 'meetings and data': 308,\n",
       " 'sap': 309,\n",
       " 'sap apo': 310,\n",
       " 'sap pp': 311,\n",
       " 'sap pm': 312,\n",
       " 'sap portal': 313,\n",
       " 'sap mm': 314,\n",
       " 'sap mii': 315,\n",
       " 'sap ariba': 316,\n",
       " 'logility': 317,\n",
       " 'lsmw': 318,\n",
       " 'sap production planning and detailed scheduling (pp/ds)': 319,\n",
       " 'cif': 320,\n",
       " 'sap screen personalization tool': 321,\n",
       " 'make to stock': 322,\n",
       " 'make to order': 323,\n",
       " 'process manufacturing': 324,\n",
       " 'procure to pay': 325,\n",
       " 'preventative maintenance': 326,\n",
       " 'repairs': 327,\n",
       " 'demand and production planning': 328,\n",
       " 'bom': 329,\n",
       " 'routings': 330,\n",
       " 'production orders': 331,\n",
       " 'work orders': 332,\n",
       " 'production costing': 333,\n",
       " 'production reporting': 334,\n",
       " 'integration points': 335,\n",
       " 'multicountry': 336,\n",
       " 'multibusiness model': 337,\n",
       " 'project implementation': 338,\n",
       " 'functional specifications': 339,\n",
       " 'regression testing': 340,\n",
       " 'change management': 341,\n",
       " 'user documentation': 342,\n",
       " 'work instructions': 343,\n",
       " 'business process': 344,\n",
       " 'security / technical requirement': 345,\n",
       " 'data migration': 346,\n",
       " 'customer focus': 347,\n",
       " 'learning on the fly': 348,\n",
       " 'interpersonal savvy': 349,\n",
       " 'drive for results': 350,\n",
       " 'kronos': 351,\n",
       " 'adp': 352,\n",
       " 'eibs': 353,\n",
       " 'server support': 354,\n",
       " 'vmware virtualization': 355,\n",
       " 'configuration management': 356,\n",
       " 'web engineering support': 357,\n",
       " 'linux': 358,\n",
       " 'red hat': 359,\n",
       " 'hp/ux': 360,\n",
       " 'middleware support': 361,\n",
       " 'sftp': 362,\n",
       " 'sharepoint': 363,\n",
       " 'cohesity': 364,\n",
       " 'f5': 365,\n",
       " 'websphere': 366,\n",
       " 'apache': 367,\n",
       " 'iis': 368,\n",
       " 'ansible': 369,\n",
       " 'project leadership': 370,\n",
       " 'creative thinking': 371,\n",
       " 'machine learning': 372,\n",
       " 'artificial intelligence': 373,\n",
       " 'virtual assistants': 374,\n",
       " 'blockchain': 375,\n",
       " 'human resources': 376,\n",
       " 'predictive analytics': 377,\n",
       " 'prescriptive analytics': 378,\n",
       " 'r': 379,\n",
       " 'python': 380,\n",
       " 'matlab': 381,\n",
       " 'mathematics': 382,\n",
       " 'monte carlo simulations': 383,\n",
       " 'simulation': 384,\n",
       " 'catastrophe modeling': 385,\n",
       " 'data mining': 386,\n",
       " 'pl/sql': 387,\n",
       " 'sql*plus': 388,\n",
       " 'oracle forms': 389,\n",
       " 'java': 390,\n",
       " 'css': 391,\n",
       " 'javascript': 392,\n",
       " 'perl': 393,\n",
       " 'html': 394,\n",
       " 'http': 395,\n",
       " 'apis': 396,\n",
       " 'system development life cycle (sdlc)': 397,\n",
       " 'complex principles and procedures of computer systems': 398,\n",
       " 'complex software applications': 399,\n",
       " 'hardware': 400,\n",
       " 'telecommunications': 401,\n",
       " 'networking principles': 402,\n",
       " 'relational database concepts': 403,\n",
       " 'design techniques': 404,\n",
       " 'tools': 405,\n",
       " 'computer file methods': 406,\n",
       " 'structured testing techniques': 407,\n",
       " 'objectoriented software development techniques': 408,\n",
       " 'interpersonal skills': 409,\n",
       " 'training skills': 410,\n",
       " 'ability to work independently': 411,\n",
       " 'ability to work in a team': 412,\n",
       " 'ellucian banner': 413,\n",
       " 'system development life cycles (sdlc)': 414,\n",
       " 'objectoriented software development': 415,\n",
       " 'technical writing skills': 416,\n",
       " 'database design': 417,\n",
       " 'er modeling tools': 418,\n",
       " 'capital projects': 419,\n",
       " 'cost allocation': 420,\n",
       " 'debt financing': 421,\n",
       " 'development impact fees': 422,\n",
       " 'economic analysis': 423,\n",
       " 'fiscal information': 424,\n",
       " 'governmental accounting': 425,\n",
       " 'grant accounting': 426,\n",
       " 'grant management': 427,\n",
       " 'infrastructure funding': 428,\n",
       " 'principales of capital planning': 429,\n",
       " 'public administration': 430,\n",
       " 'supervision': 431,\n",
       " 'team leadership': 432,\n",
       " 'training': 433,\n",
       " 'microleadership': 434,\n",
       " 'service learning': 435,\n",
       " 'multidisciplinary collaboration': 436,\n",
       " 'human resource business partners': 437,\n",
       " 'statistical methods': 438,\n",
       " 'business degree': 439,\n",
       " 'social innovation': 440,\n",
       " 'massively multidisciplinary collaboration': 441,\n",
       " 'insurance': 442,\n",
       " 'technical research': 443,\n",
       " 'environmental research': 444,\n",
       " 'business research': 445,\n",
       " 'insight generation': 446,\n",
       " 'expectations': 447,\n",
       " 'crossfunctional work': 448,\n",
       " 'curiosity': 449,\n",
       " 'creativity': 450,\n",
       " 'independence': 451,\n",
       " 'data identification': 452,\n",
       " 'solution development': 453,\n",
       " 'citation skills': 454,\n",
       " 'project managers': 455,\n",
       " 'biometric data analysis': 456,\n",
       " 'data security': 457,\n",
       " 'data privacy': 458,\n",
       " 'healthcare operations': 459,\n",
       " 'healthcare systems': 460,\n",
       " 'recordkeeping processes': 461,\n",
       " 'data preprocessing': 462,\n",
       " 'model building': 463,\n",
       " 'data presentation': 464,\n",
       " 'budgeting reports': 465,\n",
       " 'data communication': 466,\n",
       " 'datadriven decision making': 467,\n",
       " 'healthcare quality improvement': 468,\n",
       " 'cost reduction': 469,\n",
       " 'enterprise data warehouse (edw)': 470,\n",
       " 'social enterprise': 471,\n",
       " 'systemic change': 472,\n",
       " 'marketing analysis': 473,\n",
       " 'tableau': 474,\n",
       " 'power bi': 475,\n",
       " 'excel bi': 476,\n",
       " 'dashboard development': 477,\n",
       " 'descriptive analytics': 478,\n",
       " 'charting': 479,\n",
       " 'graphing': 480,\n",
       " 'exploratory analysis': 481,\n",
       " 'gis data visualization': 482,\n",
       " 'mapping': 483,\n",
       " 'crowdsourcing': 484,\n",
       " 'project finance': 485,\n",
       " 'budget variance analysis': 486,\n",
       " 'estimate to complete': 487,\n",
       " 'estimate at complete': 488,\n",
       " 'budget baseline analysis': 489,\n",
       " 'sales forecasting': 490,\n",
       " 'contract reconciliation': 491,\n",
       " 'revenue recognition': 492,\n",
       " 'percent complete': 493,\n",
       " 'cdrls': 494,\n",
       " 'microsoft power point': 495,\n",
       " 'microsoft word': 496,\n",
       " 'iso': 497,\n",
       " 'as9100': 498,\n",
       " 'erp/mrp software': 499,\n",
       " 'microsoft power bi': 500,\n",
       " 'dax': 501,\n",
       " 'power query': 502,\n",
       " 'sql server': 503,\n",
       " 'power pi': 504,\n",
       " 'microsoft office (outlook excel powerpoint teams)': 505,\n",
       " 'erp/crm systems': 506,\n",
       " 'business process automation': 507,\n",
       " 'data aggregation': 508,\n",
       " 'data interpretation': 509,\n",
       " 'strategic planning': 510,\n",
       " 'tactical planning': 511,\n",
       " 'qlikview': 512,\n",
       " 'hadoop': 513,\n",
       " 'spark': 514,\n",
       " 'nosql': 515,\n",
       " 'cloud computing': 516,\n",
       " 'agile development': 517,\n",
       " 'microsoft power bi desktop': 518,\n",
       " 'dax expressions and queries': 519,\n",
       " 'power bi (dax m formula languages)': 520,\n",
       " 'english': 521,\n",
       " 'chinesemandarin': 522,\n",
       " 'system reporting tools': 523,\n",
       " 'dashboards': 524,\n",
       " 'power pivot': 525,\n",
       " 'analytical thinking': 526,\n",
       " 'model n': 527,\n",
       " 'salesforce': 528,\n",
       " 'm formula languages': 529,\n",
       " 'api’s': 530,\n",
       " 'sharepoint design': 531,\n",
       " 'salesforce.com': 532,\n",
       " 'teams': 533,\n",
       " 'sales/marketing': 534,\n",
       " 'program development': 535,\n",
       " 'database schema design': 536,\n",
       " 'relational database': 537,\n",
       " 'er': 538,\n",
       " 'structured query language': 539,\n",
       " 'adobe photoshop': 540,\n",
       " 'objectoriented programming': 541,\n",
       " 'database schemas': 542,\n",
       " 'triggers': 543,\n",
       " 'ivr': 544,\n",
       " 'contact center technologies': 545,\n",
       " 'pega': 546,\n",
       " 'jira': 547,\n",
       " 'confluence': 548,\n",
       " 'business requirements': 549,\n",
       " 'business use cases': 550,\n",
       " 'process models': 551,\n",
       " 'business needs analysis': 552,\n",
       " 'conceptual design documents': 553,\n",
       " 'functional design': 554,\n",
       " 'workflow analysis': 555,\n",
       " 'use case analysis': 556,\n",
       " 'process modeling': 557,\n",
       " 'testing discipline': 558,\n",
       " 'test planning': 559,\n",
       " 'incident research and analysis': 560,\n",
       " 'test strategy': 561,\n",
       " 'qa testing': 562,\n",
       " 'data properties': 563,\n",
       " 'feasibility analysis': 564,\n",
       " 'meeting facilitation': 565,\n",
       " 'management and executive reporting': 566,\n",
       " 'change request management': 567,\n",
       " 'issue tracking': 568,\n",
       " 'process improvements': 569,\n",
       " 'project planning': 570,\n",
       " 'issue resolution': 571,\n",
       " 'break and hot fix support': 572,\n",
       " 'triage': 573,\n",
       " 'corrective action': 574,\n",
       " 'sql queries': 575,\n",
       " 'reports': 576,\n",
       " 'strong communication skills': 577,\n",
       " 'strong organizational skills': 578,\n",
       " 'excellence': 579,\n",
       " 'influence': 580,\n",
       " 'team and interpersonal skills': 581,\n",
       " 'process management': 582,\n",
       " 'stakeholder understanding': 583,\n",
       " 'decisionmaking skills': 584,\n",
       " 'predictive insights': 585,\n",
       " 'conflict resolution': 586,\n",
       " 'assumption resolution': 587,\n",
       " 'requirement summarization': 588,\n",
       " 'research skills': 589,\n",
       " 'network infrastructure': 590,\n",
       " 'cisco systems': 591,\n",
       " 'hp/aruba': 592,\n",
       " 'wan/lan': 593,\n",
       " 'routing and switching': 594,\n",
       " 'carrier transmission systems': 595,\n",
       " 'voip': 596,\n",
       " 'wlans/wifi': 597,\n",
       " 'cabling infrastructure': 598,\n",
       " 'business/cost analysis': 599,\n",
       " 'vendor and partner management': 600,\n",
       " 'ip video systems': 601,\n",
       " 'cloud networking infrastructure': 602,\n",
       " 'data integrity': 603,\n",
       " 'data visual': 604,\n",
       " 'program management': 605,\n",
       " 'microsoft access': 606,\n",
       " 'google workspace': 607,\n",
       " 'business operations applications': 608,\n",
       " 'system review': 609,\n",
       " 'technical support': 610,\n",
       " 'process review': 611,\n",
       " 'process redesign': 612,\n",
       " 'agent desktop': 613,\n",
       " 'case management tools': 614,\n",
       " 'use case scenarios': 615,\n",
       " 'business process modeling': 616,\n",
       " 'qa': 617,\n",
       " 'system behavior': 618,\n",
       " 'estimating': 619,\n",
       " 'incident identification and resolution': 620,\n",
       " 'documentation maintenance': 621,\n",
       " 'traceability': 622,\n",
       " 'customer needs and strategies': 623,\n",
       " 'formal and informal written communication': 624,\n",
       " 'meetings and presentations': 625,\n",
       " 'customer experience': 626,\n",
       " 'interpersonal and communication skills': 627,\n",
       " 'resourcefulness': 628,\n",
       " 'change champion': 629,\n",
       " 'process and best practices': 630,\n",
       " 'leading and mentoring': 631,\n",
       " 'project management skills': 632,\n",
       " 'data interpretation skills': 633,\n",
       " 'stakeholder needs': 634,\n",
       " 'innovative solutions': 635,\n",
       " 'system and process enhancements': 636,\n",
       " 'historical trends': 637,\n",
       " 'risk resolution': 638,\n",
       " 'requirements summarization': 639,\n",
       " 'business goals': 640,\n",
       " 'impact analysis': 641,\n",
       " 'dynamics 365 business central': 642,\n",
       " 'microsoft dynamics nav': 643,\n",
       " 'jet reports': 644,\n",
       " 'production processes': 645,\n",
       " 'assembly line processes': 646,\n",
       " 'inventory': 647,\n",
       " 'warehousing': 648,\n",
       " 'case management systems': 649,\n",
       " 'remote assistance tools': 650,\n",
       " 'content management systems (cms)': 651,\n",
       " 'agile methodology': 652,\n",
       " 'ux best practices': 653,\n",
       " 'accessibility': 654,\n",
       " 'search engine marketing (sem)': 655,\n",
       " 'optimization': 656,\n",
       " 'user manuals': 657,\n",
       " 'handbooks': 658,\n",
       " 'workflow diagrams': 659,\n",
       " 'activity diagrams': 660,\n",
       " 'cost benefit analysis': 661,\n",
       " 'risk analysis': 662,\n",
       " 'data gathering': 663,\n",
       " 'research & analytics': 664,\n",
       " 'problem identification': 665,\n",
       " 'solution recommendations': 666,\n",
       " 'it systems': 667,\n",
       " 'business process analytics': 668,\n",
       " 'financial modeling': 669,\n",
       " 'spreadsheet software': 670,\n",
       " 'ms excel': 671,\n",
       " 'presentations': 672,\n",
       " \"master's degree\": 673,\n",
       " 'data profiling': 674,\n",
       " 'data cleansing': 675,\n",
       " 'data manipulation': 676,\n",
       " 'pytest': 677,\n",
       " 'xray': 678,\n",
       " 'zephyr': 679,\n",
       " 'qmetry': 680,\n",
       " 'bitbucket': 681,\n",
       " 'data lineage': 682,\n",
       " 'test management': 683,\n",
       " 'test automation': 684,\n",
       " 'salesforce crm': 685,\n",
       " 'snowflake': 686,\n",
       " 'databricks': 687,\n",
       " 'rihv': 688,\n",
       " 'lean six sigma': 689,\n",
       " 'customer relationship management (crm)': 690,\n",
       " 'financial metrics': 691,\n",
       " 'customer health': 692,\n",
       " 'customer engagement': 693,\n",
       " 'customer adoption': 694,\n",
       " 'revenue growth': 695,\n",
       " 'business process improvement': 696,\n",
       " 'documentation': 697,\n",
       " 'user training': 698,\n",
       " \"bachelor's degree in business economics data analytics statistics\": 699,\n",
       " '4+ years of experience in a business analyst role': 700,\n",
       " 'private equity': 701,\n",
       " 'debt': 702,\n",
       " 'derivatives': 703,\n",
       " 'financial markets': 704,\n",
       " 'pricing': 705,\n",
       " 'valuation': 706,\n",
       " 'models': 707,\n",
       " 'analysis': 708,\n",
       " 'restructurings': 709,\n",
       " 'corporate actions': 710,\n",
       " 'mutual funds': 711,\n",
       " 'cfa level 3': 712,\n",
       " 'investment management': 713,\n",
       " 'investment performance': 714,\n",
       " 'statistical theory': 715,\n",
       " 'financial instruments': 716,\n",
       " 'quantitative techniques': 717,\n",
       " 'mathematical techniques': 718,\n",
       " 'fair valuations': 719,\n",
       " 'risk': 720,\n",
       " 'validation': 721,\n",
       " 'database management': 722,\n",
       " 'bloomberg': 723,\n",
       " 'market data': 724,\n",
       " 'organization': 725,\n",
       " 'accuracy': 726,\n",
       " 'common equity models': 727,\n",
       " 'black scholes': 728,\n",
       " 'binomial trees': 729,\n",
       " 'garch': 730,\n",
       " 'plain vanilla bonds': 731,\n",
       " 'complex securities valuation': 732,\n",
       " 'market data providers': 733,\n",
       " 'sales operations': 734,\n",
       " 'demand forecasting': 735,\n",
       " 'supply allocation': 736,\n",
       " 'demand generation': 737,\n",
       " 'intel foundry services (ifs)': 738,\n",
       " 'market insights': 739,\n",
       " 'strategic marketing': 740,\n",
       " 'planning': 741,\n",
       " 'supply/demand planning': 742,\n",
       " 'forecasting revenue': 743,\n",
       " 'sales enablement': 744,\n",
       " 'operations systems': 745,\n",
       " 'situational analysis': 746,\n",
       " 'organizational management': 747,\n",
       " 'supply chain operations': 748,\n",
       " 'foundry': 749,\n",
       " 'supply planning': 750,\n",
       " 'intel systems': 751,\n",
       " 'demand': 752,\n",
       " 'customer relationship management': 753,\n",
       " 'revenue optimization': 754,\n",
       " 'supply chain management': 755,\n",
       " \"bachelor's degree in business economics or supply chain\": 756,\n",
       " 'surveys': 757,\n",
       " 'legislation': 758,\n",
       " 'filing': 759,\n",
       " 'rate': 760,\n",
       " 'network': 761,\n",
       " 'westlaw': 762,\n",
       " 'insurance terminology': 763,\n",
       " 'proficiency with word excel outlook teams': 764,\n",
       " 'pricing processes': 765,\n",
       " 'compliance': 766,\n",
       " 'contract automation': 767,\n",
       " 'contract management': 768,\n",
       " 'pricing dynamics': 769,\n",
       " 'contract publishing': 770,\n",
       " 'contract tool development': 771,\n",
       " 'user acceptance': 772,\n",
       " 'enterprise systems': 773,\n",
       " 'financial risks': 774,\n",
       " 'process engineering': 775,\n",
       " 'sales and marketing': 776,\n",
       " 'complex sales': 777,\n",
       " 'product roadmaps': 778,\n",
       " 'emerging technology trends': 779,\n",
       " 'us locations': 780,\n",
       " 'requirement analysis': 781,\n",
       " 'business process documentation': 782,\n",
       " 'needs assessment': 783,\n",
       " 'test script mapping': 784,\n",
       " 'professional in business analysis (pmipba)': 785,\n",
       " 'cbap certification': 786,\n",
       " 'pmp': 787,\n",
       " 'data import and data export': 788,\n",
       " 'disaster management': 789,\n",
       " 'cost management': 790,\n",
       " 'emergency management': 791,\n",
       " 'cost recovery': 792,\n",
       " 'cost tracking': 793,\n",
       " 'demand planning': 794,\n",
       " 'process analysis': 795,\n",
       " 'business transformation': 796,\n",
       " 'collaborative process design': 797,\n",
       " 'information design': 798,\n",
       " 'planning domain': 799,\n",
       " 'industrystandard processes': 800,\n",
       " 'software development life cycle': 801,\n",
       " 'process development': 802,\n",
       " 'industry standard process': 803,\n",
       " 'business measures': 804,\n",
       " 'statistical analytics': 805,\n",
       " 'algorithmic analytics': 806,\n",
       " 'time series forecasting': 807,\n",
       " 'blue yonder': 808,\n",
       " 'erp': 809,\n",
       " 'project coordination': 810,\n",
       " 'computer skills': 811,\n",
       " 'data analyst': 812,\n",
       " 'vlookup': 813,\n",
       " 'data models': 814,\n",
       " 'datasets': 815,\n",
       " 'data figures': 816,\n",
       " 'descriptive analysis': 817,\n",
       " 'inferential analysis': 818,\n",
       " 'predictive analysis': 819,\n",
       " 'prescriptive analysis': 820,\n",
       " 'sybase': 821,\n",
       " 'db2': 822,\n",
       " 'ecda': 823,\n",
       " 'database requirements': 824,\n",
       " 'api integrations': 825,\n",
       " 'software systems': 826,\n",
       " 'modeling applications': 827,\n",
       " 'api documentation': 828,\n",
       " 'debugging': 829,\n",
       " 'integration issues': 830,\n",
       " 'industry trends': 831,\n",
       " 'database reporting': 832,\n",
       " 'data discovery': 833,\n",
       " 'decisionmaking': 834,\n",
       " 'crystal reports': 835,\n",
       " 'ssrs': 836,\n",
       " 'data trends': 837,\n",
       " 'data relationships': 838,\n",
       " 'conclusions': 839,\n",
       " 'sas statistical software': 840,\n",
       " 'aws': 841,\n",
       " 'redshift': 842,\n",
       " 's3 bucket': 843,\n",
       " 'machine learning methods': 844,\n",
       " 'data management schemas': 845,\n",
       " 'predictive modeling': 846,\n",
       " 'data quality': 847,\n",
       " 'business value': 848,\n",
       " 'maintenance metrics': 849,\n",
       " 'advanced analytics': 850,\n",
       " 'leadership briefs': 851,\n",
       " 'productivity improvement': 852,\n",
       " 'business processes': 853,\n",
       " 'servicenow': 854,\n",
       " 'it management systems': 855,\n",
       " 'statistical methodologies': 856,\n",
       " 'itil v4 foundations': 857,\n",
       " 'iso 20k': 858,\n",
       " 'text data labeling': 859,\n",
       " 'data labeling': 860,\n",
       " 'basic queries': 861,\n",
       " 'data labeling tools': 862,\n",
       " 'call transcripts': 863,\n",
       " 'text messages': 864,\n",
       " 'retail banking': 865,\n",
       " 'financial services': 866,\n",
       " 'oracle developer suite': 867,\n",
       " 'asp .net': 868,\n",
       " 'coldfusion': 869,\n",
       " 'c#': 870,\n",
       " 'office': 871,\n",
       " 'project': 872,\n",
       " 'adobe captivate': 873,\n",
       " 'lexis casesoft suite': 874,\n",
       " 'microsoft dynamics crm': 875,\n",
       " 'web development': 876,\n",
       " 'web design': 877,\n",
       " 'information architecture': 878,\n",
       " 'user experience': 879,\n",
       " 'sharepoint development': 880,\n",
       " 'office development': 881,\n",
       " 'excel development': 882,\n",
       " 'visio development': 883,\n",
       " 'project development': 884,\n",
       " 'database normalization': 885,\n",
       " 'entityrelationship diagrams': 886,\n",
       " 'data scrubbing': 887,\n",
       " 'intelligence analysis': 888,\n",
       " 'genomics': 889,\n",
       " 'bioinformatics': 890,\n",
       " 'data visualizations': 891,\n",
       " 'phylogenetic': 892,\n",
       " 'c++': 893,\n",
       " 'bash': 894,\n",
       " 'hive': 895,\n",
       " 'pig': 896,\n",
       " 'big data': 897,\n",
       " 'highperformance computing (hpc)': 898,\n",
       " 'algorithm development': 899,\n",
       " 'data editing': 900,\n",
       " 'data auditing': 901,\n",
       " 'data quality control': 902,\n",
       " 'multiomics': 903,\n",
       " 'genome assembly': 904,\n",
       " 'bacterial annotation': 905,\n",
       " 'microbial sequencing': 906,\n",
       " 'publications': 907,\n",
       " 'data representation': 908,\n",
       " 'databases': 909,\n",
       " 'business intelligence (bi)': 910,\n",
       " 'microsoft sql server': 911,\n",
       " 'travel': 912,\n",
       " 'google': 913,\n",
       " 'facebook': 914,\n",
       " 'ctrip': 915,\n",
       " 'trip.com': 916,\n",
       " 'makemytrip': 917,\n",
       " 'grab': 918,\n",
       " 'amazon': 919,\n",
       " 'pandas': 920,\n",
       " 'artificial intelligence (ai)': 921,\n",
       " 'capital one': 922,\n",
       " 'accenture': 923,\n",
       " 'upwork': 924,\n",
       " 'deloitte': 925,\n",
       " 'mckinsey': 926,\n",
       " 'bain': 927,\n",
       " 'microsoft': 928,\n",
       " 'uber': 929,\n",
       " 'lyft': 930,\n",
       " 'gojek': 931,\n",
       " 'lazada': 932,\n",
       " 'alibaba': 933,\n",
       " 'shopify': 934,\n",
       " 'expedia': 935,\n",
       " 'skyscanner': 936,\n",
       " 'allsource analyst': 937,\n",
       " 'opensource media': 938,\n",
       " 'news reports': 939,\n",
       " 'social media': 940,\n",
       " 'classified materials': 941,\n",
       " 'unclassified materials': 942,\n",
       " 'country analysis': 943,\n",
       " 'regional analysis': 944,\n",
       " 'threat assessments': 945,\n",
       " 'strategic assessments': 946,\n",
       " 'intelligence assessments': 947,\n",
       " 'intelligence products': 948,\n",
       " 'collection managers': 949,\n",
       " 'targets': 950,\n",
       " 'policymakers': 951,\n",
       " 'leaders': 952,\n",
       " 'political science': 953,\n",
       " 'public policy': 954,\n",
       " 'cybersecurity': 955,\n",
       " 'national security': 956,\n",
       " 'top secret': 957,\n",
       " 'sci clearance': 958,\n",
       " 'us citizenship': 959,\n",
       " 'pattern recognition': 960,\n",
       " 'trend identification': 961,\n",
       " 'threat identification': 962,\n",
       " 'actionable indicators': 963,\n",
       " 'written assessments': 964,\n",
       " 'oral assessments': 965,\n",
       " 'intelligence community databases': 966,\n",
       " 'intelligence tools': 967,\n",
       " 'intelligence techniques': 968,\n",
       " 'foia xpress': 969,\n",
       " 'federal web based systems': 970,\n",
       " 'foia exemptions': 971,\n",
       " 'foia request processing': 972,\n",
       " 'foia request marking and redactions': 973,\n",
       " 'foia information segregation': 974,\n",
       " 'document redactions': 975,\n",
       " 'information policy’s guidance': 976,\n",
       " 'records assessment': 977,\n",
       " 'foia requests completion': 978,\n",
       " 'foia guidelines': 979,\n",
       " 'frma': 980,\n",
       " 'departmental regulations and issuances': 981,\n",
       " 'organizational and departmental policy statements': 982,\n",
       " 'complex information management statutes': 983,\n",
       " 'foia request data entry': 984,\n",
       " 'draft acknowledgement letters': 985,\n",
       " 'records redaction': 986,\n",
       " 'juniorlevel analyst work knowledge base': 987,\n",
       " 'phylogenetics': 988,\n",
       " 'metagenomics': 989,\n",
       " 'metatranscriptomics': 990,\n",
       " 'bash script': 991,\n",
       " 'hpc': 992,\n",
       " 'pipelines': 993,\n",
       " 'attack the network tool suite': 994,\n",
       " 'regex': 995,\n",
       " 'api requests': 996,\n",
       " 'geoint tools': 997,\n",
       " 'jupyterhub': 998,\n",
       " 'jupyter notebook': 999,\n",
       " 'json': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that our obj `Pipeline()` is actually saving the newly scanned skills\n",
    "pipeline.unique_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce8ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.unique_skills['business analyst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83b80d",
   "metadata": {},
   "source": [
    "Convert the current values in the dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480fb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_postings['skill_tabulated'] = analyst_postings['job_skills'].apply(pipeline.convert_skill_to_tabulated_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d942df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>skill_tabulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst-SQL, Tableau</td>\n",
       "      <td>Zortech Solutions</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>https://www.linkedin.com/jobs/data-analyst-jobs</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market Research &amp; Insights Analyst</td>\n",
       "      <td>Indiana University Foundation</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/market-rese...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Company Description\\nAre you a high-performer ...</td>\n",
       "      <td>Data analysis, Market research, Survey develop...</td>\n",
       "      <td>1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Systems Analyst `1</td>\n",
       "      <td>Cook Medical</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/business-sy...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Overview\\nThe Business Systems Analyst 1 perfo...</td>\n",
       "      <td>Business Analysis, Technical Writing, Software...</td>\n",
       "      <td>24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior VAT and Indirect Tax Analyst</td>\n",
       "      <td>Epic</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-vat-...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>We're looking for an experienced tax professio...</td>\n",
       "      <td>Accounting, Finance, VAT/GST tax regimes, US a...</td>\n",
       "      <td>36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior HRIS Analyst (Timekeeping and Payroll)</td>\n",
       "      <td>Nordson Corporation</td>\n",
       "      <td>Greater Bloomington Area</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-hris...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Collaboration drives Nordson’s success as a ma...</td>\n",
       "      <td>Workday HCM, UKG Dimensions, Ceridian Dayforce...</td>\n",
       "      <td>72, 73, 74, 75, 8, 76, 77, 78, 79, 80, 81, 82,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       job_title  \\\n",
       "0                      Data Analyst-SQL, Tableau   \n",
       "1             Market Research & Insights Analyst   \n",
       "2                    Business Systems Analyst `1   \n",
       "3            Senior VAT and Indirect Tax Analyst   \n",
       "4  Senior HRIS Analyst (Timekeeping and Payroll)   \n",
       "\n",
       "                         company              job_location  \\\n",
       "0              Zortech Solutions         Mountain View, CA   \n",
       "1  Indiana University Foundation           Bloomington, IN   \n",
       "2                   Cook Medical           Bloomington, IN   \n",
       "3                           Epic           Bloomington, IN   \n",
       "4            Nordson Corporation  Greater Bloomington Area   \n",
       "\n",
       "                                            job_link  first_seen  search_city  \\\n",
       "0    https://www.linkedin.com/jobs/data-analyst-jobs  2023-12-20  Bloomington   \n",
       "1  https://www.linkedin.com/jobs/view/market-rese...  2023-12-20  Bloomington   \n",
       "2  https://www.linkedin.com/jobs/view/business-sy...  2023-12-20  Bloomington   \n",
       "3  https://www.linkedin.com/jobs/view/senior-vat-...  2023-12-20  Bloomington   \n",
       "4  https://www.linkedin.com/jobs/view/senior-hris...  2023-12-20  Bloomington   \n",
       "\n",
       "  search_country   job level job_type  \\\n",
       "0  United States   Associate   Onsite   \n",
       "1  United States  Mid senior   Onsite   \n",
       "2  United States  Mid senior   Onsite   \n",
       "3  United States  Mid senior   Onsite   \n",
       "4  United States  Mid senior   Remote   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0                                                NaN   \n",
       "1  Company Description\\nAre you a high-performer ...   \n",
       "2  Overview\\nThe Business Systems Analyst 1 perfo...   \n",
       "3  We're looking for an experienced tax professio...   \n",
       "4  Collaboration drives Nordson’s success as a ma...   \n",
       "\n",
       "                                          job_skills  \\\n",
       "0                                                NaN   \n",
       "1  Data analysis, Market research, Survey develop...   \n",
       "2  Business Analysis, Technical Writing, Software...   \n",
       "3  Accounting, Finance, VAT/GST tax regimes, US a...   \n",
       "4  Workday HCM, UKG Dimensions, Ceridian Dayforce...   \n",
       "\n",
       "                                     skill_tabulated  \n",
       "0                                               None  \n",
       "1  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,...  \n",
       "2     24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35  \n",
       "3  36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47...  \n",
       "4  72, 73, 74, 75, 8, 76, 77, 78, 79, 80, 81, 82,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_postings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75f375",
   "metadata": {},
   "source": [
    "### Geo-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c9ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 139/12894 [02:19<4:34:28,  1.29s/it]RateLimiter caught an error, retrying (0/2 tries). Called with (*('Maryland, United States',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  5%|▍         | 603/12894 [10:46<3:26:44,  1.01s/it] RateLimiter caught an error, retrying (0/2 tries). Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  5%|▍         | 613/12894 [11:18<4:18:34,  1.26s/it] RateLimiter caught an error, retrying (0/2 tries). Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  6%|▌         | 714/12894 [13:19<3:47:18,  1.12s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TEST Analyst positings\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analyst_postings[\u001b[33m'\u001b[39m\u001b[33mcoordinates\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43manalyst_postings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjob_location\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\tqdm\\std.py:917\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n",
      "\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n",
      "\u001b[32m    916\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    918\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[32m    919\u001b[39m     t.close()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n",
      "\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n",
      "\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m   4802\u001b[39m     func: AggFuncType,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n",
      "\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n",
      "\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n",
      "\u001b[32m   4811\u001b[39m \n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n",
      "\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n",
      "\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n",
      "\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n",
      "\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n",
      "\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n",
      "\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n",
      "\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n",
      "\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n",
      "\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n",
      "\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n",
      "\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n",
      "\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n",
      "\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n",
      "\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n",
      "\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n",
      "\u001b[32m   1747\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\tqdm\\std.py:912\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n",
      "\u001b[32m    908\u001b[39m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n",
      "\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n",
      "\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n",
      "\u001b[32m    911\u001b[39m     t.update(n=\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.total \u001b[38;5;129;01mor\u001b[39;00m t.n < t.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TEST Analyst positings\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analyst_postings[\u001b[33m'\u001b[39m\u001b[33mcoordinates\u001b[39m\u001b[33m'\u001b[39m] = analyst_postings[\u001b[33m'\u001b[39m\u001b[33mjob_location\u001b[39m\u001b[33m'\u001b[39m].progress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\src\\components\\pipeline.py:151\u001b[39m, in \u001b[36mPipeline.get_coordinates\u001b[39m\u001b[34m(self, location_str)\u001b[39m\n",
      "\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, location_str):\n",
      "\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# We need to extract the tuple coordinates from geopy, so we will use \u001b[39;00m\n",
      "\u001b[32m    148\u001b[39m     \u001b[38;5;66;03m# a function to explicitly extract it out. The general .apply() function\u001b[39;00m\n",
      "\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# will not work. \u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     location = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeocode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m location:\n",
      "\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation.latitude\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation.longitude\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:272\u001b[39m, in \u001b[36mRateLimiter.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m    270\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._retries_gen(args, kwargs)\n",
      "\u001b[32m    271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m gen:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_request_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    274\u001b[39m         res = \u001b[38;5;28mself\u001b[39m.func(*args, **kwargs)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:267\u001b[39m, in \u001b[36mRateLimiter._acquire_request_slot\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire_request_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m wait \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acquire_request_slot_gen():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:263\u001b[39m, in \u001b[36mRateLimiter._sleep\u001b[39m\u001b[34m(self, seconds)\u001b[39m\n",
      "\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sleep\u001b[39m(\u001b[38;5;28mself\u001b[39m, seconds):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[32m    262\u001b[39m     logger.debug(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m sleep(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, seconds)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "analyst_postings_locations_norm = pipeline.get_unique_locations(analyst_postings, 'job_location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35b18d",
   "metadata": {},
   "source": [
    "### Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8236ec4",
   "metadata": {},
   "source": [
    "We also want to extract the `salary` from the postings as well. Look into the csv for a manual validation if data within the job description contains salary based values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a362274",
   "metadata": {},
   "source": [
    "So the issue now is to extract the salaries from the text description. We can use regular expression, but its hard to capture EVERY single pattern that is possible from each job description. So what we'll do instead is to use ChatGPT to extract the tokens using Agentic AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aad325",
   "metadata": {},
   "source": [
    "An issue that comes across using this method is of currency. A job posting may be from Canada but posted in $USD. This process assumes that the salary provided is that of the country. So if the `job_location` & `search_country` match with their countries, then the job posting's salary is posted as the country of origin's currency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2e9876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:15<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "analyst_postings['salary'] = analyst_postings.progress_apply(pipeline.extract_salary_from_job_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e206a5",
   "metadata": {},
   "source": [
    "We need to extract the coordinates out from the location description of the job postings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882675b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_postings['avg_salary'] = analyst_postings.progress_apply(pipeline.create_avg_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6080025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 139/12894 [02:19<4:34:28,  1.29s/it]RateLimiter caught an error, retrying (0/2 tries). Called with (*('Maryland, United States',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Maryland%2C+United+States&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  5%|▍         | 603/12894 [10:46<3:26:44,  1.01s/it] RateLimiter caught an error, retrying (0/2 tries). Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('Lynn, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Lynn%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  5%|▍         | 613/12894 [11:18<4:18:34,  1.26s/it] RateLimiter caught an error, retrying (0/2 tries). Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('Arlington, MA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 367, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 482, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "    ^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\geocoders\\base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 472, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\adapters.py\", line 494, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Arlington%2C+MA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "  6%|▌         | 714/12894 [13:19<3:47:18,  1.12s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TEST Analyst positings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analyst_postings[\u001b[33m'\u001b[39m\u001b[33mcoordinates\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43manalyst_postings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjob_location\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\tqdm\\std.py:917\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    919\u001b[39m     t.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\tqdm\\std.py:912\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[32m    908\u001b[39m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[32m    911\u001b[39m     t.update(n=\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.total \u001b[38;5;129;01mor\u001b[39;00m t.n < t.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TEST Analyst positings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analyst_postings[\u001b[33m'\u001b[39m\u001b[33mcoordinates\u001b[39m\u001b[33m'\u001b[39m] = analyst_postings[\u001b[33m'\u001b[39m\u001b[33mjob_location\u001b[39m\u001b[33m'\u001b[39m].progress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\src\\components\\pipeline.py:151\u001b[39m, in \u001b[36mPipeline.get_coordinates\u001b[39m\u001b[34m(self, location_str)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, location_str):\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# We need to extract the tuple coordinates from geopy, so we will use \u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;66;03m# a function to explicitly extract it out. The general .apply() function\u001b[39;00m\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# will not work. \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     location = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeocode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m location:\n\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation.latitude\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation.longitude\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:272\u001b[39m, in \u001b[36mRateLimiter.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._retries_gen(args, kwargs)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_request_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m         res = \u001b[38;5;28mself\u001b[39m.func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:267\u001b[39m, in \u001b[36mRateLimiter._acquire_request_slot\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire_request_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m wait \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acquire_request_slot_gen():\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sumag\\Documents\\Programming\\Data Science\\analysisOfJobsInData\\venv\\Lib\\site-packages\\geopy\\extra\\rate_limiter.py:263\u001b[39m, in \u001b[36mRateLimiter._sleep\u001b[39m\u001b[34m(self, seconds)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sleep\u001b[39m(\u001b[38;5;28mself\u001b[39m, seconds):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    262\u001b[39m     logger.debug(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m sleep(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, seconds)\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# TEST Analyst positings\n",
    "analyst_postings['coordinates'] = analyst_postings['job_location'].progress_apply(lambda x: pipeline.get_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc0b222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>skill_tabulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst-SQL, Tableau</td>\n",
       "      <td>Zortech Solutions</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>https://www.linkedin.com/jobs/data-analyst-jobs</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market Research &amp; Insights Analyst</td>\n",
       "      <td>Indiana University Foundation</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/market-rese...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Company Description\\nAre you a high-performer ...</td>\n",
       "      <td>Data analysis, Market research, Survey develop...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Systems Analyst `1</td>\n",
       "      <td>Cook Medical</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/business-sy...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Overview\\nThe Business Systems Analyst 1 perfo...</td>\n",
       "      <td>Business Analysis, Technical Writing, Software...</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior VAT and Indirect Tax Analyst</td>\n",
       "      <td>Epic</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-vat-...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>We're looking for an experienced tax professio...</td>\n",
       "      <td>Accounting, Finance, VAT/GST tax regimes, US a...</td>\n",
       "      <td>[36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior HRIS Analyst (Timekeeping and Payroll)</td>\n",
       "      <td>Nordson Corporation</td>\n",
       "      <td>Greater Bloomington Area</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-hris...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Bloomington</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Collaboration drives Nordson’s success as a ma...</td>\n",
       "      <td>Workday HCM, UKG Dimensions, Ceridian Dayforce...</td>\n",
       "      <td>[72, 73, 74, 75, 8, 76, 77, 78, 79, 80, 81, 82...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12889</th>\n",
       "      <td>Business Operations Analyst</td>\n",
       "      <td>LMI Aerospace - A Member of the Sonaca Group</td>\n",
       "      <td>Washington, MO</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/business-op...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Atchison</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Who Are We\\nSonaca North America\\nis driven by...</td>\n",
       "      <td>Data Analysis, Business Intelligence, Reportin...</td>\n",
       "      <td>[1, 103, 75, 2958, 233, 104, 224, 252, 46511, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12890</th>\n",
       "      <td>Senior Military Analyst</td>\n",
       "      <td>Trideum Corporation</td>\n",
       "      <td>Leavenworth, KS</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mili...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Atchison</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Full-time\\nFort Leavenworth, KS\\nAbout Us\\nTri...</td>\n",
       "      <td>Microsoft Office Suite, Outlook, Word, Excel, ...</td>\n",
       "      <td>[271, 1664, 17, 16, 18, 5632, 46512, 46513, 1478]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12891</th>\n",
       "      <td>Senior Military Systems Analyst to support the...</td>\n",
       "      <td>Nemean Solutions, LLC</td>\n",
       "      <td>Leavenworth, KS</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mili...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Atchison</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Nemean Solutions, LLC is looking for a Senior ...</td>\n",
       "      <td>C2 Warfighting Function, Echelons Above Brigad...</td>\n",
       "      <td>[46514, 46515, 46516, 46517, 24759, 46518, 230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>Senior Military Analyst to support the Mission...</td>\n",
       "      <td>Nemean Solutions, LLC</td>\n",
       "      <td>Leavenworth, KS</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mili...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Atchison</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Nemean Solutions, LLC is looking for a Senior ...</td>\n",
       "      <td>Military Analyst, Command and Control (C2), Wa...</td>\n",
       "      <td>[46526, 39409, 46527, 46515, 46528, 46518, 465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12893</th>\n",
       "      <td>Senior Military Systems Analyst</td>\n",
       "      <td>Trideum Corporation</td>\n",
       "      <td>Leavenworth, KS</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mili...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Atchison</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Full-time\\nFort Leavenworth, KS\\nAbout Us\\nTri...</td>\n",
       "      <td>Microsoft Office Suite, Outlook, Word, Excel, ...</td>\n",
       "      <td>[271, 1664, 17, 16, 18, 5632, 46513, 1478, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12894 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               job_title  \\\n",
       "0                              Data Analyst-SQL, Tableau   \n",
       "1                     Market Research & Insights Analyst   \n",
       "2                            Business Systems Analyst `1   \n",
       "3                    Senior VAT and Indirect Tax Analyst   \n",
       "4          Senior HRIS Analyst (Timekeeping and Payroll)   \n",
       "...                                                  ...   \n",
       "12889                        Business Operations Analyst   \n",
       "12890                            Senior Military Analyst   \n",
       "12891  Senior Military Systems Analyst to support the...   \n",
       "12892  Senior Military Analyst to support the Mission...   \n",
       "12893                    Senior Military Systems Analyst   \n",
       "\n",
       "                                            company              job_location  \\\n",
       "0                                 Zortech Solutions         Mountain View, CA   \n",
       "1                     Indiana University Foundation           Bloomington, IN   \n",
       "2                                      Cook Medical           Bloomington, IN   \n",
       "3                                              Epic           Bloomington, IN   \n",
       "4                               Nordson Corporation  Greater Bloomington Area   \n",
       "...                                             ...                       ...   \n",
       "12889  LMI Aerospace - A Member of the Sonaca Group            Washington, MO   \n",
       "12890                           Trideum Corporation           Leavenworth, KS   \n",
       "12891                         Nemean Solutions, LLC           Leavenworth, KS   \n",
       "12892                         Nemean Solutions, LLC           Leavenworth, KS   \n",
       "12893                           Trideum Corporation           Leavenworth, KS   \n",
       "\n",
       "                                                job_link  first_seen  \\\n",
       "0        https://www.linkedin.com/jobs/data-analyst-jobs  2023-12-20   \n",
       "1      https://www.linkedin.com/jobs/view/market-rese...  2023-12-20   \n",
       "2      https://www.linkedin.com/jobs/view/business-sy...  2023-12-20   \n",
       "3      https://www.linkedin.com/jobs/view/senior-vat-...  2023-12-20   \n",
       "4      https://www.linkedin.com/jobs/view/senior-hris...  2023-12-20   \n",
       "...                                                  ...         ...   \n",
       "12889  https://www.linkedin.com/jobs/view/business-op...  2023-12-20   \n",
       "12890  https://www.linkedin.com/jobs/view/senior-mili...  2023-12-20   \n",
       "12891  https://www.linkedin.com/jobs/view/senior-mili...  2023-12-20   \n",
       "12892  https://www.linkedin.com/jobs/view/senior-mili...  2023-12-20   \n",
       "12893  https://www.linkedin.com/jobs/view/senior-mili...  2023-12-20   \n",
       "\n",
       "       search_city search_country   job level job_type  \\\n",
       "0      Bloomington  United States   Associate   Onsite   \n",
       "1      Bloomington  United States  Mid senior   Onsite   \n",
       "2      Bloomington  United States  Mid senior   Onsite   \n",
       "3      Bloomington  United States  Mid senior   Onsite   \n",
       "4      Bloomington  United States  Mid senior   Remote   \n",
       "...            ...            ...         ...      ...   \n",
       "12889     Atchison  United States  Mid senior   Onsite   \n",
       "12890     Atchison  United States  Mid senior   Onsite   \n",
       "12891     Atchison  United States  Mid senior   Onsite   \n",
       "12892     Atchison  United States  Mid senior   Onsite   \n",
       "12893     Atchison  United States  Mid senior   Onsite   \n",
       "\n",
       "                                             job_summary  \\\n",
       "0                                                    NaN   \n",
       "1      Company Description\\nAre you a high-performer ...   \n",
       "2      Overview\\nThe Business Systems Analyst 1 perfo...   \n",
       "3      We're looking for an experienced tax professio...   \n",
       "4      Collaboration drives Nordson’s success as a ma...   \n",
       "...                                                  ...   \n",
       "12889  Who Are We\\nSonaca North America\\nis driven by...   \n",
       "12890  Full-time\\nFort Leavenworth, KS\\nAbout Us\\nTri...   \n",
       "12891  Nemean Solutions, LLC is looking for a Senior ...   \n",
       "12892  Nemean Solutions, LLC is looking for a Senior ...   \n",
       "12893  Full-time\\nFort Leavenworth, KS\\nAbout Us\\nTri...   \n",
       "\n",
       "                                              job_skills  \\\n",
       "0                                                    NaN   \n",
       "1      Data analysis, Market research, Survey develop...   \n",
       "2      Business Analysis, Technical Writing, Software...   \n",
       "3      Accounting, Finance, VAT/GST tax regimes, US a...   \n",
       "4      Workday HCM, UKG Dimensions, Ceridian Dayforce...   \n",
       "...                                                  ...   \n",
       "12889  Data Analysis, Business Intelligence, Reportin...   \n",
       "12890  Microsoft Office Suite, Outlook, Word, Excel, ...   \n",
       "12891  C2 Warfighting Function, Echelons Above Brigad...   \n",
       "12892  Military Analyst, Command and Control (C2), Wa...   \n",
       "12893  Microsoft Office Suite, Outlook, Word, Excel, ...   \n",
       "\n",
       "                                         skill_tabulated  \n",
       "0                                                   None  \n",
       "1      [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
       "2       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]  \n",
       "3      [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4...  \n",
       "4      [72, 73, 74, 75, 8, 76, 77, 78, 79, 80, 81, 82...  \n",
       "...                                                  ...  \n",
       "12889  [1, 103, 75, 2958, 233, 104, 224, 252, 46511, ...  \n",
       "12890  [271, 1664, 17, 16, 18, 5632, 46512, 46513, 1478]  \n",
       "12891  [46514, 46515, 46516, 46517, 24759, 46518, 230...  \n",
       "12892  [46526, 39409, 46527, 46515, 46528, 46518, 465...  \n",
       "12893  [271, 1664, 17, 16, 18, 5632, 46513, 1478, 1, ...  \n",
       "\n",
       "[12894 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.13it/s]\n",
      "C:\\Users\\sumag\\AppData\\Local\\Temp\\ipykernel_40420\\146117126.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  analyst_postings_5['coordinates'] = analyst_postings_5['job_location'].progress_apply(pipeline.get_coordinates)\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.20s/it]\n",
      "C:\\Users\\sumag\\AppData\\Local\\Temp\\ipykernel_40420\\146117126.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  analyst_postings_5['Country_ISO_3'] = analyst_postings_5['coordinates'].progress_apply(pipeline.get_country_iso)\n"
     ]
    }
   ],
   "source": [
    "analyst_postings = analyst_postings.head()\n",
    "analyst_postings['coordinates'] = analyst_postings['job_location'].progress_apply(pipeline.get_coordinates)\n",
    "analyst_postings['Country_ISO_3'] = analyst_postings['coordinates'].progress_apply(pipeline.get_country_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f02b9a",
   "metadata": {},
   "source": [
    "## Data Science Postings & Skills (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6547b",
   "metadata": {},
   "source": [
    "Data Science Job Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4a14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12217 entries, 0 to 12216\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   job_link    12217 non-null  object\n",
      " 1   job_skills  12212 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 191.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>Machine Learning, Programming, Python, Scala, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>ETL, Data Integration, Data Transformation, Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>Data Lakes, Data Bricks, Azure Data Factory Pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>Java, Scala, Python, RDBMS, NoSQL, Redshift, S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "                                          job_skills  \n",
       "0  Machine Learning, Programming, Python, Scala, ...  \n",
       "1  C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...  \n",
       "2  ETL, Data Integration, Data Transformation, Da...  \n",
       "3  Data Lakes, Data Bricks, Azure Data Factory Pi...  \n",
       "4  Java, Scala, Python, RDBMS, NoSQL, Redshift, S...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_postings = pd.read_csv('../../datasets/kaggle_asanickza/Data Science Job Postings & Skills (2024)/job_skills.csv')\n",
    "datascience_postings.info()\n",
    "datascience_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57642c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12217 entries, 0 to 12216\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   job_link     12217 non-null  object\n",
      " 1   job_summary  12217 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 191.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>Company Description\\nJobs for Humanity is part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>Who We Are\\nAurora (Nasdaq: AUR) is delivering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>Location: New York City, NY\\nPosition Summary\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>Responsibilities:\\nCandidate must have signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>Dice is the leading career destination for tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "                                         job_summary  \n",
       "0  Company Description\\nJobs for Humanity is part...  \n",
       "1  Who We Are\\nAurora (Nasdaq: AUR) is delivering...  \n",
       "2  Location: New York City, NY\\nPosition Summary\\...  \n",
       "3  Responsibilities:\\nCandidate must have signifi...  \n",
       "4  Dice is the leading career destination for tec...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_postings = pd.read_csv('../../datasets/kaggle_asanickza/Data Science Job Postings & Skills (2024)/job_summary.csv')\n",
    "datascience_postings.info()\n",
    "datascience_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e8af4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12217 entries, 0 to 12216\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   job_link             12217 non-null  object\n",
      " 1   last_processed_time  12217 non-null  object\n",
      " 2   last_status          12217 non-null  object\n",
      " 3   got_summary          12217 non-null  object\n",
      " 4   got_ner              12217 non-null  object\n",
      " 5   is_being_worked      12217 non-null  object\n",
      " 6   job_title            12217 non-null  object\n",
      " 7   company              12217 non-null  object\n",
      " 8   job_location         12216 non-null  object\n",
      " 9   first_seen           12217 non-null  object\n",
      " 10  search_city          12217 non-null  object\n",
      " 11  search_country       12217 non-null  object\n",
      " 12  search_position      12217 non-null  object\n",
      " 13  job_level            12217 non-null  object\n",
      " 14  job_type             12217 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 1.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>last_processed_time</th>\n",
       "      <th>last_status</th>\n",
       "      <th>got_summary</th>\n",
       "      <th>got_ner</th>\n",
       "      <th>is_being_worked</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>search_position</th>\n",
       "      <th>job_level</th>\n",
       "      <th>job_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>2024-01-21 08:08:48.031964+00</td>\n",
       "      <td>Finished NER</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Senior Machine Learning Engineer</td>\n",
       "      <td>Jobs for Humanity</td>\n",
       "      <td>New Haven, CT</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>East Haven</td>\n",
       "      <td>United States</td>\n",
       "      <td>Agricultural-Research Engineer</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>2024-01-20 04:02:12.331406+00</td>\n",
       "      <td>Finished NER</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Principal Software Engineer, ML Accelerators</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>El Cerrito</td>\n",
       "      <td>United States</td>\n",
       "      <td>Set-Key Driver</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>2024-01-21 08:08:31.941595+00</td>\n",
       "      <td>Finished NER</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Senior ETL Data Warehouse Specialist</td>\n",
       "      <td>Adame Services LLC</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>Middletown</td>\n",
       "      <td>United States</td>\n",
       "      <td>Technical Support Specialist</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>2024-01-20 15:30:55.796572+00</td>\n",
       "      <td>Finished NER</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Senior Data Warehouse Developer / Architect</td>\n",
       "      <td>Morph Enterprise</td>\n",
       "      <td>Harrisburg, PA</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>United States</td>\n",
       "      <td>Architect</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>2024-01-21 08:08:58.312124+00</td>\n",
       "      <td>Finished NER</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Dice</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>McKinney</td>\n",
       "      <td>United States</td>\n",
       "      <td>Maintenance Data Analyst</td>\n",
       "      <td>Mid senior</td>\n",
       "      <td>Onsite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "             last_processed_time   last_status got_summary got_ner  \\\n",
       "0  2024-01-21 08:08:48.031964+00  Finished NER           t       t   \n",
       "1  2024-01-20 04:02:12.331406+00  Finished NER           t       t   \n",
       "2  2024-01-21 08:08:31.941595+00  Finished NER           t       t   \n",
       "3  2024-01-20 15:30:55.796572+00  Finished NER           t       t   \n",
       "4  2024-01-21 08:08:58.312124+00  Finished NER           t       t   \n",
       "\n",
       "  is_being_worked                                     job_title  \\\n",
       "0               f              Senior Machine Learning Engineer   \n",
       "1               f  Principal Software Engineer, ML Accelerators   \n",
       "2               f          Senior ETL Data Warehouse Specialist   \n",
       "3               f   Senior Data Warehouse Developer / Architect   \n",
       "4               f                            Lead Data Engineer   \n",
       "\n",
       "              company       job_location  first_seen search_city  \\\n",
       "0   Jobs for Humanity      New Haven, CT  2024-01-14  East Haven   \n",
       "1              Aurora  San Francisco, CA  2024-01-14  El Cerrito   \n",
       "2  Adame Services LLC       New York, NY  2024-01-14  Middletown   \n",
       "3    Morph Enterprise     Harrisburg, PA  2024-01-12     Lebanon   \n",
       "4                Dice          Plano, TX  2024-01-14    McKinney   \n",
       "\n",
       "  search_country                 search_position   job_level job_type  \n",
       "0  United States  Agricultural-Research Engineer  Mid senior   Onsite  \n",
       "1  United States                  Set-Key Driver  Mid senior   Onsite  \n",
       "2  United States    Technical Support Specialist   Associate   Onsite  \n",
       "3  United States                       Architect  Mid senior   Onsite  \n",
       "4  United States        Maintenance Data Analyst  Mid senior   Onsite  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_postings = pd.read_csv('../../datasets/kaggle_asanickza/Data Science Job Postings & Skills (2024)/job_postings.csv')\n",
    "datascience_postings.info()\n",
    "datascience_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b092d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12217 entries, 0 to 12216\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   job_link    12217 non-null  object\n",
      " 1   job_skills  12212 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 191.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>Machine Learning, Programming, Python, Scala, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>ETL, Data Integration, Data Transformation, Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>Data Lakes, Data Bricks, Azure Data Factory Pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>Java, Scala, Python, RDBMS, NoSQL, Redshift, S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "                                          job_skills  \n",
       "0  Machine Learning, Programming, Python, Scala, ...  \n",
       "1  C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...  \n",
       "2  ETL, Data Integration, Data Transformation, Da...  \n",
       "3  Data Lakes, Data Bricks, Azure Data Factory Pi...  \n",
       "4  Java, Scala, Python, RDBMS, NoSQL, Redshift, S...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_skills = pd.read_csv('datasets/kaggle_asanickza/Data Science Job Postings & Skills (2024)/job_skills.csv')\n",
    "datascience_skills.info()\n",
    "datascience_skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46128ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12217 entries, 0 to 12216\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   job_link     12217 non-null  object\n",
      " 1   job_summary  12217 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 191.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>Company Description\\nJobs for Humanity is part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>Who We Are\\nAurora (Nasdaq: AUR) is delivering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>Location: New York City, NY\\nPosition Summary\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>Responsibilities:\\nCandidate must have signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>Dice is the leading career destination for tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "                                         job_summary  \n",
       "0  Company Description\\nJobs for Humanity is part...  \n",
       "1  Who We Are\\nAurora (Nasdaq: AUR) is delivering...  \n",
       "2  Location: New York City, NY\\nPosition Summary\\...  \n",
       "3  Responsibilities:\\nCandidate must have signifi...  \n",
       "4  Dice is the leading career destination for tec...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_summary = pd.read_csv('datasets/kaggle_asanickza/Data Science Job Postings & Skills (2024)/job_summary.csv')\n",
    "datascience_summary.info()\n",
    "datascience_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55c99e",
   "metadata": {},
   "source": [
    "We have to tabulate `job_skills` to be callable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22e0ae",
   "metadata": {},
   "source": [
    "First; check for the unique values within the column we are parsing. We need to check how `None` can be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743ba7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine Learning, Programming, Python, Scala, Java, Data Engineering, Distributed Computing, Statistical Modeling, Optimization, Data Pipelines, Cloud Computing, DevOps, Software Development, Data Gathering, Data Preparation, Data Visualization, Machine Learning Frameworks, scikitlearn, PyTorch, Dask, Spark, TensorFlow, Distributed File Systems, Multi node Database Paradigms, Open Source ML Software, Responsible AI, Explainable AI',\n",
       " 'C++, Python, PyTorch, TensorFlow, MXNet, CUDA, OpenCL, OpenVX, Halide, SIMD programming models, MLspecific accelerators, Linux/unix environments, Deep learning frameworks, Computer vision deep learning models, ML software and hardware technology, Inference on edge platforms, Cloud ML training pipelines, HPC experience, Performance troubleshooting, Profiling, Roofline model, Analytical skills, Communication skills',\n",
       " 'ETL, Data Integration, Data Transformation, Data Warehousing, Business Intelligence, Data Modeling, Data Architecture, Data Quality, Data Validation, Data Cleansing, Performance Optimization, Performance Tuning, Troubleshooting, Documentation, Reporting, Data Analysis, Collaboration, Communication, SQL, Informatica, Talend, Apache NiFi, AWS Redshift, Azure SQL Data Warehouse, Financial/Banking, CloudBased Data Platforms, Regulatory Compliance',\n",
       " \"Data Lakes, Data Bricks, Azure Data Factory Pipelines, Spark, Python, Business Intelligence, Data Warehouse, SQL Server, Azure, ETL/ELT, SQL Server Integration Services, TSQL, Data Formatting, Data Capture, Data Search, Data Retrieval, Data Extraction, Data Classification, Information Filtering, Data Mining Architectures, Modeling Standards, Reporting, Data Analysis Methodologies, Data Engineering, Database File Systems Optimization, API's, Analytics as a Service, Relational Databases, Dimensional Databases, Entity Relationships, Data Warehousing, Facts, Dimensions, Star Schema Concepts, Star Schema Terminology, Project Management, Organizational Skills, Collaboration, Communication, Technical Presentaion Skills, 12+ Years of Relevant Experience\",\n",
       " 'Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Unit testing, Agile engineering, Big data technologies, Cloud computing (AWS Microsoft Azure Google Cloud), Distributed data/computing tools (MapReduce Hadoop Hive EMR Kafka Spark Gurobi MySQL), Realtime data and streaming applications, NoSQL implementation (Mongo Cassandra), Data warehousing (Redshift Snowflake), UNIX/Linux, Shell scripting',\n",
       " 'Data Warehouse (DW), Extract/Transform/Load (ETL), Oracle, VPD, MuleSoft, Cloudera Apache Hadoop Ecosystem, Hadoop, Java, Python, R, AI/ML, Predictive Analytics, Business Objects, Tableau, OBIA/OBIEE, Oracle Cloud Financials, Power Designer, Erwin, UNIX, ODBC, JDBC, Perl DBI, Shell Scripting, PL/SQL, SQL Developer, SQL Plus, SQL Loader, TOAD, Data Profiling, SourceTarget Mapping, Transformations, Business Rules, OWB, ODI, Data Stage, Informatica, SQL, Unix Server, Windows Workstation, Windows Server, Microsoft Office, Excel, Analytic Skills, ProblemSolving, Communication Skills, Teamwork, Accountability, Attention to Detail, Accuracy',\n",
       " 'Machine Learning, Generative AI, Cloudbased services, Risk management, Project management, Process management, Data model, PRIME system, Lean, Green Belt Certification, CRISC, CISM, CRCM, CIPP, ABA Risk Management Certification, Compliance, Legal, Regulatory, Operations, Cybersecurity',\n",
       " 'Data Loss Prevention (DLP), Cybersecurity, Web Proxy, Email, Endpoint Solutions, Symantec Data Loss Prevention (DLP), Agile Delivery, Public Cloud Security, MultiCloud Environments, IT Delivery Projects, JIRA, Cloud Certifications (AWS), Security Certifications (CISSP), Computer Science, Systems Engineering, Software Development Practices, Platform Engineering',\n",
       " \"Problem solving, Data analysis, Predictive analysis, Trend identification, Performance monitoring, Communication, Reporting, Financial risk management, Business process analysis, Excel, Visio, Smartsheet, SNOW, SAP, Tableau, SQL, R, Power BI, Collaboration, Leadership, Bachelor's degree, Professional certification, Graduate degree\",\n",
       " 'Machine Learning, System Design, Multimodal Input, Prototyping, Data Management, Scalability, Reliability, Performance, DevOps, Python, C++, Distributed Systems, Infrastructure Design, Complex Systems, Software Engineering, ML Pipeline Automation, ML Workflows, ML Systems',\n",
       " \"SQL, Database Administration, Advanced Statistical Analysis, Predictive Analytics, Forecasting Techniques, Data Abnormalities Detection, Contact Workforce Management, Crossfunctional Collaboration, Performance Analytics, Business Intelligence, Root Cause Analysis, Forecasting and Scheduling, Service Level Management, Hindsight Analysis, Business Communication, Problemsolving, Decisionmaking, Excel, ACD, BackOffice Work Routing, WFM Software, Bachelor's degree in business accounting finance statistics or economics\",\n",
       " 'Master Data Analyst, Data/Master Data Experience, Polished People/Senior Stakeholder Engagement, Data Cleansing, Data Ingestion Pipelines, Business Process Workflow, Data Matching & Deduplication, Standardization, SemiAgile Delivery Framework, MDM Strategies, MDM Policies, MDM Functional Implementation, MDM Requirement Analysis, Deterministic Matching, Probabilistic Matching, Master Data Hierarchy Management, HighLevel Design, Data Profiling, Data Scorecard Development, Exception Management, LargeScale MDM Project Implementation, Thought Leadership, Data Quality Analysis, Data Governance Policies, Data Transformation, ETL Flows, SourcetoTarget Mapping, Requirements Gathering, Agile/Waterfall SDLC, Time Management, Business Communication, Analytical Skills, ProblemSolving, Informatica MDM/DG, SAP MDM, Data Enrichment, Dun & Bradstreet, SQL',\n",
       " 'Corporate Law, Private Equity, Venture Capital, M&A, Securities Law, SEC Periodic Reporting, Stock Exchange Rules, Corporate Governance, Capital Raising, Mergers and Acquisitions, Equity Offerings, Debt Offerings, Banking and Finance, Litigation, Collaborative Environment, Exchange Regulations',\n",
       " 'SQL, AWS, DBA case tools, SQL Server, SQL programming, Scripts, Structured query language, Algorithmic thinking, Programming languages API, Data backup, Data recovery, Data security, Data integrity, Database standards, End user applications, Database design, Database documentation, Database coding, Problemsolving, Attention to detail, Accuracy, Communication skills, Interpersonal skills',\n",
       " 'Machine Learning Operations (ML Ops), Engineering, Evaluation Experimentation, Monitoring Processes, ML Workflows, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Autonomy, Flexibility, Teamfirst Mentality, Mentoring, NYCbased, Hiring, Financial Workflows, Product Ownership',\n",
       " 'Data Engineering, Data Analytics, Data Warehousing, Data Modelling, Data Pipeline Design, ETL (Extract Transform Load), Cloud Computing, Data Lake, Data Strategy, Data Management, Data Governance, SQL, Python, Advanced Statistics, Machine Learning, Team Leadership, Business Requirements Gathering, Business Intelligence, Reporting, Data Visualization, Data Quality, Data Security, Full UK Driving Licence',\n",
       " 'Data Analysis, Data Visualization, Statistical Techniques, Machine Learning, Data Mining, SQL, Python, R, Tableau, Power BI, Data Modeling, Data Quality Management, Data Processing, A/B Testing, Statistical Modeling, Hypothesis Testing, Data Management, ETL Processes, Business Intelligence, DataDriven Decision Making',\n",
       " 'Hardware Validation, ML/AI Frameworks, Test Execution, Automation Development, Debugging, GPU, HighPerformance Computing, Python, Linux, C/C++, Schematics, Circuit Design, Data Traffic Management, Electrical Laboratory Equipment, Computer Architecture, Processors, Interconnects, Firmware, ML/AI Models, Workloads',\n",
       " 'Data governance, Data lineage, Data interdependencies, Data standards, Data consistency, Governance rules, Governance controls, Data quality, Data analytics, Business intelligence, Data lakes, Data Warehouse, Data gaps, Data flows, Data catalogues, Relational databases, Data transformations, Data mappings, Data modelling, Data findings, Data recommendations, Data consolidation, Agile Product Owner, Healthcare IT systems, Electronic Medical Record systems, EDW, Data Maturity Model, Tableau, Power BI, Data modernization, Data security, Data stewards, Dashboards, Data assets, Data feeds, Data privacy, Data governance policies, Data governance meetings, Third party data, First party data, Derivative data, Second party data, Product backlog, Product development, Product stakeholders, Health Informatics, Public Health, Data Science, Epidemiology, Biostatistics, Public Policy, Health Policy, Health Services Research, Health Information Sciences, Computer Science, Data governance efforts, Data sharing projects, Agile Product Owner, Enterprisewide data governance, Data lakes, Data maturity models, Health and human services',\n",
       " 'SQL Server, SSIS, Power BI, Azure, AWS, Windows, Linux, PowerShell, SQL DBA, System Utilities, Debugging Tools, Monitoring Tools, Corporate Database Standards, Strategic Database Technologies, Data Management Principles, Data Management Techniques, Information Systems',\n",
       " 'Revenue Cycle, Healthcare Data Analysis, Data Visualization, Epic Clarity Reporting, SQL, Python, R, Analytic Methods, Statistical Methods, Project Management, Agile Development, Communication Skills, Technical Abilities, Epic Revenue Cycle Certification, Certified Health Data Analyst (CHDA) certification, Bachelor of Science degree, 4+ years of relevant experience, 2+ years of data analysis in healthcare, Strong grasp of analytic and statistical methods, Clinical Experience, Excellent written and oral communication skills, Excellent organizational and analytical skills, Strong technical abilities',\n",
       " \"Data Engineering, Data Architect, Data Infrastructure, Data Pipelines, AWS S3, AWS EMR, GraphQL API, Data Science, Data Analysis, SQL, NoSQL, DataFocused Programming Languages, Python, R, Scala, Petabytes of Data, Bachelor's Degree, Master's Degree, AWS Certified Cloud Data Engineer, ETL Systems, ELT Systems\",\n",
       " 'Data analytics, Data engineering, Data science, Data processing, Data pipelines, Data visualizations, Machine learning, Software engineering, Python, SQL, Scala, Hadoop Distributed File System (HDFS), Spark, Presto, Linux, Ansible, Docker, Kubernetes, Jupyter Notebooks, Pandas, Bokeh, Superset, Airflow, Statistics, Communication skills, Problemsolving skills',\n",
       " 'Azure, ADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates, Python, SQL, Scala, SparkSQL, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, Storm',\n",
       " 'Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Management Tools, Data Classification, Data Retention',\n",
       " 'Data Lifecycle Management, File System Management, Data Governance, Network Share Management, Project Folder/File Structure Creation, Network Share/File/Folder Permissions Management, Large Data Set Migration, Legacy to Modern Systems Data Transfer, NonHomogenous File/Directory Structures Data Integration, Microsoft Distributed File Systems, Windows Server OS Administration, Microsoft Office 365, Active Directory, Granular NTFS File Permissions, Windows File ACLs, NFS Shares, CloudBased Storage Systems, Azure, AWS, Communication Skills, Troubleshooting, Organizational Skills, SQL',\n",
       " 'Software Engineering, Microservices, Full Stack Development, Cloud Computing (AWS Microsoft Azure Google Cloud), Programming Languages (Python Go), AWS Tools and Services, Data Management, Data Compliance, Mentoring, Technical Problem Solving, Agile Practices, People Management, Open Source Frameworks (Airflow Spark Presto Snowflake), Regulatory Compliance',\n",
       " \"Senior Architect, Data Analytics, Product Line, Project Management, Design, Strategy, Agile Development, Collaboration, Presentation Skills, Leadership, Communication, Business Acumen, Master's Degree, Computer Science, Engineering, Math, Security Clearance, Washington DC\",\n",
       " 'Variable Cost Productivity, Data Analytics, Costbenefit Analysis, Return on Investment Analysis, Product Configuration Management, Locally Sourced Items, Data Mining, LEAN, Six Sigma, ISO, CFDA, SQL, HTML, CSS, JavaScript, Python, C++, Java',\n",
       " 'Python, Java, Scala, Agile, Data Modeling, Machine Learning, Distributed Microservices, Cloud Computing, AWS, Microsoft Azure, Google Cloud, Open Source RDBMS, NoSQL, Cloudbased Data Infrastructure, SQL, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX/Linux, Agile Engineering Practices',\n",
       " 'Data science, Machine learning, Artificial intelligence, Data management, Data analysis, Data visualization, Business intelligence, Data mining, R, Python, Pytorch, Tensorflow, Model building',\n",
       " 'Data Analysis, Reporting, Team Management, Data Systems Improvement, SQL, Tableau, Excel, Leadership, Decision Making, Time Management, Communication, Team Work, Annual Leave, Benefits, Performance Bonus, Pension Scheme, Life Assurance, Colleague Discount, Training',\n",
       " \"Data Visualization, Business Intelligence, Power BI, DAX, SQL, TransactSQL, OLTP, OLAP, ETL/ELT, Excel, Data Structures, Data Integrity, Software Development Lifecycle, Security, Privacy, Documentation, Communication, Interpersonal Skills, Presentation Skills, Critical Thinking, Problem Solving, Analytical Skills, Initiative, Teamwork, Time Management, Prioritization, DoD 8570, SECNAV 5239.2, Secret Clearance or ability to obtain, Bachelor's degree in Computer Science Information Systems or Computer Engineering, 4 years related experience, 3+ years of BI development using Power BI or equivalent tool, 3+ years of SQL and databases, 3+ years of Excel experience, Knowledge of ETL/ELT techniques and methodologies along with best practices, Exceptional analytical critical thinking and problemsolving abilities, Highly selfdirected, Takes initiative, Can work independently while being a strong contributor to a team, Effective prioritization and time management skills, Can thrive in a dynamic environment in which priorities might change frequently, Excellent communication and interpersonal skills, Strong documentation and presentation skills, Work requires CSWF certification, Security+ or CISSP\",\n",
       " 'Data Loss Prevention (DLP), Cybersecurity, DLP Controls, SaaS, IaaS, Information Security Policy, Standards, Web proxy, Email solutions, Endpoint solutions, Communication skills, Technical initiatives, Collaboration, Enterprise cybersecurity, Technical delivery, Product security, Software development practices, Platform engineering, Troubleshooting, Investigating, Configuring, Supporting, Data protection, Symantec Data Loss Prevention (DLP), High School Diploma, GED, Cybersecurity, Information technology, Agile delivery model, Public cloud security, Multicloud environments, JIRA, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, McLean VA, Performancebased incentive compensation, Health, Financial benefits, Capital One Careers website, Equal opportunity employer, Diversity and inclusion, Accommodation during the application process, Technical support, Recruiting process',\n",
       " 'Data Management, Statistical Analysis, SAS, R, Simulation Modeling, AgentBased Modeling, Dynamic Modeling, Discrete Event Modeling, Data Science, Health Economics, Healthcare, Large Language Models, Longitudinal Surveys, Statistical Programming, Medicare, Medicaid, Marketscan Data, Problem Solving, Interpersonal Skills, Communication Skills, Teamwork, Statistics, Biostatistics, Quantitative Analysis, U.S. Citizenship, Security Clearance',\n",
       " 'Data Modelling, Data Architecture, Data Governance, Data Standards, Data Analysis, Data Synthesis, Data Profiling, Source System Analysis, Metadata Management, AWS, Azure, SQL Server, RedShift, JIRA, Confluence, Draw IO, Visio, Collibra, Informatica, Communicating between Technical and NonTechnical Subjects, Engaging Collaboratively with Stakeholders, Leading Process Development, Evaluating Existing Data Systems, Providing Comprehensive Documentation, Championing Opportunities for Enhancing Current Data Architecture, Designing Data Architecture that Deals with Problems Spanning Different Business Areas, Identifying Similarities between Problems to Devise Common Solutions, Understanding the Impact on the Organisation of Emerging Trends in Data Tools Analysis Techniques and Data Usage, Working within a Strategic Context and Communicating How Activities Meet Strategic Goals',\n",
       " 'Python, Data Engineering, Data Science, Business Intelligence, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Data Warehouses, Kafka, Storm, SparkStreaming, ETL, TDD, Pair Programming, Continuous Integration, Deployment, Agile Engineering Practices, Dimensional Data Modeling, Schema Design, Legal Compliance, Data Classification, Data Retention',\n",
       " \"Data Quality Manager, Data Quality Standards, Data Quality Framework, Data Quality KPIs, Data Council, Data Owners, Data Stewards, Data Quality Issues, Data Quality Initiatives, Microsoft Purview, Data Quality Rules, Data Quality Processes, Data Quality Procedures, Data Quality Mindset, Best Practices, ProblemSolving, Analytical Abilities, Communication Skills, Collaboration Skills, SelfStarter, Initiative, Bachelor's Degree, Computer Science, Information Management, Statistics, Data Quality Management, Data Quality Concepts, Data Quality Methodologies, Data Quality Frameworks, Microsoft Purview, Data Quality Tools, Competitive Salary, Development Opportunities, Event Day Tickets, Internal Events, Free Lunches, Free Private Medical Cover, Contributory Pension Scheme, Additional Thank You Days, Volunteering Days, Annual Leave, Hybrid Working Model, Remote Work, Fixed Location, Wembley Stadium, St. George's Park, Processing Centre, Homebased Contract\",\n",
       " 'Machine learning, Deep learning, Computer vision, Object detection, Python, C++, Linux, Git, Automotive AI, Medical device, Data pipelines, Design, Deployment, Algorithm optimization',\n",
       " 'Ontology, Data Modeling, Linked Data, SPARQL, Graph Databases, Python, R, JSON, OpenAPI/YAML, AVRO, SQL, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, Agile Principles, Processes, Methodologies, Amazon Web Services, Communication Skills, Project Management, ProblemSolving, DetailOrientation, Reading Comprehension, Writing Skills',\n",
       " \"Data Analytics, Data Engineering, Data Mining, Exploratory Analysis, Predictive Analysis, Statistical Analysis, Artificial Intelligence, Machine Learning, Data Visualization, Microsoft Office (Power BI), Tableau, R, Python, SQL, MultiINT Analytics, Automated Predictive Analytics, TS/SCI, Master's Degree, Bachelor's Degree, 12+ Years of Experience, 5+ Years of Related Experience\",\n",
       " 'ETL, Master Data Management, Data pipelines, Databricks, AWS services, Data quality, Data validation, Data cleansing, Data enrichment, Agile Scrum SDLC, Unit testing, Integration testing, User acceptance testing, Scala, Java, Apache Spark, AWS Glue, Data Lakehouse architecture, OpenSearch, Amazon S3, Amazon Redshift, Tika, Docker, ElasticSearch, NoSQL databases, AWS Lambda, SQS, NiFi',\n",
       " 'Database Platform Operations, Microsoft SQL Server, Oracle, Sybase ASE, MongoDB, Snowflake, Unix / Linux, Windows Server, Bash, PowerShell, Python, SQL, TSQL, PLSQL, Clustering, HighAvailability, Replication, Disaster Recovery, Backup and Recovery, SQL and Systems Performance Tuning, Network Optimization, Storage Optimization, Server Optimization, Virtualisation, Logical and Physical Database Design, CIS Security Hardening, Security and Auditing, AWX / Ansible Tower, Git, Ansible, Terraform, TeamCity, Serena Deployment Automation (SDA), Jenkins, Kubernetes, Docker, Infrastructure as Code, Continuous Integration (CI), Continuous Development (CD), Agile, Site Reliability Engineering, DevOps, Active Directory, LDAP, Kerberos, BMC ControlM, Oracle Cloud Control, IBM Tivoli / Netcool, Oracle ODA, Nutanix HCI, VMWare ESX, Networking Protocols (TCP/IP DNS DHCP VLAN’s), Cloud Computing, IaaS, PaaS, SaaS, Azure, AWS, GCP, Oracle, GDPR, SOX, Dell EMC PowerStore (SAN), Isilon (NAS), Rubrik, EMC Networker, Data Domain, IBM Tivoli Storage Manager, CyberArk, Splunk, Qualys, Cisco Tetration, ServiceNow, JIRA, Confluence, Communication, Interpersonal Skills, Pressure Handling, ProblemSolving Skills, ResultsDriven, Accountability, Proactive Approach, Motivation, Urgency Management, Prioritization, Structured Approach, Logical Approach, Attention to Detail, Accuracy, Performance Under Pressure, Conflict Management, Large Workload Management, Tight Deadline Management, Complex Technical Communication, Diversity and Inclusion',\n",
       " \"Data Analysis, Machine Learning, Python, PySpark, SQL, Statistical Models, Analytics Environment, Data Science, Communication Skills, Collaboration Skills, Analytical Skills, Problem Solving Skills, Bachelor's Degree in Computer Science Computer Information Systems Mathematics Statistics or Economics, Experience in a Consumer Packaged Goods Company\",\n",
       " 'Electrical project management, Building Electrical systems, Electrical Distribution, Emergency Generators, Uninterruptible Power Supplies (UPS), High/Medium/Low voltage applications, Heating Ventilation Air Conditioning Plumbing, Critical Water systems, Fire/Life Safety Protection Systems, Root cause analysis, Computerbased estimating, OSHA Certification, College degree, Journeyman’s license, Master electrician’s license, Strong communication skills, Organizational skills, Teamwork skills',\n",
       " \"Data Engineering, Real Time API design, Cloud native platforms, Serverless computing, High performance computing, Microservices, Data pipelines, Data Mesh, Data Fabric, Multitenancy, Data platform strategies, Software engineering, Problem solving, Operational platforms, Distributed operational systems, Resilience, Scalability, High availability, Data cleaning, Data transformation, Mentoring, Recruiting, Bachelor's Degree, Master's Degree, At least 7 years experience, At least 8 years experience, At least 10 years experience\",\n",
       " 'Remote work, IQA Award, IT industry qualification (Level 4 or above), Handson experience in IT/Data sector, Level 4 teaching qualification (or willingness to pursue one), Understanding of Ofsted EIF, Coaching and guidance, Performance reviews, Observation, Corrective actions, Performance management, Quality assurance, Compliance, Targets/KPIs, Strategy implementation, Teamwork, Communication, Motivation, Problemsolving',\n",
       " 'Data Science, Quantitative Field, Technical Leadership, Interdisciplinary Collaboration, Execution and Velocity, Research and Statistical Depth, Scalable Design, Mentorship and Development, Forecasting Systems, Marketplace Dynamics, Experimental Design and Analysis, Stakeholder Alignment, Metric Frameworks, Hybrid Work, Pay and Benefits',\n",
       " 'Data privacy, Cybersecurity law, Contract law, Litigation, Legal compliance, Nondisclosure agreements, Sales and purchasing agreements, Distributor/reseller agreements, Supply agreements, Marketing and other service agreements, Commercial contracts, Employment law, Antitrust, Intellectual property, Export control, Environmental regulations, Diversity equity and inclusion, Communication skills, Multitasking, Teamwork, Travel, Spanish, Portuguese, Law school graduation, California law practice admission, 612 years corporate law department or law firm experience, Law',\n",
       " \"Data Engineering, Leadership, Project Management, Data Warehousing, P&C Insurance, SQL, Talend, ETL, AWS, Big Data, Orchestration, Cloud Computing, Data Security, Data Governance, Communication, Collaboration, Snowflake, Agile, Java, Python, Linux, Hadoop, Spark, NoSQL, Machine Learning, AI, Data Analytics, Business Intelligence, Bachelor's or Master's degree in Computer Science Data Engineering or a related field from an accredited college or university, Proven handson experience with Snowflake SQL Talend AWS and big data technologies, Indepth knowledge of the Property and Casualty (P&C) insurance domain, Experience leading and managing data engineering teams, Strong communication skills with the ability to collaborate effectively across departments, Certifications in Snowflake Talend AWS or related technologies, Familiarity with data governance security and compliance standards\",\n",
       " 'Infrastructure, Troubleshooting, Optimization, Azure, DBXUC, Snowflake, DataOps, Resource Optimization, Cost Management, Pipeline Monitoring, Alerts, Failure Resolution, Data Flow, Logic Apps, Access Management, Logic Apps failures, APM Configuration',\n",
       " 'Top Secret Security, Configuration Management, Change Control, Technical Review Boards, Physical Configuration Audits, Functional Configuration Audits, IT Configuration Management, Software Configuration Management, Hardware Configuration Management, Firmware Configuration Management, Peripheral Configuration Management, Data Requirements, Contract Data Requirements List (CDRL), Data Calls, Data Requirements Review Boards (DRRBs), Statement of Work (SOW), Statement of Objectives (SOO), Performance Work Statement (PWS), Department of Defense (DoD), DoDI 5230.24, DODD 5230.25, DoD 5010.12M, DoD 5000.02R, DFARS 252.2277013, DFARS 252.2277014, ASSIST Database, MS Office Suite, Baseline Management, Version Control, Data Duplication, Media Labeling, Release Notes, CM Reports, Change Control Documentation, Baseline Administrative Activities, Briefings, Software Development Life Cycle, Database Management, Attention to Detail, Organizational Skills, Multitasking, Teamwork, US Citizenship',\n",
       " 'Program management, Project management, Data Center, DMZ, B2B, VOR, Building Networking solutions, Wireless LAN, Structured Cabling, Cable Systems, Pathways, Public transport systems, Private transport systems, WAN, Optical systems, SwitchedEthernet, LTE, SDWAN, Cloud, Virtualized services, Network Appliance, Video, Voice, Voice over IP (VoIP), UC, Collaboration services, Jira, Confluence, LeanAgile, Scrum, Kanban, SAFe Agile methodologies, Change Management, Service Delivery, PMP, ITIL, CCNA, Risk management, Decision making, Strategic thinking, Business results, Ownership, Accountability, Communication, Influence, Organization, Detail orientation, Technical acumen, Partnership, Automation, Process improvement, Financial services, Insurance, Banking, Investment banking, Nimble, Flexible, Proactive, Reactive',\n",
       " 'Data analysis, Data extraction, Data transformation, Data visualization, Data reporting, Epic Clarity reporting, SQL, Python, R, Statistical methods, Analytic methods, Epic Clarity Data Model certification, Epic Revenue Cycle certification, Certified Health Data Analyst (CHDA) certification, Project management, Agile Development, Written communication, Oral communication, Organizational skills, Analytical skills, Technical skills, User observation, Workflow analysis, Clinical experience, Healthcare data, Revenue cycle operations, EHR systems, Clinical teams, Operational teams, Senior leadership, Datadriven solutions, User experience, Informatics foundation, Digital solutions, Health system solutions',\n",
       " 'Project Management, Civil Engineering, Construction Management, LargeScale Projects, Healthcare Projects, Commercial Projects, Scheduling, Quality Assurance, Quality Control, Cost Management, Document Control, Site Safety, Leadership, Communication, Teamwork, Organizational Skills, Attention to Detail, ProblemSolving Skills, BS in Construction Management or Civil Engineering, 5+ Years of Experience in Construction Industry, Proven Track Record of Effectively Managing Job Sites, Positive Attitude and Strong Management Ability',\n",
       " 'Data Warehouse Architecture, Data Warehouse Modeling, ETL Processing, Data Processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, Python, PySpark, SQL, Data Profiling, Process Flow, Metric Logging, Error Handling, ETL Architecture, System Architectural Decisions, Development Standards, AWS Expertise, Kinesis, Glue (Spark), EMR, S3, Lambda, Athena, Streaming Services, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data, NoSQL, Java',\n",
       " 'Data Analytics, SQL, Tableau, Data Visualization, Business Requirements, Data Analysis, User Documentation, Data Integration, Report Monitoring, Test Result Documentation, Statistics, Mathematics, Computer Science, Visual Software, Medical Device, Pharmaceutical, Regulatory Industry, ERP Systems, Data Exploration, Presentation Skills, Writing Skills, SAP, CRM',\n",
       " 'Medical Laboratory Science, ASCP Certification, Hematology, Chemistry, Immunohematology, Blood Banking, Microbiology, Serology, Coagulation, Urinalysis, Laboratory Safety, Laboratory Equipment Operation and Maintenance, Data Accuracy and Record Keeping, Oral and Written Communication, Mathematical Skills, Reasoning Ability, Patient Care, Blood Borne Pathogens, Laboratory Reagents, Unpleasant Patient Elements, Electrical and Mechanical Hazards',\n",
       " 'Medical Lab Technician (MLT), Compassion, Attention to Detail, Acute Care Experience, Per Diem Nursing, Travel Nursing, Locum Tenens, Interim Leadership, Clinical Instructor, HCA Healthcare, EVIND1',\n",
       " \"AI/ML, ML frameworks, Pytorch, SageMaker, HuggingFace, Image Processing, Computer Vision, Generative AI, GPU workload optimization, Model serving, Orchestration, Scaling, GPU resource management, Communication, Collaboration, Analytical skills, Problemsolving, Datadriven decisionmaking, Latest trends and technologies in AI/ML, Bachelor's/Master's in Computer Science Electrical Engineering Computer Vision or related field\",\n",
       " \"Sales, Prospecting, Client Engagement, Data Analysis, Research, Analytics, Commercial Real Estate, B2B Sales, Consultative Selling, Client Training, Product Demonstrations, Business Intelligence, Marketing, Communication, Customer Service, Leadership, Teamwork, Problem Solving, Strategic Thinking, Bachelor's Degree, ClientFacing Experience, Outside Sales Experience, Motor Vehicle License, Driver's License, Driving Record, Operational Motor Vehicle, CoStar, LoopNet, S&P 500\",\n",
       " 'Data Analysis, Data Modeling, Data Warehouse Systems, Power BI, Tableau, DB2, SQL Server, Oracle, Data Warehouse Architecture, ETL Processes, SQL, GQL, SAS, Data Quality Processes, Metadata Management, Relational Database Platforms',\n",
       " 'Statistical Analysis, Data Management, Statistical Software (R SAS STATA), Database Management (Red Hat Qualtrics REDCap ACCESS), Data Interpretation, Data Visualization, Report Writing, Data Modeling, Data Presentation, Manuscript Preparation, Team Supervision, Epidemiology, Biostatistics, ArcGIS, MAXQDA, AtlastTI, GIC, COVID19 Vaccination, Affirmative Action',\n",
       " 'Medical Technologist, Medical Laboratory Technician, Blood Bank, Laboratory, Clinical, Test, Equipment, Diagnosis, Treatment, Disease, Procedures, Maintenance, Shift Work, Shift Differentials, Sign On Bonus, Relocation Assistance, StateoftheArt Equipment',\n",
       " 'SQL, Tableau, Data Analytics, BI, Data Manipulation, Data Visualization, Data Modeling, Data Mining, Data Warehouse Design, Data Extraction, Dashboards, Data Communication, Data Analysis, Data Interpretation, FinTech',\n",
       " 'Hadoop, Spark, Postgres, Angular JS, Scala, NoSQL, Python, SQL, Java, MapReduce, Hive, EMR, Kafka, Gurobi, MySQL, Mongo, Cassandra, Redshift, Snowflake, UNIX/Linux, Agile engineering practices, Data pipeline frameworks, Data APIs, Data delivery services, Analytical models, Cloud Production environments, Multitier multiplatform systems, Data warehousing, Realtime data, Streaming applications',\n",
       " 'Data Center Infrastructure, Server Hardware Troubleshooting, Network Hardware Troubleshooting, Server and Network Equipment, Data Center Environments, Server Provisioning, Network Provisioning, Break/Fix Tasks, Server Hardware, Network Infrastructure, Scalability, High Availability, OnCall Rotation, Edge/POP Sites, Low Voltage Cabling, BOMs, Vendor Management, Automation, Root Cause Analysis, Runbooks, SOPs, MOPs, Fitness Program, CalTrain Go Pass, 401k Program, Flexible Vacation Policy, Medical Coverage, Dental Coverage, Vision Coverage, Unlimited Snacks, Onsite Fitness Center, Fitness Credit, Catered Lunches',\n",
       " 'Data Engineering, Data Analysis, Predictive Modeling, Data visualization tools, Python, R, SQL, Tableau/Domo, AS400, ERP systems, Inventory Management, Forecasting, Time Series Analysis, Regression Analysis, Machine Learning Algorithms, Communication skills, Team Collaboration',\n",
       " 'Data Loss Prevention (DLP), Symantec, Data Protection, Cybersecurity, SaaS, IaaS, Web proxy, Email, Endpoint solutions, Cloudbased infrastructure, Agile delivery model, Public cloud security, Multicloud environments, JIRA, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, CISSP, GIAC, CISM, CCSP, CISA, Security+, URL filtering, Proxy, Network DLP, Scripting, Technical writing',\n",
       " 'Environmental Science, Engineering, Computer Science, GIS, Environmental Science, Statistics, Environmental Health, Toxicology, Data Research, Data Analysis, SQL, Tableau, Python, R, Data Science, Data Stewardship, Data Management, Quantitative Analysis, Statistical Modeling, Program Evaluation, Project Management, Communication Skills, Spatial Analysis, Qualitative Analysis',\n",
       " 'Healthcare Data Analysis, Data Visualization, SQL, Python, R, Statistical Methods, Epic Clarity Reporting, Revenue Cycle Modules, Project Management, Agile Development, User Observation, DataDriven Solutions, Informatics, Strategy Documents, Data Dictionaries, Communication Skills, Organizational Skills, Analytical Skills, Technical Abilities',\n",
       " 'Data management, Cancer research, Oncology clinical trials, Database development, ICHGCP, Biomarkerdriven clinical research, Genomics, Biomarker assays, Experimental Cancer Medicine Centres (ECMC), Cancer Research UKfunded research, Project management, Organizational skills, Computer proficiency, Interpersonal skills, Teamwork, Communication skills',\n",
       " 'Data Analysis, Statistics, Machine Learning, Algorithms, A/B Testing, Data Visualization, SQL, R, Python, Tableau, Power BI, Data Management, ETL',\n",
       " 'Microsoft Office Suite, Data Analysis, Problem Solving, Technology Skills, DetailOriented, Multitasking, Investigative Skills, Teamwork, SelfMotivation, Medical Dental Vision Insurance, PTO/ Paid Holidays, 401(k) with company match, ProfitSharing/ Defined Contribution, Companypaid ST and LT Disability, Maternity Leave/ Parental Leave, Companypaid Term Life/ Accidental Death Insurance, Thirdparty employee benefits administrator, Quality Service, Innovative Solutions, Performance Evaluation, Growth Funds, Career Opportunities',\n",
       " 'Machine Learning, Software Development, Data Engineering, DevOps, Scalability, Agile, Python, Scala, Java, Distributed Computing, Cloud Computing, SQL, ScikitLearn, PyTorch, Dask, Spark, TensorFlow, Data Gathering, Data Preparation, Data Pipelines, Data Warehousing, Data Lakes, AWS, Azure, Google Cloud Platform, DataBricks, SnowFlake',\n",
       " \"Data Loss Prevention (DLP), Web proxy, Email DLP, Endpoint solution, Technical writing, Agile frameworks, Cloud computing environments, Cloud security, Symantec Data Loss Prevention (DLP) infrastructure engineering, Cyber security practices, URL Filtering, Proxy, Network DLP, JIRA, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, IT Delivery projects, Handson, Agile delivery model, Software engineering principles, Cyber security, Software development practices, Platform engineering, Basics qualification, GED, Cybersecurity, Information technology, Bachelor's Degree, Systems Engineering, Computer Science, Scripting, Multicloud environments, Performance based incentive compensation, Financial benefits, Health benefits, Incentive compensation, Capital one benefits, Fair criminal records screening act, Criminal history inquiries, Accommodation, Email\",\n",
       " 'Data Analytics, Data Modeling, Database Design, Spark Framework, ELT (Extract Load Transform), SQL, Agile Project Delivery, Business Intelligence Tools (Tableau MicroStrategy Alteryx), Data Processing Technologies (Apache Spark Kafka), Cloud Environment (AWS), Python, Airflow, Pipeline Automation, ETL Processes, Data Quality Standards, Data integrity, Analytical skills, Multitasking, Communication, Problem Solving, Organizational Skills, Independent Work',\n",
       " 'Machine Learning, Python, Scala, Java, Scikitlearn, PyTorch, Dask, Spark, TensorFlow, AWS, Azure, Google Cloud, Distributed Computing, Data Pipelines, AI, Computer Science, Electrical Engineering, Mathematics, Distributed File Systems, Multinode Database Paradigms, Open Source ML Software, Research Paper Authorship, Data Science, Agile',\n",
       " 'Audit, Risk Management, Data Management, Financial Crimes and Fraud, AWS, Data Analysis, Audit Procedures, Communication, Interpersonal Skills, Certified AntiMoney Laundering Specialist (CAMS), Certified Internal Auditor (CIA), Certified Public Accountant (CPA), Certified Regulatory Compliance Manager (CRCM), Certified Fraud Examiner (CFE), Certified Data Management Professional (CDMP), Certified Information Security Manager (CISM), Certified Info Security System Professional (CISP), Business Understanding, Problem Solving, Leadership, Teamwork, Attention to Detail, Regulatory Compliance, Financial Services, Critical Thinking, Business Partnering',\n",
       " 'Data Warehouse, Teradata Technology, UNIX, SQL, Oracle, System of Records, Data Governance, Data Management, Data Lineage, Data Quality, Data Science, KPI Dashboards, Data Analysis, Data Modeling, Data Interpretation, ProblemSolving, Methodical Approach, Logical Approach, Work Planning, Deadlines, Accuracy, Attention to Detail, Engineering, 5 Years of Work Experience',\n",
       " 'Electrical Engineering, Mechanical Engineering, Computer Science, Project Management, Vendor Management, Power Distribution, Data Center Mechanical Cooling Systems, Technical Concepts, Feedback, Analytical, Quantitative, Data, Metrics, Assumptions, Recommendations, Decisions, Contracts, Negotiation, Ambiguous Environment, Leadership Principles, Functional Depth, Technical Complexity, Data Center Project Management, Construction Management, Project Oversight, Electrical Systems, Mechanical Systems, Controls Systems, Architectural Systems, Innovation, Problem Solving, MultiDisciplinary Teams, Design, Deployment, Direction, Coordination, Implementation, Execution, Control, Completion, Strategy, Commitments, Goals',\n",
       " 'Data Governance, Data Management, Data Quality, Clinical Data Standards, Medicare, Medicaid, Care Management, Data Analytics, Data Audit, Data Integration, Data Quality Assurance, Data Security, HL7, FHIR, Arcadia, Agile Methodology, Jira, Rally, SQL, SAS, Python, Hadoop, Teradata, Snowflake, Tableau, Collibra, Infosphere, Alation',\n",
       " 'Machine Learning, Data Visualization, Python, Scala, Java, R, Julia, Matlab, Spark, Hadoop, Agile Development, Geospatial Data, Tensorflow, Torch, Computer Vision, Natural Language Processing',\n",
       " 'Field Application Engineering, Product and Solution Recommendation, Customer Needs Identification, Requirement Modifications, Product Specifications Creation, Technical Marketing Support, Project Execution, Followup Technical Support, Engineering Expertise, Design Work, Product Application Information, Customer Support, Customer Engineering Relationships, Product and Future Roadmap Understanding, Voice of Customer Tools, Crossfunctional Team Collaboration, Design Cycle Penetration, Customer Needs Advocacy, Product and Process Qualifications, Engineering Experience, Connector Knowledge, Manufacturing Processes, Development Engineering, Electronics Components, Project Leadership, Organization and Prioritization, Interpersonal and Negotiating Skills, Communication Skills, Computer Proficiency, 3D Design Applications, Travel Willingness, Independent Work, Networking and Influencing Skills, Production Level Drawings, Tolerance Analysis, Integrity, Accountability, Teamwork, Innovation',\n",
       " 'Data Analysis Engineer, BEV/FCEV database build/monitor, Data Visualization, Dashboards, SQL, Python/Scala, Distributed Computing, Airflow DAGs, AWS Glue, PowerBI, Grafana, Engineering, Cloudbased timeseries stream processing systems, Electrified propulsion systems, Physical vehicle test data, CANbased data loggers, CAN/LIN based SAE standard communication protocols, U.S. work authorization, Equal employment opportunity',\n",
       " 'Data Engineering Management, Cloudbased Data Platforms, Data Solutions, Data Warehousing, Snowflake, SQL Server, Azure, System Architecture, Data Design, Python, SQL, Scala, Java, Realtime Data, Streaming Applications, Public Cloud (AWS Microsoft Azure Google Cloud), Data Quality, Metadata Management, Data Ops, Project Management, Agile Ceremonies, ProblemSolving, Root Cause Analysis, Continuous Improvement, Innovation, Distributed Data Platforms, Data Life Cycle, Security, Supply Chain, Distribution',\n",
       " 'Machine Learning, Python, Numpy, Pandas, Jupyter, TensorFlow, PyTorch, Numba, Cloud services (GCP AWS Azure), Clinical team, Voice biomarker software, Depression and anxiety detection, Speech processing/recognition, Audio classification, Agile methodologies, Experiment tracking and reproducibility tools (MLFlow WandB DataBricks), FDA approvals for a softwareasamedical device (SaMD)',\n",
       " 'Data Protection, Data Loss Prevention (DLP), Cybersecurity, Information Security, SaaS, IaaS, Agile, Cloud Computing, Operating Systems, Databases, Virtualization, Networks, Symantec Data Loss Prevention (DLP), JIRA, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional',\n",
       " 'Data extraction, Data transformation, Data analysis, Healthcare data, Data visualization, Epic Clarity reporting, SQL, Analytic methods, Statistical methods, Epic Clarity Data Model certification, Epic Clinical Data Model certification, Certified Health Data Analyst (CHDA) certification, Project management, Stakeholder observation, Python, R programming, Agile Development, Written communication, Oral communication, Organizational skills, Analytical skills, Technical skills, Clinical experience, Inpatient setting, Outpatient setting',\n",
       " \"Data science, Statistical methods, Regression, Cluster analysis, HLM, Social network analysis, Text analysis, Longitudinal methods, Experimental methods, Python, R, SQL, Git, Data storytelling, Communication, Collaboration, Problemsolving, Analytical thinking, Attention to detail, Ability to work independently and as part of a team, Bachelor's or Master's degree in Data Science Statistics or a related field\",\n",
       " 'Data Lifecycle Management, Network File System Management, Data Governance, File System Management, Cloudbased Storage Systems, Microsoft Distributed File Systems, Windows Server OS, Active Directory, NTFS File Permissions, Windows File ACLs, NFS Shares, Azure, AWS, Strong Communication Skills, Attention to Detail, Troubleshooting Skills, Organizational Skills',\n",
       " 'Machine Learning, Python, Scala, Java, Distributed Computing, Cloud Computing, Data Pipelines, Scikitlearn, PyTorch, Dask, Spark, TensorFlow, AWS, Azure, Google Cloud Platform, Responsible AI, Data Gathering, Agile, Data Preparation, Proof of Concept, Scalability, Performance, Monitoring, Continuous Integration, DevOps',\n",
       " 'Health Informatics, Data Analysis, Population Health, Data Gathering, Data Storage, Data Sharing, Data Interpretation, Data Integrity, Electronic Medical Records, Health IT Systems, Structured Query Language (SQL), SAS Analytic Software, Data Formats, Reporting, Macro Language, Troubleshooting, Health Information Exchange (HIE), Clinical Data Capture, Social Determinants of Health, Public Health Programs, Data Quality Management, Data Governance, Data Security, Data Privacy, Data Visualization, Project Management, Communication, ProblemSolving, Analytical Thinking, Attention to Detail',\n",
       " 'Data Center Operations, Tape Operations, Smart Hands, DCIM Data Entry, Queue Management, Prioritization, SLA/OLA, Scheduling, Resource Assignment, Rack Elevation, Floor Space, Power Assignments, Project Deployment, Project Decommission, Change Management Requests, DCIM Tool Auditing, Data Accuracy, Cloud Computing, Business and Technology Consulting, Industry and Digital Solutions, Application Development, Managed EdgetoCloud Infrastructure Services, BPO, Systems Integration, Global Data Centers',\n",
       " 'Statistical Modeling, Machine Learning, Cell Failure Detection, Battery Cycle Life Prediction, Bayesian Optimization, Prognostics, Predictive Maintenance, Multivariate Modeling, Experimental Design, DataDriven Modeling, Energy Storage Systems, Electrochemistry, Lithium Metal Batteries, HPC/Cloud Computing, Python, C/C++, TensorFlow, PyTorch',\n",
       " 'Laboratory Inventory Management System (LIMS), HIPAA, General Policies and Procedure Compliance, PHI Privacy, Data Entry, Computer Operation, Communication Skills, Critical Thinking, Attention to Detail, Positive Attitude, High School Diploma, Typing 45 WPM, Laboratory Technology',\n",
       " \"Python3, Pyspark, Scripting, Hadoop, SQL, S3, Machine Learning, ML models, Random Forest, CNN, Regression models, Data Architecture, Data Warehousing, Software Engineering, Distributed Systems, Stream Processing, ML Modeling, Computation, Data Replication, Storage, Centralized Computation, Data API's, Agile, Test Driven Methodologies, Linux, Systems Administration\",\n",
       " \"Machine Learning, Artificial Intelligence, Deep Learning, SparkML, Scikitlearn, Caret, Mlr, Mllib, Python, R, RESTful API, Data Modeling, Data Mining, Data Visualization, Machine Learning Pipelines, MLOps, Software Development, Model Deployment, Natural Language Processing, Computer Vision, Predictive Analytics, Forecasting, Time Series Analysis, Regression, Classification, Clustering, Dimensionality Reduction, Feature Engineering, Data Preprocessing, Data Cleaning, Data Transformation, Data Labeling, Model Evaluation, Model Tuning, Model Optimization, Model Deployment, Model Monitoring, Model Maintenance, Consulting, Communication, Problem Solving, Critical Thinking, Analytical Skills, Attention to Detail, Teamwork, Leadership, Flexibility, Adaptability, Strong Work Ethic, Passion for Learning, Bachelor's Degree in Computer Science or STEM, Master's Degree in Computer Science or STEM, PhD in Computer Science or STEM, 5+ years of experience in Machine Learning, 3+ years of experience in AI/Deep Learning, 3+ years of experience in Software Development, Experience with ML Libraries, Experience with Data Visualization Tools, Experience with Public Speaking, Ability to write Production Level Code, Ability to develop experimental and analytic plans for data modeling processes, Ability to determine cause and effect relationships, Consulting experience and track record of helping customers with their AI needs, Publications or presentation in recognized Machine Learning Deep Learning and Data Mining journals/conferences\",\n",
       " 'Data Engineering, Data Analysis, Data Modelling, Python, SQL, ETL/ELT, Data Warehousing, Data Lakes, Data Security, Data Privacy, Data Governance, Google Cloud Platform (GCP), Data Management, Backup and Recovery, Database Management, Leadership, Mentorship',\n",
       " 'Machine learning, Regression, Natural Language Processing (NLP), Neural networks, Quantitative research methods, Statistics, Bayesian analysis, Pandas, Scikitlearn, Stats models, TensorFlow, MXNet, SageMaker, R, API development, Java frameworks, Web services, UI development, Git, Atlassian Jira, Atlassian Confluence, Slack, Agile methodology, Computer science, Mathematics, Physics, Economics, Engineering, Statistics, Operations research, Quantitative social science',\n",
       " 'Machine Learning, AI, Python, Data Gathering, Data Quality, System Architecture, Coding Best Practices, Lean / Agile Development Methodologies, Programming Languages (Python R Scala Java SQL), Deep Learning, CNNs, RNN, LSTMs, Generative AI, Large Language Models, Cognitive Services (AWS GCP Azure IBM Watson), Chatbots (Azure Chatbot Google DialogFlow Alexa RASA Amazon Lex), Perception, Computer Vision, Time Series Data, Text Analysis, Big Data (HDFS Hive Spark Scala), Data Visualization Tools (Tableau), Query Languages (SQL Hive), Applied Statistics, Distributions, Statistical Testing, Regression',\n",
       " 'Data Science, Data Engineering, Data Visualization, Cyber Security, Natural Language Processing, Machine Learning, Quantitative Social Science, Statistics, Artificial Intelligence, Deep Learning, Distributed & Scalable Data Engineering, Social Data Analytics, Healthcare, Energy, Finance, Security, Research, Grant Writing, Service, IndustrySponsored Projects, Outreach, Recruiting, Diversity and Inclusion, Affirmative Action, Clery Act',\n",
       " 'Machine Learning, Programming, Data Engineering, Cloud Computing, Distributed Computing, Data Analytics, Agile Development, Team Leadership, Python, Scala, Java, AWS, Azure, Google Cloud Platform, scikitlearn, PyTorch, Dask, Spark, TensorFlow, Data Warehousing, Data Lakes',\n",
       " \"SAS, Statistical programming, Data analysis, Reporting, Clinical data, Early clinical research studies, Data sets, Statistical analyses, Tables, Figures, Electronic data transfers, Quality Control, Study protocol, Department procedures, SAS programming experience, Bachelor's degree in Statistics, Mathematical Science, Physics, Interpersonal skills, Teamwork, Problemsolving skills\",\n",
       " 'Datapower, APIC, XML, XSL, XPath, XQuery, IIB, IBM WebSphere Message Broker, IBM Websphere MQ, Eclipse, IBM RAD, Tivoli/Wily monitoring, Java, REST, Web services, Data integration techniques, Messaging, Transformation engines, Nonstandard services, APIs, SOAP/REST, UNIX/LINUX, Eclipse, IDEA, Visual Studio, Swagger/Yaml, APIconnect, XSLT, Communication skills, Organized, Multitasking, Assertiveness',\n",
       " 'Oracle Database, Oracle Business Intelligence, Unix, OBIEE, Informatica, Tableau, Power BI, ETL, Cloud Computing, Data Center, Data Analytics, Data Security, Performance Tuning, System Analysis, Data Modeling, Technical Documentation, SQL, Data Warehousing, Database Administration, Oracle M8 Supercluster, Oracle RAC, Oracle GoldenGate, Unix System Administration, RMAN, Data Recovery, Database Migration, Oracle OEM, Data Archiving, Data Compression, Parallel Processing, Database Partitioning, Backup and Recovery, Disaster Recovery, Security Policies, Federal Regulations, Oracle RAC, Hadoop, SAP HANA, Big Data, Business Intelligence, Advanced Analytics, Predictive Analytics, Data Visualization, Machine Learning, Artificial Intelligence, Project Management, Data Warehouse Management, Technical Collaboration, Architectural Enhancement, Software Development, Business Intelligence',\n",
       " 'Machine Learning, Deep Learning, Python, NumPy, Pandas, Numba, PyTorch, TensorFlow, Jupyter, AWS, Azure, GCP, MLFlow, Research, Voice Biomarkers, Computer Science, Speech Processing, Clinical Team Collaboration, Agile Methodologies, Medical Device Software',\n",
       " \"Azure DevOps, Database design, SQL, TSQL, Data architecture, Data warehousing, Visual Basic, C#, Programming, ETL, Data modeling, RDBMS, Foglight, Dimensional modeling, Crossfunctional collaboration, Time management, Bachelor's degree, Analytical skills, Problemsolving, Critical thinking\",\n",
       " 'Software Production Engineering, Data Engineering, Python, Java, Bash, Puppet, Typescript/JavaScript, Red Hat Enterprise Linux, Prometheus, Grafana, Oracle, PostgreSQL, Cassandra, Elasticsearch, DoD 8570 IAT Level 1 Certification, DoD 8570 Computing Environment Certification, Amazon Web Services, Onpremises servers, Active DoD TS/SCI with CI Polygraph, BS in Computer Science / Engineering',\n",
       " 'CRM, Data Management, Admissions and Recruitment, Microsoft Office Suite, Mail Merge, AdmissionPros, Jenzabar CX, Relational Databases, Knowledge of English, Communication Skills, Organizational Skills, Customer Service Skills, Attention to Detail, Confidentiality, Ability to lift recruitment material',\n",
       " \"Data analysis, Strategy, Project management, Leadership, Excel, Tableau, SPSS, STATA, R, Data visualization, Data management software, Communication, Social impact, Education, Equity, Leadership development, Teamwork, Problemsolving, Analytical skills, Bachelor's degree, Master's degree, Fulltime work experience, Graduate degree, Authorization to work in the United States, Commitment to the full program term\",\n",
       " 'SQL, Data science, Mathematics, Statistics, Economics, Computer Science, A/B testing, SQL, Data analysis, Data governance, Data consistency, Machine learning, Data engineering, Data tracking, Data integrity, Data security, Product management, Product design, Software engineering, Business leadership, Datadriven decisionmaking, Communication, Problemsolving',\n",
       " 'Medical Lab Scientist Certification, Clinical Laboratory Science, Medical Laboratory Science, Biomedical Science, Biology, Biochemistry, Lab Information Systems (LIS), Hospital Information Systems (HIS), Quality Control, Proficiency testing, Laboratory Equipment maintenance, Staff competency, Process improvement, Training and orientation, Laboratory diagnostic tests, Test results confirmation, Quality control programs, Laboratory instruments and equipment monitoring, Troubleshooting, New employees and students training, Computer function, Continuing education, Inventory control and maintaining supply levels, Compliance, Regulatory, Licensing, Certification, ASCP, CLIA, Work Experience, Benefits',\n",
       " 'Data Architect, Technical Consultant, Data & Analytics, Azure Data Platform, On Prem, Cloud Data technologies, Solutioning, Consulting, Handson experience, Data Pipelines, Data warehousing, Data Modelling, ADF, Databricks, ETL, Data Lake, Synapse, Data Storage, DB Schema, Data Model, Performance Optimization, SQL Server, SQL Programming, Stored Procs, Functions, Azure Data Services, Oracle EDW, Data Migration, AI Skills, Client Models, API Layer, Reporting, Power BI, Paginated Reports, Power BI Embedded, Premium, DAX, Tableau',\n",
       " 'Data Operations, Data Management, Data Structure, Data Taxonomy, Data Tagging, Data Quality, Data Governance, Data Reporting, Business Optimization, Strategic Projects, Personalization, Relational Targeting, Aprimo, ClickUp, PowerBI',\n",
       " 'Clinical data management, Data integrity, Data consistency, Data collection, Data validation, Clinical trial documentation, Regulatory compliance, Clinical databases, Electrical engineering, Neuroscience, Clinical physiology, Nursing, Biotechnology, Clinical research, Brain signal analysis, Image data processing (fMRI/EEG), FDA processes, OHRP, HIPAA, Python, Java, MATLAB, Coding',\n",
       " 'Mechanical Engineering, HVAC, Domestic Water Systems, Steam, Compressed Air, Piped Services, Planet FM, Reactive Maintenance, Preventative Maintenance, Predictive Maintenance, ReliabilityCentred Maintenance',\n",
       " 'Commissioning, Technical Knowledge, Project Management, Troubleshooting, Construction, Communication, Teamwork, Leadership, Customer Service, Business Development, Marketing, Investigation, ProblemSolving, PreFunctional Testing, Functional Testing, Data Centers, ESOP, 401(k), Wellness Program, Flexible Work Arrangements',\n",
       " 'Data Engineering, Software Development, Big Data Architecture, Cloud Platforms, GCP, Python, Data Management, Relational Databases, NoSQL Databases, Cloud Systems, Cloud Architecture, Version Control, GitHub, Written Communication, Verbal Communication, Adaptability, Industry Trends, Technologies, STEM Degree, Management Experience, GCP Certifications, BigQuery',\n",
       " 'Data Conversion, Data Integration, Data Validation, Data Cleansing, Data Mapping, Data Extraction, ETL (extract transform load), Microsoft SQL Server, Workday Integration Certification, Enterprise Resource Planning (ERP), Automation, Python, SOAP, ReSTful web services, Workday Studio, EIBs, XML, XSLT, Workday Report Writing, Calculated Fields, Collaboration, Communication, ProblemSolving, Analytical Skills, Presentation, Organization, Prioritization, Time Management, SelfInitiation, Team Player, Flexibility, Creativity, Work under Pressure',\n",
       " 'Machine Learning, Big Data, Data Science, Business Intelligence, Data Analytics, Predictive Modeling, Advanced Analytics, Artificial Intelligence, Statistical Modeling, Data mining, Text Analytics, Natural Language Processing, Pattern Recognition, Data Extraction, Data Transformation, Data Visualization, Machine Learning Algorithms, Statistical Software, R, Python, Java, Scala, SQL, Hadoop, Spark, HDFS, MapReduce, Kafka, NoSQL, Databases, Tableau, PowerBI, Periscope, Business Objects, D3, ggplot, SAS Visual Analytics, Hadoop Distributed File System (HDFS), Hive, HBase, Sqoop, Pig, Oozie, ZooKeeper, Flume, Mahout, Hive, Pig, Oozie, ZooKeeper, Flume, Mahout',\n",
       " \"Risk analysis, Systems analysis, Process analysis, Vulnerability management, Security controls review, Policy and procedure review, Compliance reviews, Cybersecurity tools evaluation, Automation, Troubleshooting, Proactiveness, Unconventional solutions, Legal and regulatory compliance, Windows system administration, Unix/Linux system administration, Security protocols, Cryptography, Authentication, Authorization, TCP/IP networking, Audit, Information Systems, GRC, PCI DSS, FFIEC, Python, PowerShell, Microsoft's Power Platform tools, Power Automate, Power Apps, Power BI\",\n",
       " 'Data Visualization, Data Analytics, Statistical Analysis, Advanced Analytics, Predictive Modeling, Hadoop, Python, SQL, Project Management, Communication, Interpretation, Tableau, GeoSpatial Tools, SAS, SPSS, R, Excel, Reporting',\n",
       " 'Machine Learning, Actuary, Actuarial Science, Forecasting, Optimization, Finance, Project Management, Risk, Casual Insurance, Life Insurance, Data Science, Statistical Applications, Communication Skills, Business Risk, Team Management, Statistics, Economics, Applied Mathematics, Computer Science',\n",
       " 'Data Science, Statistical Modeling, Machine Learning, Python, Statistics, Business Acumen, Data Analysis, Problem Solving, Critical Thinking, Communication, 10+ Years of Experience',\n",
       " 'Data Analysis, SQL, Coding, Statistical Modeling, Predictive Modeling, Descriptive Statistics, Business Domain Knowledge, Financial Standards, SOX, FP&A, EndtoEnd Quantitative Thinking, Communication skills, Problem Solving, Organization, Time Management, Task Prioritization, Business Insights, Crossfunctional Teams, Business Objectives, Business Initiatives, Business Partners',\n",
       " 'Financial services, Money management, Customer service, Banking, Lending, IRA, CD, Loans, Deposits, Withdrawals, Transfers, Payments, KYC, AML, NMLS, Compliance, Leadership, Teamwork, Innovation, Problem solving, Adaptability, Communication, Flexibility, Multitasking, Attention to detail, Accuracy, Integrity, Ethics, Professionalism, Work ethic, Motivation, Passion for helping others, Learning and development, Career growth, Benefits, Salary, Vacation, Sick leave, Holidays, Wellness day, Volunteer time, Flexible spending account, Health savings account, 401(k) plan, Tuition reimbursement, Rate discounts on loans, Incentives, Bonuses, Equal opportunity employer, Diversity and inclusion, Pay transparency',\n",
       " 'SAP S4, Corrugated Container Characteristics, Product Hierarchy, Data transformation, Data governance, Data Stewardship, Data quality, Master data management, Business analysis, Requirements management, Process improvement, Communication, Teamwork, Problemsolving, SAP delivery, Change management, Business acumen, Attention to detail, Data optimization',\n",
       " 'Data Scientist, Clearance: U.S. Citizen Government Suitability Clearance, Dashboarding, Data Integration, Data Curation, Reporting Methodologies, Robotic Process Automation, Criminal Investigation, Caselevel Document Compilation, Indexing, Searching, Analytics, Natural Language Processing, Big Data Analytics, Data Visualization, Statistical Analyses, Clustering Techniques, Multiple Regression, Factor Analysis, Artificial Intelligence, \"Trustworthy AI\", Data Science Adoption, Coaching, Feedback, Training, SAS, R, SQL, Python',\n",
       " 'Generative AI, LLMs, Vector Stores, Cloud fundamentals, AWS, Distributed systems, Memory CPU profiling, Unit tests, System tests, Unix, Bash, Cloud networking, Docker, Kubernetes, Containerization, CI/CD, Git, Release Management, Machine Learning, AI systems, Python, Golang, Java, Rust',\n",
       " 'Data Entry, Banking, Customer Service, Retail, Confidentiality, Requirements Verification, Validation, Time Management, Comprehension, Interpersonal Skills, Verbal Communication, Written Communication, Accuracy, Attention to Detail, Prior Data Entry Experience, Customer Service Experience, Retail Experience',\n",
       " \"Supply Chain Data Analytics, Data Management, ConsumerFocused, DataDriven, Operational Insights, Operational Analysis, Operational Performance Improvement, Global Product Flows, Data Governance, Data Extraction, Data Storage, AI/ML Application, Visualization Management, Data Management Technologies, Automation Tools, Governance Approaches, FactBased Analysis, Data Maturity, Data Integrity, Process Adherence, Technical Documentation, Bachelor's Degree, Supply Chain, Industrial Engineering, Statistics, Computer Science, EndtoEnd Understanding, Data Management Strategies, Data Extraction, Data Storage, Visualization Tools, AI/ML Tools, Problem Solving, Analytical Skills, Presentation Skills, Written Communication, Verbal Communication, Leadership Ability, Team Management, Interpersonal Skills, Employee Development Skills, Energetic, Sense of Urgency, Strong Results, Flawless Service, Robust Data Foundation, Exceptional Relationships, Global Engagement, Total Rewards Program, Career Growth, Goose Rewards, ICON Rewards, CG Gives, Physical and Mental Health Support, Inclusion, Diversity, Equity\",\n",
       " 'ETL, Data extraction, Data transformation, Data loading, Data mapping, Analytical support, Operational support, Maintenance support, Enterprisewide systems, Application software, Data flows, Hardware/software compatibility, Interface design, Emerging technologies, Technical documentation, Troubleshooting, Customer support, Machine characteristics, Storage capacity, Processing speed, Input/output requirements, Computer Science, Electrical Engineering, Computer Engineering, Java, J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML, XHTML, CSS, Python, Shell Scripting, JSON, Windows, Linux, Distributed Computing, Blade Centers, Cloud infrastructure, Problem solving, Database methodologies, Continual process improvement, Proactive approach, Task completion, Equal Employment Opportunity, Affirmative Action',\n",
       " 'AWS, Java, Scala, Python, SQL, RDBMS, NoSQL, Redshift, Snowflake, Machine learning, Distributed microservices, Full stack systems, Cloud computing, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Data warehousing, UNIX/Linux, Shell scripting, Agile engineering',\n",
       " \"UPS, Generators, HV, PDU's, CRAC units, Graphical information, Alarm data, Electrical apprenticeship, Industrial/manufacturing/commercial/critical environment, Main distribution board switching, ACB's, UPS modes, Critical cooling systems, SLA's/KPI's, Air Conditioning, Humidity control, BMS systems, Plant monitoring, H&S requirements\",\n",
       " 'Data Science, Statistical Analysis, Clinical Trials, Registries, RealWorld Data, Data Transformation, Data Mining, Machine Learning, Artificial Intelligence, Data Analytics, Data Visualization, Data Warehousing, Data Management, Data Governance, Databases, SAS, R, MATLAB, Python, Java, C++, Clinical Development, Clinical Research, Regulatory Affairs, Bioinformatics, Statistics, Mathematics, Computer Science, Psychology, Finance, Bachelor of Science (BSc), Master of Science (MSc), Doctoral Degree (PhD), ICH Guidelines, GCP Guidelines, Statistical Methodology, Clinical Development Plan, English (Fluent)',\n",
       " 'SQL, Data governance, Data modeling, Data munging, Data management, Data warehousing, Data lakes, Data integration, Data pipelines, Big data platforms, Cloud technologies, Serverless environments, Python, R, RDBMS, NoSQL, PostgreSQL, MySQL, MongoDB, Microsoft Power BI, Spotfire, Machine learning, Artificial intelligence, Databricks, Data Virtualization, SQL Server, Streamsets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, Oil and gas operations',\n",
       " 'EQuIS software, SQL, Data management plans, SOPs, QAPPs, Metadata, Web applications, Data portals, Data analysis, Compliance reporting, Technical support, Power BI, SharePoint, Data collection, Data processing, Data communication, Database skills, Programming languages, Deductive reasoning, Accuracy, Completeness, Environmental data lifecycle, Data analytics, Dashboards, Environmental compliance, Environmental projects, Freshwater, Groundwater, Marine environments, Biological sciences, Engineering, Environmental sciences, Natural resource sciences, Technical expertise, Clientfocused delivery, Project management',\n",
       " \"Database Development, Data Analysis, ETL/ELT, Data Migration, Stored Procedures, Data Preparation, Data Architecture, SQL, Banking Domain, Communication Skills, Teamwork, Attention to Detail, Bachelor's in Computer Science, 6+ Years of Experience, Financial or Payment Processing Experience, Data Mart Experience, Background Check, Drug Test, FullTime, MondayFriday Schedule, Health Insurance, Dental Insurance, Vision Insurance, 401(k) Matching, Life Insurance, Paid Time Off, Parental Leave, Disability Insurance, Childcare Assistance, Education Reimbursement, Fitness Membership, Volunteer Time Off\",\n",
       " \"Medical Technology, Laboratory Analyst, Blood Analysis, Urine Analysis, Spinal Fluid Analysis, Gastric Juice Analysis, Blood Transfusion Preparation, Blood Group Typing, Microbiology, Antibiotic Sensitivity Testing, Laboratory Equipment Calibration, Preventive Maintenance, Laboratory Inventory Management, Infection Control, Hazardous Waste Management, Quality Assurance/Quality Improvement, Laboratory Safety, Problem Solving, Employee Training, Bachelor's Degree in Medical Technology, MT Certification or Eligibility, Equal Opportunity Employer\",\n",
       " 'Microsoft Office Suites, IDX, HealthQuest, Proteus, SQL Programming Langauage, IMACS, MedAssets AVEGA Cost Accounting System, Financial analysis, Data extraction, Data analysis, Budgeting, Financial accounting, Business planning, Reporting, Communication, Teamwork, Project management, Problemsolving, Detailoriented, Accuracy, Time management, Analytical thinking, Critical thinking, Decision making',\n",
       " 'Machine Learning, Natural Language Processing, Python, NumPy, Pandas, Numba, PyTorch, TensorFlow, Jupyter, Deep Learning, Model Efficiency Optimization, Cloud Services (GCP AWS Azure), Speech Processing, Audio Classification, MLFlow, WandB, DataBricks, Sprint Management, Agile Methodologies, FDA Approvals, Product Management, Research, Medical Device Software, Communication, Teamwork, Problem Solving, Adaptability, Collaboration, Strategic Planning, Agile Planning, MVP (Minimum Viable Product), Feedback, Time Management',\n",
       " 'Medical Technology, Medical Laboratory Technician, MT/CLS (ASCP), MLT/CLS (ASCP), Blood Bank, Clinical Laboratory Tests, Data Analysis, Diagnosis, Disease Treatment, Equipment Operation and Maintenance, Laboratory Equipment, FastPaced Work Environment, HighVolume Lab Experience',\n",
       " 'AI fundamentals, Cloud Data Platform (Azure) pipelines, ETL principles, Dimensional modelling, ETL, Reporting tools, Data governance, Data warehousing, Structured data, Unstructured data, SAP/NoSQL clusters, Snowflake, Data models, RDBMS, Operational data store (ODS), Data marts, Data lakes, Azure Data Factory, Databricks, Integration Runtime Services, SQL, SQL Analytics, TSQL, Python, Scala, Performance tuning, Agile methodologies, Stakeholder management',\n",
       " 'LLM, Generative AI, Document extraction, Chat, Python, Knowledge graphs, Ontologies, Cloudnative environment, AWS, AWS Neptune, Neo4J graph database, Graphbased ontology, Structured data, Unstructured data, ML models, Big data infrastructure, AWS Tex tract, AWS Comprehend Medical, Code level experience',\n",
       " 'Data Center Engineering, Network Architecture, Network Design, Network Configuration, Network Testing, Network Integration, Network Security, Network Troubleshooting, Cisco ACI, Nexus Dashboard, VxLAN Fabric, Nexus 7k 9k FEX, CCNA/CCNP/CCIE, Public Cloud Certifications, Network Architecture Experience, Network Centric Mode, Application Centric Mode, Spine and Leaf Architecture, Data Center Networking, BGP, ISIS, VRFs, QoS, MPLS, Segment Routing, Connectivity to HighSpeed Public Internet Facilities, Highly Available Network Architectures, Failover, Redundant Configurations, SoftwareDefined Networking, Traffic Engineering, Network Configuration Automation, Perl, Python, Bash',\n",
       " 'BSA/AML Compliance, Record Keeping, Report Generation, Microsoft Office, Organizational Skills, Time Management, Regulatory Compliance, Communication Skills, Analytical Skills, Training and Development, Cash Handling, Currency Transaction Reports, Monetary Instruments, Beneficial Ownership, Risk Management, Financial Crime, KYC (Know Your Customer), CIP (Customer Identification Program), FinCEN (Financial Crimes Enforcement Network), CTR (Currency Transaction Report), Branch Audits, Trend Analysis, Research, Presentation Skills, Project Management',\n",
       " \"Data Center Technician, Server Hardware, Cabling, Labeling, Data Center Management, Data Center Guidelines, Physical Environments, DaVita Data Centers, Troubleshooting, Hardware Inventory, Maintenance Logs, After Hours Support, Supervision, Physical Activity, Step Ladder, Confidentiality, Data Center Architecture, Data Center Experience, High School Diploma, Data Center Infrastructure Manager (DCIM), Certified Network Cable Installer (CNCI), BICSI Certifications, Data Center Certified Associate Certification, Competitive Compensation, Full Benefits, Industry Leading Organization, TeamOriented Culture, Employee Stock Ownership Program, Engagement Manager, Medical Dental Vision and Life Insurance, ClearlyRated's Best of Staffing® Client and Talent Award\",\n",
       " 'Data Engineering, ETL Development, Data Pipelines, Python, AWS Data Analytics, Amazon Athena, AWS Glue, Data Modeling, Kimball Design Approach, Inman Design Approach, Data Vault Design Approach, Consulting Skills, Analytical Skills, Written Communication, Verbal Communication, Presentation Skills, Exploratory Data Analysis, Data Cleansing and Aggregation, Terraform, Star Schema, PySpark, Utility Industry',\n",
       " 'Research Assessment, Participant recruitment, Participant retention, Data collection, Data entry, Data maintenance, Data cleaning, Data reporting, Data analysis, Database creation, Database editing, Database maintenance, Participant tracking, Incentive distribution, Presentation assistance, Poster creation, Manuscript preparation, Literature reviews, General office duties, Microsoft Word, Microsoft Excel, Microsoft Outlook, REDCap, Interpersonal skills, Communication skills, Problemsolving skills, Prioritization skills, Attention to detail, Initiative, Independence, Flexibility, Empathy, Ability to engage diverse populations, High school diploma or equivalent, 12 years of related experience, Reliable/efficient transportation',\n",
       " 'AWS, Cloud infrastructure, Python, Databricks, Snowflake, Spark, Agile methodologies, Atlassian tools, Jira, Git, Bitbucket, CI/CD, Kubernetes, Monitoring, Alerting tools, S3, IAM, ETL methods, Dimensional data models, Data warehouses, SQL, Relational databases, Data access methods, Data tools, Data manipulation',\n",
       " 'Data center maintenance, Server installation, Network hardware installation, Storage hardware installation, Cabling/patching, Copper cabling, Fiber optic cabling, Decommissioning hardware, Shipping coordination, Remote hands duties, Power cycling equipment, Hardware component replacement, Network card reseating, Cabling testing, Network activity checks, Firewall status checks, Error detection, LED alerts, System monitoring tools, Audit work, Cabling documentation, Hardware documentation, Incident diagnosis, Incident resolution, Access control, Visitor management, ISO 9001 compliance, ISO 27001 compliance, ISO 20000 compliance, ITIL compliance, Ticket management, Local policies and procedures, SC security clearance, BPSS security clearance, Problem solving, Troubleshooting, Fault diagnosis, Ownership of issues, Resolution of issues, Learning attitude, Professionalism, Flexibility, Communication skills, Teamwork, Driving license, Personal car',\n",
       " 'Data Science, Data Analytics, Statistical Modeling, Data Modeling, Quantitative Analysis, Python, SQL, Spark, Git, Jupyter, AWS, Azure, Databricks, Statistical Analysis, Testing Frameworks, Machine Learning, Data Engineering, A/B Testing, Team Leadership, Mentoring, Communication, Relationship Building, Problem Solving, Business Intelligence, DataDriven Storytelling, User Journey Analysis, Customer Engagement, Customer Conversion, Customer Retention, Business Strategy, Root Cause Analysis, Forecasting, Data Visualization, Data Interpretation, Data Integration, Data Warehousing, Data Management, Data Mining',\n",
       " 'Microsoft Office Suite, Project management software, Mechanical systems, Electrical systems, HVAC, Plumbing, Low Voltage, Fire Sprinkler, LEED accreditation, Leadership, Interpersonal skills, Construction personnel hoists, Lifting up to 50 lbs, Airborne particles, Caustic chemicals',\n",
       " 'Data Science, Machine Learning, AI, Algorithms, Predictive Modeling, Business Challenges, TimeSeries Data, Machine Learning Tools, Community Building, Leadership',\n",
       " 'Data Analytics, Data Warehousing, SQL, ELT, AWS, Python, Apache Spark, Kafka, Apache Airflow, Data Reporting, Data Cataloging, Microstrategy, Tableau, Looker, Alation, Cloud Computing, Agile Development, Communication Skills, ProblemSolving, Organizational Skills, Leadership',\n",
       " 'Spark, Scala, Databricks, AWS, AWS S3, Big Data, SQL, Cloud terminologies, AWS Glue ETL, Java, Python, RDD, Dataframes, Datasets, Streaming, Git Hub, Jenkins, Artifactory, CI/CD, Agile scrum, Talend Big Data, Unit testing, UAT, Prod Deployment, AWS Cloud, AWS Glue ETL, Databricks Delta, Redshift',\n",
       " 'PowerBI, SAP, SharePoint, SQL, Synapse, Snowflake, PLM, Data mining, Data cleansing, Data validation, Error handling, Data infrastructure, Data processing, Data storage, Data retrieval, Data analysis, Data modeling, Statistical analysis, Data visualization, Dashboards, Data dictionaries, Data lineage, Front end applications, PowerApps, SharePoint Classic, SharePoint Online, QuickBase Applications, Databases, Custom APIs, Webservices, Databricks, AWS, Cloud systems, Excel, Gsheets, Supply chain processes, Supply chain terminology, Communication skills, Presentation skills, Teamwork',\n",
       " 'Data Platform, Data Modeling, Data Analytics, BI Tools, DBT, SQL, Snowflake, Experimentation, Product analytics tools, Datadriven product development, Data Science, Data Cleanup, Data Warehousing, CloudBased Warehouses, Platform mindset, Agile, Scalability, SQL, DBT, Snowflake, Data Integration, Data Transformation, Data Analysis, Data Visualization, Data Reporting, Data Mining, Machine Learning, Artificial Intelligence, AWS, Azure, GCP, Python, R, SAS, SPSS',\n",
       " 'Biostatistics, Statistics, Public Health, Data Science, Business Intelligence (BI), SAS, Excel, R, Python, Data Analysis, Data Interpretation, Data Gathering, Data Preparation, Data Modeling, Data Visualization, Microsft Excel, SAS, R, Python, Open Source Tools, Statistical Modeling, Programming',\n",
       " \"Data analysis, Data interpretation, Data modeling, Data validation, Data visualization, Data mining, Data reporting, Business intelligence, Datadriven insights, Data requirements, Data accuracy, Data consistency, Statistical analysis, Data presentation, Data visualization tools, Tableau, Power BI, QlikView, Alteryx, SQL, Python, R, Excel, Data preparation, Data transformation, Data cleansing, Data integrity, Problemsolving, Attention to detail, Communication skills, Presentation skills, Teamwork, Accuracy, Business acumen, Datadriven recommendations, Statistical analysis techniques, Methodologies, Bachelor's degree, Mathematics, Statistics, Economics, Computer science, Data analysis tools, Programming languages\",\n",
       " 'Data Analysis, Tableau, Tableau Desktop, R, RStudio, Data Visualization, Dashboard Development, Report Development, Operations Research, Statistics, SQL, Python, Hadoop, Spark, Cassandra, AWS, Azure, Government Clearance, Public Trust, U.S. Citizenship',\n",
       " 'Data Analytics, Data Management, Data Cleansing, Data Enrichment, CRM Software, AI Tools, Data Integrity, Data Analysis, Data Visualization, Business Intelligence, SQL, APIs, Project Management, Stakeholder Management, Communication Skills, Business Process Knowledge, ERPs, CRMs, Integrations, SAP, Finance',\n",
       " 'SQL, SQL databases, Microsoft Access, JavaScript, Power BI, Data wrangling, Data focused solutions, Stakeholder engagement, Communications skills, Freight sector knowledge, Retail sector knowledge, Onsite work',\n",
       " \"Data Privacy, Data Protection, Privacy Certifications, Data Subject Access Requests (DSAR), DPIA's, Privacy Regulations, Privacy Policies, Risk Assessments, Project Management, Privacy Incident Investigation, Privacy Training, Regulatory Compliance, Confidentiality, Collaboration, Communication, Problem Solving, Time Management, Attention to Detail, Regular Attendance, Professionalism, Safety\",\n",
       " \"SQL, Tableau, A/B Testing, Personalization, MultiArmed Bandits, Machine Learning, Data Mining, Data Analysis, Problem Solving, Project Management, Communication, Data Visualization, Experimental Design, Statistical Modeling, Business Intelligence, Data Warehousing, Data Integration, Data Governance, Data Security, Data Ethics, Bachelor's Degree (Math Statistics Computer Science), Master's Degree (Data Science Business Analytics)\",\n",
       " 'Cyber Risk, Machine Learning, Generative AI, Risk Management, Cloud Risk Management, Process Management, Project Management, Compliance, Legal, Regulatory, Operations, Technology, Cybersecurity, Business Process Management, PRIME system, Data model, Change Management, Risk Guide, Controls, Software as a Service (SaaS), Risk Certifications (CRISC CISM CRCM CIPP ABA Risk Mgmt Certification)',\n",
       " 'Software, Data Products, Azure, Cloud Environment, Data Governance, Data Modelling, Data Warehouse, Data Structures, Algorithms, Data Lifecycle, Data Distributions, Data Partitions, Disaster Recovery, High Availability, Query Languages, Source Control, CI/CD Pipelines, Scrum/Agile, Python, Scala, Java, SQL, Azure DevOps, Azure Data Factory, Azure Data Lake, Azure SQL DB, Synapse, Cosmos DB, Data Management Gateway, Azure Storage Options, Stream Analytics, Event Hubs, Databricks, Spark, Delta Lake',\n",
       " 'SQL, Data Visualization, Data Management, Analytical Mindset, Management/Mentoring, Member Journey Analysis, Pain Point Identification, Business Function Alignment, Stakeholder Alignment',\n",
       " 'Data Analysis, Reporting, Databases, Dashboards, Spreadsheets, Data Integration, Data Presentation, SharePoint, MS Power Automate, MS Power Apps, Power BI, MS Excel, Visual Basic, Macros, Data Connections, Pivot Tables, MS Access, SQL Server, Engineering Processes, Project Management, Public Sector Experience, Independent Work, Written Communication, Verbal Communication, Overtime, Commuter Benefits, Health Insurance, Sick Time, 401k',\n",
       " 'Autonomy, Artificial Intelligence, Machine Learning, Computer Vision, Data Science, Software Engineering, Robotics Engineering, Python, C++, OpenCV, PyTorch, TensorFlow, DevOps, Docker, Linux, Agile, JIRA, Bitbucket, Confluence, Atlassian, AI/ML, Computer Vision, Data Processing, Software Development, Software Testing, System Analysis, System Design, Requirements Gathering, Communication, Problem Solving, Analytical Skills, Decision Making, Team Work, Multitasking',\n",
       " 'Machine learning, Python, Java, JavaScript, C/C++, Data structures, Algorithms, Computer vision, Natural language processing, Federated learning, Distributed training, PyTorch, TensorFlow, Statistical heterogeneity, System heterogeneity, Security, Privacy, Communication skills, Software engineering, MLOps, LLMOps, Geodistributed machine learning, Observability, Evaluation, Governance, Collaboration, Artificial intelligence, Healthcare, Finance, Insurance, Automotive, Advertising, Smart cities, IoT, Computer vision, Natural language processing, Data mining, Timeseries forecasting, Computer science, Distributed computing, Cloud computing, Networking, Mobile development, Systems design, Scalable infrastructure',\n",
       " 'Machine Learning, Deep Learning, Model Training, Model Deployment, Model Evaluation, Data Analysis, Fraud Detection, Risk Management, Credit Risk, Transaction Fraud, Identity Theft, Model Scoring, Model Infrastructure, Statistical Analysis, Hypothesis Testing, Quantitative Analysis, Data Manipulation, Data Querying, Metric Definition, Data Slicing, Advanced Degree, Production Environment, Industry Experience',\n",
       " \"Database Architecture, SQL Optimization, DevOps SQL Environment, Database Configuration and Optimization, Team Collaboration, SQL Server Data Tool Suite, SQL Server IAM Administration, Visual Studio Database Projects, SQL Server/Python Interactivity, Azure DevOps, ETL Environment, Memory Utilization Optimization, Data Integration, Data Warehousing, Data Modeling, Data Quality, Data Security, Data Governance, Data Analytics, Business Intelligence, Software Development, Programming, Communication, Problem Solving, Critical Thinking, Analytical Skills, Attention to Detail, Bachelor's Degree in Computer Science Information Systems or related field\",\n",
       " 'Content Management, Joomla, Website Management, Content Management System, Wildlife Conservation, Biodiversity',\n",
       " 'Teamwork, Statistics, Data Modeling, Advanced Mathematics, Machine Learning, Simulation, Python, R, SQL, Hadoop, Deep Learning, Data Analysis, Data Science, Exploratory Data Analysis, Critical Thinking, Research, Healthcare, Patient Care, Health Outcomes, Mathematics, Programming',\n",
       " 'Data Engineering, AWS Data Analytics Stack, Amazon Athena, AWS Glue, ETL and Data Pipelines, Python, Terraform, Kimball, Data Modeling, Inman, Data Vault Design Approaches, ELT Jobs, Data Vault Design Approaches, Star Schema, PySpark, Exploratory Data Analysis, Data Cleansing, Data Aggregation, Consulting Skills, Analytical Skills, Written & Verbal Communication, Presentation Skills, Data Lake, Data Processing, Project Management, Data Science, Visual Analytics, Big Data Processing, Meeting Facilitation',\n",
       " 'Data architecture, Cloud engineering, Data Lake platform implementation, Terraform, AWS services, RDBMS, Spark, Hadoop, Kafka, Java, Python, CI/CD pipelines, Infrastructure as code, Serverless big data solutions, Leadership skills, Mentoring, Influencing, Partnering with engineering teams, Development of patterns, Standards and best practices, Data architecture strategy, Data solutions identification, Data engineering team management, Enterprise solutions delivery, System architectures design, Documentation creation, Proof of concepts design, Analytical skills, Problemsolving skills, Teamwork, Collaboration, Startup personality, Passion for fastpaced environments',\n",
       " 'Data Architect, AWS, Snowflake, Informatica, Fivetran, Hightouch, Python, Data modeling, Database development, Data warehousing, Analytics, ETL, Azure, Business Intelligence, Master Data Management, Data staging, Data Modeling, Database Performance Tuning, ASP.NET, ADO.NET, ASP.NET MVC, Razor, Entity Framework, LINQ, Microsoft Access, Oracle, PL/SQL, Oracle workbench, C++, C#, Visual Basic, Postgres DB, Vertica DB, MySQL, graphic database design, Visual Basic Database programming, OLEAutomation reports, Seagate Crystal Report Designer 9, Pentaho BI Solutions, Mondrian SaikuServer, Python, BASH/Script programming, VBA programming, Excel, Power BI, Word, Outlook, Data architecture, Platform selection, Technical architecture design, Application development, Testing, Implementation, Data management, Data accessibility, Data accuracy, Data security',\n",
       " 'Google Cloud Platform, Data & Analytics, Data infrastructure and Visualization, Infrastructure Services, Collaboration & Productivity, Locationbased Services, ETL/ELT, Data Pipelines, Data Warehouses, Data Lakes, Structured and unstructured data, Analytics, Reporting, Cloud data platforms, Scalability, Reliability, Security, Performance, Best practices, Data privacy, Compliance, Data governance, Cloud computing, Infrastructure, Storage, Platforms, Data, Python, Java, Javascript/Typescript, TerraForm, Scala, SQL, AWS, Azure, GCP, S3, BigQuery, Communication, Interpersonal skills, Cloud computing, Data, Information lifecycle management, Big Data, Data migrations, Computer Science',\n",
       " 'Data extraction, Data cleansing, Data integration, B2B experience, Lead generation, Data privacy regulations, Data quality checks, Data mapping, Data transformation, Excel, Microsoft Dynamics, Data standards, Data integrity, Data accuracy, Minimum Essential Coverage (MEC), Medical plan, Dental, Vision, Term life, Discount prescription program, Critical illness, Accident, Telebehavioral health, 401(k) plan, Sick leave, Minimum Value (MV) PPO, Employee Stock Purchase Plan, Paid holidays',\n",
       " 'Data Ingestion, Data Warehousing, Data Pipelining, Data Modeling, Data Types, SQL, Data Engineering, CTEs, Complex Stored Procedures, Snowflake, DBT, Python, OOP, Unit Testing, Prefect Orchestration, Internal Documentation, HL7, AWS, SAM, Lambda, S3, API Gateway, GCP, Cloud Functions, GCS, Big Query, Salesforce API v51+, Salesforce, Streamlit',\n",
       " 'Data analytics, Data storage, Data management, Data retrieval, Data visualization, Statistical analysis, Survey analysis, Sales analysis, Operational data analysis, Data communication, Government operations, Utilities',\n",
       " 'Data Engineering, Data Analysis, Business Intelligence, Tableau, Data Cleaning, Data Transformation, Apache Spark, Hudi, EMR Cloud Services, Kubernetes, Data Pipelines, Data Provenance, Data Preprocessing, Data Standards, ETL Processes, Database Administration, Oracle, MySQL, MariaDB, MongoDB, ElasticSearch, API Connectors, Data Catalog, Data Mapping, Data Models, Analytical Thinking, Critical Thinking, Teamwork, Collaboration, Python, R, SQL',\n",
       " 'Python, Java, Apache Spark, Apache Hive, BigQuery, Distributed Computing, Data Engineering, Data Integration, Data Modeling, Data Cleansing and Transformation, Query Optimization, Database Management, Data Quality Checks, Data Validation, Advanced Data Management, Machine Learning Pipelines, Data Security and Privacy, Data Analytics, ProblemSolving, Troubleshooting, AdTech Platforms, Cloud Computing, Software Development, Project Management, Agile Development, Communication Skills, Collaboration Skills, Data Visualization, Dashboard Creation, Experiment Design, A/B Testing, Statistical Analysis, Data Science, Data Mining',\n",
       " 'Python, RDBMS, Kafka, SQL, PySpark, Hive, Logistic regression, Naïve Bayes, SVM, Decision trees, Neural networks, AWS, Java, Tensorflow, PyTorch, Natural Language Processing, ML systems, Streaming data flows, Computer science, Electrical engineering, Statistics, Econometrics, Operations research, Signal processing, ETL data pipeline, Data engineering, Machine learning, Predictive analytics, Prototyping, Model validation, A/B testing, Model efficiency, Model performance, Fintech, Legal industry',\n",
       " 'MLOps, AzureML, Azure DevOps, Azure Kubernetes Service (AKS), Azure Databricks, Machine learning frameworks, scikitlearn, Azure Resource Manager templates, Infrastructure as Code (IaC), Git, CI/CD pipelines, Automation scripting, Python, PySpark, PowerShell, Azure CLI, Security standards, Agile methodology, SAFe methodology, Problemsolving, Troubleshooting, Cross global location experience, Azurespecific certifications, Collaboration, Data science, Engineering, Data pipelines, Data governance, Integration, Versioning, Lineage tracking, Data management, Security, Regulatory standards, Monitoring, Application performance, Troubleshooting, BI, Code quality',\n",
       " 'Database Management, System Administration, IT Application Development, SAP Software Systems, Java, XML, C2S, Tableau, Apache Tomcat, HTML, HTML5, Cascading Style Sheet (CSS), Web API (Application Programming Interface), WCF (Windows Communication Foundation), SQL, Risk Management Framework (RMF)',\n",
       " \"Data analysis, Predictive modeling, Cash flow modeling, Loan performance modeling, Data preparation, Data cleansing, Variable reduction, Model performance assessment, SAS, SAS EG, Python, R, SQL, Statistical programming, Statistical techniques, Risk classification algorithms, Credit risk scoring, Model validation techniques, Model metrics, Communication skills, Continuing education, Selfstarter, Fastpaced environment, Collaborative teamwork, Bachelor's degree, Master's degree, PhD\",\n",
       " 'Data Science, Python, Pandas, Sklearn, SQL, Data Structures, Algorithms, Machine Learning, Natural Language Processing, Recommender Systems, Classifiers, Chatbots, Jupyter Notebooks, MLflow, Databricks, AWS Cloud Solutions, S3, Glue, Lambda, SageMaker, Spark, Hive, Hadoop, Pig, HiveQL, Pig Latin, Oozie, Flume, Sqoop, HBase, Cassandra, MongoDB, Redis, Elasticsearch, Kibana, Logstash',\n",
       " 'Python, Java, Computer Science, Information Systems, Data Architect, Cloudbased Data Solutions, AWS, S3, Redshift, Aurora, Glue, EMR, EventBridge, Lake Formation, SQL, Data Querying, Performance Optimization, Cloud Infrastructure Technologies, Containers, Kubernetes, Serverless Computing, ProblemSolving, Analytical Skills, Data Structures, Data Modelling, Data Lineage, Data Catalogues, Streaming, Messaging, Kafka, Kinesis, Data Security, Privacy, Encryption, Access Controls, Compliance, Data Protection Regulations, Snowflake, Cloud Platforms, Data Management, AWS Certified Solutions Architect',\n",
       " 'Data Analyst, ETL, SCRUM, SQL, Excel, Pentaho Kettle, Laserfiche, Rootcause analysis, Data quality, Business processing, Communication, Report development, Agile development, Testing, Civil judgment, Tax lien, Bankruptcy, Public record data',\n",
       " 'Data Architecture, Data Management, Data Integration, Data Visualization, Big Data, SQL, Data Modeling, Informatica, Microsoft Data Services, MicroStrategy, Tableau, Denodo, Collibra, Erwin, ETL, API, Data Mining, Data Quality, Data Governance, Predictive Analytics, Relational Databases, Cyber Security, Team Leadership, Communication Skills, Technical Problem Solving',\n",
       " 'Data Processing, Data Analytics, Data Science, Manufacturing Domain, Data Analysis, Hypothesis Development, Recommendation Making, Simulation Development, Optimization Tools, Scenario Planning, Predictive Model Building, Prescriptive Model Building, Production Model Maintenance, Written Communication, Verbal Communication, Presentation Development, Presentation Delivery, Cloud Big Data, Cloud Database, Data Management, Data Pipeline Development, Microsoft Azure, Azure Data Warehouse, Databricks, Azure Analysis Services, Power BI, Operations Research, Management Science, Industrial and Systems Engineering, Statistics, Mathematics, Economics, Computer Science, Algorithm Design, Algorithm Development, Data Modeling, Business Insights, Solution Approach, Collaboration, Project Leadership, Project Initiatives, Solution Development, Task Execution, Analytical Methodologies, Report Preparation, Presentation Preparation, Technical Communication, NonTechnical Communication, Data Science Practice, Replicable Solutions, Data Product Codification, Recipe Development, Project Documentation, Process Flowcharts, Data Science Knowledge, Technical Skill Sets, Data Sources, Business Travel',\n",
       " 'DataCenter Technician, DataCenter Analyst, Database tools, Ad hoc reporting, Advanced Excel skills, Pivot tables, Lookups, Asset management, Asset lifecycle, Financial metadata, Asset audits, Contract opportunity, Onsite, 12 Months, Pay Range: $30$31 per hour, Health benefits, Dental benefits, Vision benefits, Term life, Short term disability, AD&D, 401(k), Sick time, Paid leaves, Employee Assistance Program (EAP), Equal Opportunity Employer, Reasonable accommodation',\n",
       " 'Data Mining, Data Management, Data Analysis, Data Standardization, Report Development, Report Execution, Data Entry, Data Interpretation, Data Quality Control, Database Maintenance, PCMS, P&IDs, PFDs, ISOs, U1',\n",
       " 'GCP, Data architecture, Migration, Implementation, Platform, Data flow, Data quality, ETL, Data movement, Integration, Compute Engine, App Engine, Kubernetes Engine, Cloud Storage, BigQuery, Dataflow, Logical data model, Physical data model, ETL, Reporting, Analytics, Source to target mapping, Interface processes, Standards, Data issues, Mentoring, GCP Best practices, Industry standards, Documentation, GCP solutions, GCP architectures, GCP design patterns, Functional requirements, Nonfunctional requirements',\n",
       " \"Big Data Developer Lead, Agile, SQL, Programming Languages, Data Warehousing, Data Mining, OTLP, ERP, Data Lake, ELT/ETL Tools, Informatica, Oracle, CI/CD, Jenkins, Bamboo, Hadoop, MapReduce, Pig, Hive, HBase, Flume, ZooKeeper, MongoDB, NoSQL, Cassandra, Spark, Scala, Python, Linux, Kafka, Redis, Hortonworks, Business and IT Operations, Enterprise Data Solutions, Technical Architecture, Data Platforms, Enterprise Data Architecture, Written and Verbal Communication, Data Analysis, Data Infrastructure, Technical Solution and Architecture, Machine Learning, Pattern Recognition, Text Mining, Clustering, AI, Functional and Technical Requirements, Reusable Artifacts/Frameworks, Reusable Assets, Industry Solutions, Reference Architecture, Design, Development, QA Best Practice, Product Marketing, Product Owners, Scrum Masters, Senior Business Analysts, Agile Team, Onshore and Offshore Team, Technical Guidance, Requirements and Design, Software Architecture, Coding, Integration, Testing, Feature Definition, Computer Science, Computer Engineering, Software Related Discipline, Master's Degree, Conceptual Data Models, Logical Data Models, Physical Data Models, Enterprise Data Modeling Tools, Strategic Data Planning, Standards, Procedures, Governance, Data Quality Engineering, Metadata Consolidation, Metadata Integration, Metadata Model Development, Metadata Maintenance, Repository Management, Data Warehouse Design, Data Mining, Data Security, Enterprise Data Warehouse, Modeling Constructs, Methodologies, Practices, Physical Databases\",\n",
       " 'Data science, Artificial intelligence, Data engineering, User experience design, Software development, Data architecture, Performance optimization, Multinode data distribution, Partitioning, Indexing, Inmemory data structures, Parallelization techniques, Explain plans, Tracing techniques, SQL, SQLbased scripting languages, ELT/ETL design, Development, Implementation, APIs, Relational databases, Semistructured (JSON), Flat files (CSV), Data lineage, Data profiling, Data quality, Data warehouse security, Dedicated/system users, Roles, Rowlevel security, Masking/anonymization, SSO capabilities, Time management, Organizational skills, Agile environment, Leadership experience, Attention to detail, Startup environment, Ambiguity, Google, AWS, SAP, Oracle, Microsoft',\n",
       " 'Python, API Integration, SQL, Tableau, Tableau Prep, Data Engineering, Excel, Big Data, Snowflake, Apache Spark, Pyspark, Hive, Presto, HMS, Scala, Data Exploration, Data Transformation, ETL, Jupyter Notebook, Data Modeling, Reporting, Business Requirements, Data Visualization, Coherent Insights, Data Driven Decision Making, Tableau Expertise, Production Tableau Dashboards, Technical Background',\n",
       " 'Data Analysis, Statistics, Business Analytics, Data Visualization, Tableau, Power BI, Python, SQL, Excel, Data Preparation, Modeling, Data Structures, Requirement gathering, Project Specifications, GitHub, R',\n",
       " 'Data Analysis, SQL, AWS Redshift, MS Office Suite, Database Management, Email and MS Teams, Internet, Intranet, Share Point, Mathematical Ability, Data Modeling, Data Interpretation, ProblemSolving, Research Skills, Methodical Approach, Logical Thinking, Work Planning, Deadlines Management, Accuracy, Attention to Detail, Interpersonal Skills, Team Work, Verbal Communication, Written Communication',\n",
       " 'Data Analysis, Data Interpretation, Data Modeling, Statistical Concepts, Data Visualization, Data Mining, Data Cleaning, Business Intelligence, SQL, DB2, Microsoft Office, Relational Databases, Visual Basic, Macros, Tableau, Power BI, Statistical Analysis, Data Ethics, Attention to Detail, Critical Thinking, Data Preprocessing, Financial Reporting, Visio, MS Project, Data visualization Tools, Certified Business Analysis Professional (CBAP), HIPAA, LOMA, Proficiency in data cleaning, Creating effective visualizations, Ability to solve problems, Business knowledge, Knowledge of data privacy',\n",
       " 'Data Science, Analytics, Machine Learning, AI, Python, SQL, SAS, Big Data, Mathematical Optimization, Graph Algorithms, Statistics, Databases, Project Management, Communication, Problem Solving, Leadership, Consulting, Negotiating, Conflict Resolution, Business Intelligence, Visualization, Experimental Design, Research, Presentation, Data Mining',\n",
       " 'DB2, Teradata, DataStage, PySpark, Data Migration, Teradata Architecture',\n",
       " 'Informatica, IICS, Data architect, Data warehousing, AWS, Red shift, Communication, Life science, Supply chain',\n",
       " 'DB2, Teradata, DataStage, PySpark, Data Migration, Decision Making, Problem Solving, Analytical Thinking, Resource Management, Business Acumen',\n",
       " 'Data analysis, Attention to detail, Multitasking, Time management, Written and oral communication, Crossfunctional collaboration, Personal and professional integrity, Microsoft Excel, SAP (preferred), KPI database maintenance, Report generation, NPI (new product introduction) BOM and service cost, OEM cost claim validation, iCost tool administration, BOM structure, Costing, Materials cost, Price troubleshooting, Vendor management',\n",
       " 'Power BI, SQL, Data Visualization, Business Intelligence, Data Analysis, Data Manipulation, Data Integrity, Data Reporting, Project Management, Analytical Skills, ProblemSolving Skills, Interpersonal Skills, Team Interaction, Communication Skills, Presentation Skills, Time Management, Prioritization, Attention to Detail, Initiative, Results Orientation, MS Office Suite, Python, R, Marketing, Retail Category Management, CPG, Rewards Programs',\n",
       " 'C#, RESTful Services, SQL Server, Azure SQL, SSIS, ADF, PowerApps, Flow, PowerBI, Data Transformation, Query Optimization, Microsoft Data Platforms, Database Concepts, Normalization, Indexing, Physical Modelling, Logical Modelling, SQL Queries, Performance Tuning, RESTFul Services Development, API Integrations',\n",
       " 'Excel, Box, Quip, Web Crawling, Data Extraction, Data Management, APIs, Project Management, Ecommerce Project Management, International Ecommerce, Telco Ecommerce, Data Entry',\n",
       " 'Bioinformatics, Applied Mathematics, Statistics, Python, R, Computer Science, DevOps, Machine Learning, Data QC, Data Preprocessing, Feature Selection, Model Building, Model Deployment, Whole Exome Sequencing, RNAseq, Highthroughput Proteomics Data, Clinical Bioinformatics, Real World Biomarker Data, Clinical Trial Data Analysis, ML Ops, OpenSource R, Python ML Modeling Libraries',\n",
       " 'Database Architecture, Conceptual Data Modeling, Logical Data Modeling, MS SQL Server Enterprise, RDBMS Development, SQL Server DBMS, Database Tuning, Database Migration, AWS, Source to Target Mapping, Data Documentation, Case Management, Educational Systems, Data Analysis, Data Profiling, Data Interfaces, Data Modeling, Reverse Engineering, Forward Engineering, Technical Architecture, Physical Components, Data Conversion, Data Migration',\n",
       " 'AWS, GSCNet, JTEN, JLCCTC, NSC, SIMNET, STE, CTC, FMS Web, MCTP, NATO, ABCANZ, OPFOR, SITFOR, CONUS, OCONUS, Linux, Cisco, Microsoft, VMware, Secret clearance, Security +, Computing Environment certification, Information Assurance Technician Level II (IAT II) Certification, Database development and leading, Military unit structures and researching unit compositions and structure',\n",
       " 'Technical infrastructure management, Data center construction, Network augmentations, Infrastructure knowledge, Network knowledge, Data center knowledge, Hardware knowledge, Program management, Project management, Certifications, Qualifications, Leadership, Influence, Orchestration, Expertise, Scheduling, Prioritization, Tracking, Management, Travel',\n",
       " \"Customer service, Sales, Coaching, Teamwork, Communication, Problem solving, Decision making, Attention to detail, Organization, Microsoft Office, Banking regulations, Compliance, NMLS registration, Background check, Physical dexterity, Lifting, Bending, Stooping, Standing, Bachelor's degree in Business/Finance, 3 years of financial or sales management experience\",\n",
       " 'Data Cloud Architect, Azure Data Factory, Azure Data Lake Storage (ADLS), SQL, Azure Data Platform, Data Warehouse, Relational Database, Development, Performance Tuning, Business Intelligence (BI), ETL, CI/CD Pipeline Automation, ETL, Master Data Management, Data Quality, Metadata Management, Data Profiling, MicroBatches, Streaming Data Loads, Collibra, Ataccama, Data Integration, Data Architecture, Azure, Databricks, Azure DevOps, Azure Log Analytics, Relational Modeling, Dimensional Modeling, Unstructured Data Modeling, Logical Thinking, Problem Solving, Collaboration, Global Delivery Environment, Lean / Agile Development, Design Dimensional Model, Data Lake Architecture, Data Vault, Snowflake Logical Data Warehouse',\n",
       " 'PCIe technology, PCIe system and interface troubleshooting, Validation testing and characterization, Datacenter compute or storage system development, X86 or ARM64 Processors, NVMe SSD, CXL technologies, Test coverage, Production rampups, Manufacturing test requirements, Test methodology, Test plan, Test flow, Rootcausing of PCIe device related failures, Data analysis of manufacturing test logs, Software tools/infrastructure, New product development, Productization, Hardware software and product engineering teams, AI hardware and software architecture, Linux commands and utilities, Problemsolving and troubleshooting expertise, Rootcause analysis, Selfinitiative, Interpersonal skills, Flexibility to adapt to new technologies, CXL design validation and testing',\n",
       " 'Commissioning, Pre functional testing, Functional testing, Data centers, Construction, Strong communication skills, Teamwork, Leadership, Problemsolving, Investigation, Troubleshooting',\n",
       " 'Lab Science, ASCP Certification, Microbiology, Chemistry, Hematology, Transfusion Service, Proficiency Examination, Technical skills, General supervision, Part Time',\n",
       " 'Data Acquisition, Data Logging, Cloud Data, Embedded Controls, CAN Networks, LIN Networks, Ethernet Networks, HighPerformance Instrumentation, Vehicle Design Validation, Sensor Systems, Data Analysis, Mechanical Engineering, Mechatronics, Big Data Systems, Quality Control, Vehicle Systems',\n",
       " 'Datadriven projects, Data strategy, Data journalism, Data visualization, Data standards, Data analysis, Data science, Data mining, Data reporting, Statistics, Data concepts, Computer science fundamentals, Application building, Microsoft technology, Opensource technology, R programming, Python programming, SQL, Excel, Webscraping, Curiosity, News judgment, Legal knowledge, Government knowledge',\n",
       " \"HR Business Partner, Talent Acquisition, Performance Management, Employee Relations, Compensation and Benefits, Diversity and Inclusion, Data Center Expertise, Bachelor's degree, Master's degree or HR certification, 7+ years' HR Business Partner experience, Labor laws and regulations, Interpersonal and communication skills, Analytical and problemsolving abilities, Confidentiality and handling sensitive information, Resultsoriented and proactive mindset, Medical/Dental/Vision Insurance, Parental Leave, 401k, STD/LTD, Discretionary PTO, HSA/FSA\",\n",
       " 'Healthcare staffing, Travel nursing, Allied health, Permanent placement, Medical professionals, Career path, Recruiting, Travel season, Home placement, Economy, Technology',\n",
       " 'Oracle databases, Technical leadership, CDRSS redesign, Communicable Disease Reporting and Surveillance System (CDRSS), Active monitoring (AM), Direct active monitoring (DAM), Viral hemorrhagic fevers (VHF), Ebolavirus disease (EVD), Data storage organization presentation, User needs identification, Complex technical analytical professional services, Program/member services, Evaluation, Datadriven, Web sites, Office operations, Oracle database administration, Web server and middleware tools, New database setup, Data integration, System modifications, Database creation and maintenance, Development, Testing, Education, Production',\n",
       " 'Machine Learning, Product Management, Python, Spark, SQL, Data Science, Agile, Cloud Computing, Data Pipelines, Agile Delivery, Business Analysis, Software Engineering',\n",
       " 'Data Integrity, Master Patient Index, Electronic Health Record, Duplicate Medical Records, Epic Chart Corrections, Scanned Document Corrections, Patient Amendment Requests, Record Overlays, Microsoft Excel, Microsoft Outlook, Microsoft Word, Analytical Skills, Interpersonal Skills, Verbal Communication Skills, Written Communication Skills, Computer Skills, Computer Programs, Applications, High School Diploma, G.E.D., IT Setting, Healthcare Setting, SelfDirected, FastPaced Learning, Problem Resolution, Fully Remote, Health Information Management',\n",
       " \"Technical Writing, XML, DTD, Style Sheets, Schema, S1000D, US Navy Weapon Control Systems, Strategic Systems Command, Sea Systems Command, 9/80 Schedule, Flex Time Off, Paid Parental Leave, Healthcare Benefits, Health & Wellness Programs, Employee Resource Groups, Social Groups, Bachelor's Degree, Master's Degree, 2+ Years Relevant Experience, DOD Secret Security Clearance, US Citizenship, Pittsfield MA Office\",\n",
       " 'Data architecture, Cloud architecture, Azure data services, Microsoft Azure ADF, Power BI, Azure Synapse, Snowflake, Data modeling, Metadata management, ETL processing, Data ingestion, Data visualization, Data governance, Data quality management, Realtime data pipelines, Data analytics, Data platform instrumentation, Log data processing, Monitoring, Python, Java, SQL, PLSQL, Business requirements, Conflict resolution, Analytical skills, Problemsolving skills, Mentoring, Coaching, Proactiveness, Selfmanagement, Detailorientation',\n",
       " \"Sales, Customer service, Computer skills, Problem solving, Travel, KPI's, Mentoring, Training, Uncapped earning potential, Travel discounts, Career progression\",\n",
       " 'Crossfunctional collaboration, Stakeholder communication, Problemsolving, Build and release processes, Vendor management, Research and recommendations, Standards compliance, Information security, Automation, Telemetry, Service monitoring, PCI compliance, System configuration, Software control, Data analysis',\n",
       " 'Datadriven insights, Predictive analytics, Financial modelling, Data visualization, Value driver trees, Selfservice reporting, Data modelling, Data transformation, Alteryx, Power BI, Communication skills, Collaboration, Decisionmaking, Financial concepts, Analytical skills, Problemsolving, Planning systems, Analytics systems, Enterprise Resource Planning (ERP), SQL, Python, Hybrid work, Flexible work, Superannuation, Salary packaging, Open office layout, Resume, Cover letter, Criminal background checks, Right to work eligibility, Academic qualifications, Bankruptcy checks',\n",
       " 'Machine Learning, Transformer Inference, NLP, Kernel Parallelism, Python, Computer Science, Electrical Engineering, Mathematics, Deep Learning, Transformer Models, Backpropagation, Structured Sparsity, Low Precision Floating Point, Attention Mechanisms, IndustrialScale Training, GPU Programming, Equity Package, Medical Dental and Vision Packages',\n",
       " 'SQL, Python, Data Warehousing, Data Modeling, Data Access, Data Storage, SQLlike Languages, Databricks, SparkSQL, Airflow, Communication, RelationshipBuilding, Documentation, ProblemSolving, Analysis, Statistics, Data Mining, Tableau Software',\n",
       " 'Data analysis, SQL, Tableau, Power BI, Qlik Sense, Advana, Palantir, Python, PySpark, Databricks, Hypothesis testing, Regression, Geospatial tools, Time series analysis, Business intelligence, Data visualization, Data cleaning, Data quality, Data security, Data protection, Statistical methods, Machine learning, Reporting, Dashboards, KPIs, Data engineering, Data science, Data lineage, Active topsecret clearance, Bachelor of Science in STEM, Data engineering certification, Geospatial tools',\n",
       " 'Java/Scala, SQL, Python, Cloud Technologies (AWS Azure GCP), Columnar Databases, Data Pipelines, NoSQL Databases, Data Analytics, Distributed Data Processing, Software Development, Data Engineering, Big Data, Machine Learning, Realtime Data Processing, Hadoop, MapReduce, Spark, Flink, Kafka, Impala, HBase, Snowflake, Redshift',\n",
       " 'Data Architect, Cloud Architect, Cloud Computing, Data Management, Solution Brainstorming, White Boarding, Efficiency Improvement, Anomaly Detection, Current Computing Trends, Relevant Training, Certifications, Professional Communication, Quality Work Commitment, Effective Written Communication, Effective Verbal Communication',\n",
       " 'Data Science, Machine Learning, AI, R&D, Grant Writing, Entrepreneurship, Patent Development, Behavioral Health Technology, Wearables, RealTime Intervention, NSF Protocol, Research Support, Technical Research Assistance',\n",
       " 'Data science, Predictive modeling, Machine learning, Bayesian methods, Generalized linear models, Nonlinear models, Decision trees, Multivariate analysis, Nonparametric estimation, SQL, R, Python, STEM, Economics, Statistics, Programming, Data mining, Quantitative analysis, Predictive analytics, Databases, Communication skills',\n",
       " 'Cloud based data solutions, Analytical data warehouses, Cloud data migration solutions, Data analytics, Data reporting, Sales pursuit management, Direct sales, Cloud transformation strategy, Cloud computing, Data warehousing, Analytical platforms, Data migration, AI/ML use cases, Data platform, Data models, Deployment architectures, Edgebased use cases, CLevel client relationship building, Teamwork, Collaboration, Leadership, Problem solving, Decisionmaking, Communication, Interpersonal skills, Integrity, Credibility, Character, Eventdriven architectures, Domain driven design, Humancentric solutions, Compensation, Incentive compensation, Sales Achievement Bonus Plan, Diversity, Inclusion, Equal Opportunity Employer, Reasonable accommodation, Work authorization',\n",
       " 'Data Science, Algorithm Quality Trade Studies, Synthetic Aperture Radar Imagery, Image Processing, Analytical Procedure, Confidence Values, Verification and Validation, Automated Tasking, Scripting, Databricks, Python, Jupyter Notebook, Presentations, Reports, Mathematics, Statistics, Computer Science, Domain Expertise, Analysis, Presentation, Synthetic Aperture Radar Image Science, Multispectral, Programming, Intelligence Community Analytical Workflows',\n",
       " 'DeFi, NFTs, DAOs, Web 3.0, SQL, R, Python, Data modeling, Experimental design, Data visualization, Business intelligence, Data analytics, Product development, Team management, Strategic thinking, Communication, Collaboration, Problem solving, Leadership, Data storytelling, Business metrics, Continuous learning, Data science, Analytics, Experimentation, Scalable processes, Automation, Databased analyses, Complicated insights, Clear communication, Positive energy, Efficient execution, Selfdriven, Prioritization, Collaboration, Proactive management, Causal inference, Structured data, Unstructured data, Big data, Fintech, Product growth',\n",
       " 'MLOps, Python, Numpy, Pandas, Numba, Torch, Tensorflow, Jupyter, Deep Learning, Cloud Services (GCP AWS Azure), Agile Methodologies, FDA approvals for SaMD, Machine Learning, Computer Science, Speech Processing, Audio Classification, Communication Skills, Leadership, Mentoring, Product Documentation, Software Development, Data Analysis',\n",
       " 'Data Engineering, Data Pipeline Frameworks, Data APIs, Data Delivery Services, HighVolume Data Delivery, RealTime Data Delivery, Hadoop, Streaming Data Hub, Scala, Spark, Postgres, Angular JS, NoSQL, Python, SQL, Java, Big Data Technologies, MapReduce, Hive, EMR, Kafka, MySQL, UNIX/Linux, Agile Engineering Practices, Data Warehousing',\n",
       " 'Tableau, SQL, Snowflake, Star Schema, Data modeling, Data security, Rolebased access controls, Business intelligence, Data visualization, Data analysis, Reporting, Data pipelines, Data refreshes, User permissions, Servant leadership, Manufacturing Industry BI projects, Quantitative analysis, Relational databases',\n",
       " 'AWS, Spark, Glue, Python, C#, EKS, Lambda, Snowflake, SQL, Redshift, PySpark, Aurora Postgres, REST API, Data pipelines, Data engineering, Data architecture, Data analytics, Data science, Big data, Cloud computing, Software development, Team collaboration, Problemsolving, Communication, Business intelligence, Data visualization, Data mining, Data warehousing, Data lakes, Data governance, Data security',\n",
       " 'Privacy law, Data protection, GDPR, Consumer protection law, Artificial intelligence, Machine learning, Mobile apps, Interestbased advertising, Marketing, Adtech, Fintech, Analytical skills, Problemsolving skills, Attention to detail, Communication skills, Independent work, Remote work',\n",
       " \"Database design, Cloud solution development, Largescale and highconcurrency system design, Optimization studies, Golang, Python, C++, Code cleanliness, Refactoring skills, Unit testing, Databases, Relational databases, NoSQL databases, Analytical databases, Document databases, Vector databases, Data transmission products, Database data replication mechanisms, Database kernel, Database middleware, Database management platform, Data logging, Data analysis, Message queue, ETL, Data processing technologies, Bachelor's or Master's degree in computerrelated fields, 4+ years of experience in software development, Strong communication skills, Teamwork spirit, Problem analysis and problemsolving skills\",\n",
       " 'Database Administration, SQL, Amazon Web Services (AWS), Data Architecture, Data Modeling, Data Management, Relational Databases, NonRelational Databases, Data Marts, Geospatial Data, Unstructured Data, Leadership, Technical Guidance, Business Strategy, Cloud Computing, Business Requirements, SQL Database Development, AWS Cloud Solutions Architecture',\n",
       " 'Machine Learning, Deep Learning, Data Analysis, Programming, Python, NumPy, Pandas, Numba, Torch, Tensorflow, Jupyter, Model Optimization, Cloud Services (GCP AWS Azure), Communication Skills, Research Skills, Leadership Skills, Empathetic Leader, Agile Planning, Sprint Management, Feedback, Multidisciplinary Skills, Product Documentation, Speech Processing, Audio Classification, Experiment Tracking, Reproducibility Tools (MLFlow WandB DataBricks), FDA Approvals, SoftwareasaMedical Device (SaMD)',\n",
       " 'Python, Tableau, Data Quality Assurance, Data Visualization, Data Analytics, Data Interpretation, Statistical Techniques, Exploratory Data Analysis, GCP, SQL, Data Modeling, Machine Learning, Teamwork, Communication, Problem Solving, Analytical Thinking, Business Intelligence, DataDriven Decision Making, Business Requirements Gathering, Data Transformation, Data Integration, STEM Degree, Reinsurance Sector Experience',\n",
       " \"ArcGIS Desktop, ArcGIS Pro, Geoprocess, Geospatial Data Engineering, Python, Data Analysis, Data QA/QC, Collaboration, Geo Products, Remote Sensing, Image Processing, Geospatial Data Editing, Web Service Architecture, ESRI, GeoServer, OGC, Metadata Standards, BigData, RDF, SEO, Geospatial/GIS Application Development, Story Map, Data Quality Control, Data Publishing, Metadata, Geospatial Databases, Geodatabases, Data Integration, Cartographic Products, Geoprocessing, Story Maps, Dashboards, Web Applications, Bachelor's Degree in IT, GeoSciences, Government Experience, Earth Management, Earth Sciences, Context Switching, Fast Typing/Coding, Visual Prototyping\",\n",
       " 'Data Management, Clinical Research, Database programming, Data queries, Data validation, CRF design, Manual validation, Electronic validation, SOPs, Communication with clients and site, GLP, nonGLP studies, Biology, Chemistry, Organization skills, Planning skills, MS degree, BS degree',\n",
       " 'Data Engineering, Machine Learning, Data Mining, Data Wrangling, Data Pipelines, Data Security, Data Integration, Data Modeling, Data Quality, Agile Project Delivery, Azure PaaS Services, DevOps, Visualization Tools, Power BI, Cognos, SQL, Python, Java, Hadoop, Spark, Kafka, AWS, GCP',\n",
       " 'Machine Learning (ML), ML Ops, Engineering, Evaluation Experimentation, Monitoring, ML Workflow, Orchestration, Data Engineering, ETL Pipeline, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Scrappy, Autonomy, Flexibility, Teamfirst Mentality, Passion for Vision, NYCbased, Allin',\n",
       " 'Principal Data Engineer, Enterprise Data Lakehouse Platform, Microservicesdriven platform, Data ingestion, Data engineering, Data Architect, Data Platform Development, Data Ecosystem, Data Quality Frameworks, Data Governance Frameworks, Observability Frameworks, Data Analytics, Machine Learning, Data Modeling, Database Schemas, Data Storage Solutions, BigQuery, SQL, Python, Spark, DBT, Airflow, Kafka, Kubernetes, Docker, GCP, Data Lakehouse Architectures, Container Technologies, DataOps Principles, Test Automation, Grafana, Datadog, Data Mesh Architecture, Semantic Layers, Scalable IoT Architectures, SQL, Python, Spark, DBT, Airflow, Kafka, Kubernetes, Docker',\n",
       " 'Bioinformatics, NextGeneration Sequencing Data Analysis, Data Analysis Tools, Python, R, C++, C#, Java, GATK, Bioconductor, SQL, Document Databases, Phylogenetics, Virology, Immunoinformatics, Organizational Skills, Communication Skills',\n",
       " 'Blood tests, Microscopy, Laboratory equipment, Graduation from accredited MT program, National exam by Board of Registry of American Society for Clinical Pathology',\n",
       " 'Machine Learning, C/C++, Parallel programming, ARM architectures, ARM Neon SIMD programming, Performance optimization, Deep Learning, Machine Learning, Computer Vision, Natural Language Processing, GPU programming, DSP programming, Python, TensorFlow, PyTorch',\n",
       " 'Data Analysis, Advanced Statistical Techniques, Data Visualization, Data Mining, Machine Learning, A/B Testing, Data Manipulation, Data Management, ETL Processes, SQL, R, Python, Tableau, Power BI, Data Modeling, Hypothesis Testing, Software Development, Algorithms, Business Intelligence, Communication, Collaboration, Team Environment, Statistics, New Media',\n",
       " 'Statistical modeling, Machine learning, Deep learning, Data science, MLOps, Statistical analysis, Probability theory, Azure Data Studio, Data bricks, Data mining, Text mining, Natural language processing, Computer vision, Healthcare domain experience',\n",
       " 'Data analysis, Data visualization, SQL, Data mining, Data science, Python (programming language), R (programming language), Tableau, Analytical skills, Databases, Business analysis, Business intelligence (BI), Microsoft SQL Server, Product Analyst',\n",
       " 'Data Interpretation, Information Delivery, Data Analysis, Data Integration, Data Management, Data Quality, Metadata Management, Data Privacy, Data Collection, Business Requirements, Data Analysis Tools, Analytical Skills, ProblemSolving Skills, Communication Skills, Technical Degree, SQL, NoSQL, Data Modeling, Data Visualization, Reporting, Statistics, Machine Learning, Artificial Intelligence, Big Data, Relational Databases, Data Warehousing',\n",
       " 'Oracle 12c, Oracle 19c, Oracle Forms, Oracle Reports, PHP, Database Development, Web Development, SQL, PL/SQL, WebLogic, Database Performance Optimization, Database Security, Software Design, Software Development, Software Maintenance, Problem Solving, Communication, Teamwork, .NET, SOAP, IIS',\n",
       " 'AWS, Lambdas, Sage Maker, ECS, R, Python, SQL, Clinical trial forecasting, Data science, Modeling, Machine learning, Statistics, Cloud computing, Software development, Data analysis, Problemsolving, Communication, Teamwork',\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Agile, Unit testing, AWS, Microsoft Azure, Google Cloud, Distributed data/computing tools, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Realtime data, Streaming applications, NoSQL implementation, Mongo, Cassandra, Data warehousing, UNIX/Linux, Shell scripting',\n",
       " 'Machine Learning, Product Management, Engineering, Software Testing, Emerging Technologies, Data Science, Software Development, Collaboration, ProblemSolving',\n",
       " 'Microsoft Dynamics CRM/CE, .NET Framework, C#, CRM API, REST/ODATA, SOAP, ADO.NET, ASP.NET, Windows Communication Foundation, Workflow Foundations, JS, HTML, Objectoriented design, Service Oriented Architectures, CRM/CE customization, Serverside code, Custom business logic, Integration modules, Workflow assemblies, Plugins, OpenShift',\n",
       " \"Medical Laboratory Technician, Medical Technologist I, Medical Technologist II, ASCP, MLT, MLS, MT, MLT ASCP, Clinical Experience, Associate's degree, Bachelor's degree, Health Coverage, Vision Coverage, Dental Coverage, PTO, Tuition Reimbursement, Retirement Funds, Compassion, Professionalism, Positive Outlook, Advocacy, Collaboration, SafetySensitive Position\",\n",
       " 'Data Engineering, Software Development, Big Data Architecture, Cloud Computing (GCP), Team Leadership, DataDriven Solutions, Project Leadership, Collaboration, Version Control (GitHub), Adaptability, Strategic Thinking, Communication (Written and Verbal), Python Programming, Data Management, Relational Databases, NoSQL Databases, Cloud Systems and Architecture, STEM (Undergraduate and Graduate Degree), Management Experience, GCP Certifications (BigQuery)',\n",
       " 'Data Engineering, Data Science, Apache Spark, Big Data, Data Pipelines, Software Engineering, Data Applications Engineering, Kafka, Cloud Infrastructure, AWS, Azure, GCP, REST API, BI Tools, Python, Scala, Hadoop, SQL, CI/CD, Unit Testing, Integration Testing, Automation, Orchestration',\n",
       " 'Tableau, Data analytics, Marketing analytics, Salesforce, Data visualization, Reporting, Dashboard creation, Data extraction, Data manipulation, A/B testing, Google Analytics, Programmatic advertising, Problemsolving, Communication, Presentation, Storytelling, Multitouch attribution models, Marketing automation, Salesforce Certified, Google Certified',\n",
       " 'Customer service, Sales, Communication, Consultative sales approach, Product knowledge, Presentation skills, Closing skills, Negotiation skills, Motivation, Proactiveness, Learning, Problemsolving, Standing, Walking, Bending, Lifting, Carrying',\n",
       " 'DeFi, NFTs, DAOs, Web 3.0, Direct communication, Candid feedback, Data Science, Experimentation, Analytics, Advanced modeling, Business performance measurement, Data pipeline engineering, Statistics, A/B testing, Causal inference, ML, SQL, Python, Continuous learning, Efficient execution, Data engineering, Lower level work',\n",
       " 'Manager, API design, Data pipelines, Machine learning, Software tools evaluation, Software tools implementation, User manuals, Documentation, Team management, Mentoring, Cloud technologies, Database structures, Android development',\n",
       " 'Data Analysis, HR Systems, HR Data, HR Metrics, Data Extraction, Data Manipulation, Reporting, Data Quality Management, Data Integrity, Customer Service, Organizational Structure, Time Management, Multitasking, Prioritization, Business Systems, Microsoft Office Suite, ThirdParty Suppliers, Sensitive Data Handling, Policy Compliance, Legislative Frameworks',\n",
       " 'Big Data, Spark, Python, Scala, SQL, MongoDB, Cassandra, NoSQL databases, Healthcare IT projects, Agile, SQL queries, Data analysis, Analytical skills, Statistics, System coding, Unit testing, Documentation, Leadership, Mentoring, Design, Architecture, QA, Integration, Technology roadmap',\n",
       " 'Marketing Analytics, Digital Channel Insights, Audience Insights, DataDriven Insights, Data Visualization, SQL, Python, R, Tableau, Power BI, Data Analysis, Data Science, Customer Relationship Management (CRM), Marketing Automation, Business Acumen, Numerical and Analytical Skills, Executive Level Presentations, Change Management, Complex Initiatives, Business Analytics, Business Questions, DataDriven Insights, Commercial Impact, Leadership, Influence, Relationship Building, Statistical Analysis, Technical Expertise, Marketing Resources, Data Privacy Regulations, GDPR, CCPA',\n",
       " 'Data Science, Software Engineering, Machine Learning, Bidder Algorithm Development and Deployment, RealTime Online Bidding, Big Data (AWS Spark), Python, Java, C++, Experience in finding the best advertiser for a user, Realtime bidding experience, Data understanding and industry trends awareness, Model Building and Deployment, Bidder model performance, Experience in delivering a bidder model in production',\n",
       " 'Python, Java, AWS, Apache Spark, Apache Kafka, REST, Data Engineering, Data Science, Agile Development, Algorithms, Design Patterns, Performant Code, Data Processing Tools, Relational Databases, Analytics Databases, Source Control, CI/CD Tools, Unit Testing, Integration Testing, Load Testing, Communication, Problem Solving, Organization, Analytical Skills, Leadership, Cloud Platforms, Serverless Architectures, Big Data, Streaming Technologies',\n",
       " 'Data warehousing, Data modeling, Data engineering, Data governance, Cloud computing, Big data technologies, Spark/Databricks, Redshift, Snowflake, Hadoop, Python, Bash/Z shell, Linux, Azure, Alteryx, Tableau, Power BI, Looker, SQL, ELT, Dimensional modelling, Data governance, Structured data, Unstructured data, Business requirements, Communication, Demos, Consulting, Architecture, Implementation',\n",
       " 'Product Management, Data Analysis, Market Research, Investment Due Diligence, Competitive Analysis, Spreadsheet Modelling, UX Wireframing, Editing, Writing, Customer Interviews, Presentation Creation, KPI Data Collection, Digital Marketing, Microsoft Excel, Google Sheets, Engineering, Computer Science, Business/Finance, Venture Capital, Investment Firms, Management Consulting, Project Management, Social Media, Web Design, Graphic Design, Cybersecurity, Data Science',\n",
       " 'Machine Learning, Artificial Intelligence, Data Science, Python, Java, C++, C#, Scala, TensorFlow, PyTorch, JAX, Kubernetes, Containers, SQL, NoSQL, Cloud Computing, Google Cloud, Azure, AWS, Vertex AI, Tensorboard, MLFlow, Weights&Biases, Computer Graphics, Reinforcement Learning, Generative Modeling, Recommendation Systems',\n",
       " 'Machine learning, Computer graphics, Deep learning, Data science, Generative models, Diffusion models, Generative adversarial network (GAN), Character animation, Python, Java, C++, C#, TensorFlow, PyTorch, Keras, JAX, 3ds Max, Maya, Blender, Houdini, Unreal, Unity, Procedural content generation, Google Cloud, AWS (Amazon Web Service), Azure, OpenGL, DirectX, Vulkan, Cloud computing, Realtime computer graphics',\n",
       " 'Data Science, Artificial Intelligence, Machine Learning, Cloud Computing, Data Mining, Predictive Modeling, Datadriven Design, Engineering Systems, Curriculum Vitae, Cover Letter, Letter of Application, PhD in Physical Sciences or Engineering, Research or Work Experience in Data Science',\n",
       " 'Ontology, Data Modeling, Data Strategy, Semantic Technology, Semantic Solutions, Metadata, Taxonomy, Semantics, Computational Linguistics, Linked Data, RDF, RDFS, OWL, SKOS, Protégé, TopQuadrant, PoolParty, Stardog, Data.World, SQL, SPARQL, Graph Databases, Python, R, JSON, OpenAPI/YAML, AVRO, Agile Principles, Processes, Methodologies, Amazon Web Services, ProblemSolving, Project Management, Communication Skills, Teamwork, Time Management, Reading and Writing Skills, Presentation Skills',\n",
       " 'Data analysis, Data mining, Data representation, Data visualization, Data science, Analytical skills, Statistics, Business intelligence, Business analysis, Tableau, Microsoft SQL Server, Machine learning, Java, Python, R, SQL, MS Excel',\n",
       " 'Agile, AWS, Big Data Technologies, Cassandra, Cloudbased data warehousing services, Data Engineering, Distributed data/computing tools, EMR, Gurobi, Hadoop, Hive, Java, Kafka, Machine learning, MapReduce, Mongo, MySQL, NoSQL, Open Source RDBMS, Programming languages, Python, RDBMS, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux, Agile engineering practices, Fullstack development tools and technologies, Distributed microservices, Machine learning',\n",
       " \"SQL Server, MySQL, Oracle SQL, Data Warehousing, Backup and Recovery, System Optimization, Security, Troubleshooting, Collaboration, Customer Service, Automation, Disaster Recovery, SQL Server 2016, MySQL Server 8.0, Oracle SQL, Fire Safety, Bachelor's in Computer Science, 35 Years Experience\",\n",
       " 'Big Data Engineer, Healthcare Business, System Code, Unit Test, Documentation, Mentoring, Design, Architecture, Platform, Components, QA, Integration, Technology Roadmap, Cost Effectiveness, Business Value, Competitiveness, SDLC, Agile, Programming Languages, Python, Scala, Spark, SQL, Advanced SQL Queries, Big Data Platform, Analytical Skills, Data Sets, Patterns, NoSQL Databases, MongoDB, Cassandra, Healthcare IT Projects',\n",
       " 'Machine Learning techniques, Classification, Clustering, Feature engineering, Artificial Intelligence, Natural Language Processing, Computer Vision, Algorithms, Data structures, Java, Python, R, MongoDB, HBase, Linux, Critical thinking, Problemsolving skills, Databases, NoSQL databases, Software development',\n",
       " 'HR Operations, Admin, Senior Analyst, SAP, SuccessFactors, HRIS, MS Office, Word, Excel, Outlook, PowerPoint, SharePoint, Data Entry, Customer Support, HR Core Systems, Change Management, Reporting, Escalation, Diversity and Inclusion, Confidentiality, Tact, Diplomacy, Teamwork, Attention to Detail, Communication, Research, Data Analysis, Problem Solving, Training, Orientation, Policies, Procedures, Audits, Compliance, Payroll, Benefits, Compensation, Recruitment, Retention, Performance Management, HR Planning, Organizational Development',\n",
       " 'Oncology nursing, Chemotherapy administration, Hydration therapy, Medication administration, Patient assessment, Technical skills, Laboratory analysis, Xray interpretation, Scan interpretation, Drug reaction monitoring, Venipuncture, Implanted port access, Central venous access device maintenance, Alternative delivery systems, Patient flow management, Phone triage, Documentation, Medical assistant resource, Communication with physicians and advanced practice providers, Patient and family needs assessment, BSN degree (preferred), Oncology experience (preferred), Current RN state license, Current BCLS certification, OCN certification (preferred), Willingness to travel',\n",
       " 'Enterprise Architecture, Product Cluster Architecture, Tech Leads, Technology roadmaps, Business requirements, Platform technology roadmap, Portfolio and Investment plans, Strategic roadmap, Global architecture, Emerging trends and technologies, Platform architecture, Product architecture, License efficiency, Vendor strategy guidelines, Scalable products, Applicationusage best practices, Interplatform technical direction, Security and compliance, Portfolio financial management, Total Cost of Ownership (TCO), Emerging technologies, Initiative proposal, Cost efficiency, Design and planning, Executive Business Management, Program Architecture Delivery, Financial management, Architecture initiatives, Business and architecture domains, Functional coach of Architects, Platform risk and Governance framework, Masters degree, Management experience, Fluency in English (word and writing)',\n",
       " 'Data Architecture, Data Monitoring, Data Remediation, Scorecards, Business Rules, Data Quality Framework, Informatica IDQ, SDLC, Unit Testing, SQL, Python, Data Bricks, System Documentation, Technical Design, Programming Specifications, Estimates, Work Plans, Partnerships',\n",
       " 'MBBS, AHPRA, Fellowship of the Royal Australian and New Zealand College of Obstetrics and Gynaecology, Experience in supervision training and professional development, Experience in management of clinical services, Experience in RANZCOG OSCE examination coordination, Continuing Medical Education, Clinical leadership, Communication skills, Motivation, Initiative, Time management, Commitment to negotiation and clinical team building, Experience in quality assurance activities, Evidence based best practice, Research, Medical education, Staff training, Supervision, Obstetric and Gynaecological ultrasound, Working with Children Check, Vaccinations, Australian citizen, Permanent resident, Appropriate work visa',\n",
       " 'Data Engineering, Digital Transformation, Pipeline Development, Data Mapping, Data Ingestion, Data Management, Data Triaging, Data Automation, Data Loading, Big Data Technologies, Data Warehousing, ETL, SQL, Azure Ecosystem, Databricks, ADF, Blob Store, ADLS, PySpark, Spark, Python, NLP, Structured Data, Unstructured Data',\n",
       " 'Enterprise Data Management, Programming, Agile, Scrum, Waterfall, Python, Restful Services, SCADA (Supervisory Control and Data Acquisition), Chemical Engineering, Electrical & Computer Engineering, Computer Science, Informatics, Multitasking, Background noise resistance, Ability to travel, Nerf dartthrowing skills',\n",
       " 'Ontology, Data Modeling, Data Strategy, Semantic Technology, Knowledge Organization, Empathy, Design Thinking, Semantic Solutions, Semantic Strategy, Industry Trends, Emerging Solutions, New Insights, Customer Needs, Business Needs, DataDriven Culture, Agile Discipline, Technology Architecture, Secure Solutions, Resilient Solutions, Performant Solutions, Scalable Solutions, Customer Problems, Business Problems, Resource Management, Minimum Viable Products, Extensible Modeling, Semantic Portfolio Strategy, Customer Alignment, Resource Allocation, Impediment Resolution, Top Talent Attraction, Talent Development, Talent Management, Iterative Development, OutcomeFocused Development, WellManaged Development, Strategic Semantics Vision, Ambiguity Tolerance, Learning Hunger, New Challenge Seeking, Communication Skills, Influencer Skills, Open Communication Styles, Diverse Idea Listening, Action Bias, Commitment Making, TradeOff Making, Impediment Removal, Iterative Value Delivery, Minimum Viable Product Focus, Customer Focus, Customer Relationship Building, Learning Appetite, New Technology Exploration, Humble Unknown Exploration, Internal Solution Seeking, Team Player Skills, Team Goal Driving, Coaching Skills, Mentoring Skills, Information Science, Computer Science, Engineering, Library Science, Ontology, Semantics, Computational Linguistics, Metadata, Taxonomy, Linked Data, Canonical Data, Ontologies, JSON, XML, RDF, RDFS, OWL, SKOS, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, Business Strategy, Enterprise Semantic Solutions, SQL, SPARQL, Graph Databases, Graph Technologies, Python, R, JSON, OpenAPI/YAML, AVRO, Agile Principles, Agile Processes, Agile Methodologies, Amazon Web Services, ProblemSolving Skills, Reading Skills, Writing Skills, Project Management Experience, Communication Skills, Presentation Skills, Teamwork Skills, Deadline Meeting Skills',\n",
       " 'Data Analytics, Tableau, SQL, Python, Programming, Data Visualization, Data Exploration, Presentation, Statistics, Mathematics, Reporting, Documentation, User Experience, Testing, Software Integration, Microsoft SQL, SAP, CRM, ERP, Medical Device Development, Regulatory Affairs, Biotechnology, Pharmaceuticals, Quality Management Systems, Data Requirements, Data Analysis, Business Needs, Data Sources, Data Integration, Visualization Dashboards, User Documentation, Data Accuracy, Data Consistency, Quality Performance, Prioritization, Time Management, Adaptability, Analytical Thinking, Problem Solving, Collaboration, Teamwork, Communication, Writing, Documentation, Research',\n",
       " \"SQL Server, Database Administration, Data Warehousing, SOC1 Compliance, VMware Infrastructure, ESXi, Avamar Backups, Windows Server, Office 365 Exchange, Active Directory, Microsoft SCCM, Microsoft Server Technologies, Audit Controls, Customer Service Orientation, Multitasking, Time Management, Communication Skills, Teamwork, Confidentiality, Independent Work, Bachelor's in Computer Science or Technology, 3+ Years of SQL Server Database Administration Experience, Server Administrator Experience, Backup Software Experience (IDPA Avamar Dell Recover Point)\",\n",
       " 'Data Management, Funding Management',\n",
       " 'AI Infrastructure, Machine Learning Platforms, PyTorch, DeepSpeed, FSDP, Distributed Training, LLM Inference Latency Optimization, Kernel Fusion, Quantization, Dynamic Batching, Python, Docker, Kubernetes, Infrastructure as Code, Terraform',\n",
       " 'Biostatistics, Data Science, Biomedical Engineering, Computer Science, R programming, Python programming, Exploratory data analysis, Machine learning, Statistical analysis, Data manipulation, Data visualization, Matlab programming, Biosensors technology, Statistical analyses, Data preprocessing, Quality control, Data analysis, Statistical analysis plans, Data summaries, Data presentation, Communication skills',\n",
       " 'Data Analysis, Reporting (Excel PowerPoint), Adobe Analytics, Data Collection, Measurement Planning, UAT Validation, Troubleshooting, Web Browser Debugging Tools, Insightful Analysis, Data Audit, Documentation, Collaboration, Test & Learn Projects, A/B Testing, Multivariate Testing, Digital Marketing, Customer Acquisition, Quantitative Analysis, Consulting, Communication, Statistics, Creative Circle, Freelance, Equal Employment Opportunity',\n",
       " 'Azure Data Engineering, Azure Synapse Link, Azure Pipelines, Azure Synapse Analytics, Medallion Lakehouse architecture, Power BI, MS Dynamics, Data Platform Building, Data Consumption',\n",
       " 'Machine learning, Artificial intelligence, Data science, Python, Statistical modeling, Entity resolution, Graph theory, Neural networks, Spark, Predictive insights, Data mining, Largescale data processing, Crossfunctional collaboration, Mentoring, Communication skills, Lending, Small business industry classification',\n",
       " 'Data Management, ETL, Mortgage Servicing, Black Knight MSP, Application Development, Hardware Platforms, Network Systems, Agile Methodologies, Jira, Confluence, Zypher, JMeter, ELK Stack, Kabana, Dynatrace, Web API Design, REST API, SOAP, XML, JSON, DevOps, Tableau, Power BI, .NET, Visual Studio, SQL, SSIS, GitLab, Team Management, Risk Management, ProblemSolving, Analytical Skills, DecisionMaking, Communication Skills',\n",
       " \"SAS, Microsoft Office, Access, Excel, Word, EvaluationWeb, RW CAREWare, Data management, Data analysis, Data reporting, Data validation, Subrecipient support, Statistical methods, Health disparities, Quality management, Technical assistance, Effective communication, Interpersonal skills, Organizational skills, Evaluation experience, Data management experience, Healthcare data experience, Bachelor's degree\",\n",
       " 'Enterprise Architecture, Application Architecture, Data Architecture, Business Analysis, Functional Analysis, Data Analytics, Software Development, Cloud Infrastructure, ERP, CRM, Ecommerce, Data Modelling, Data Governance, Data Security, Development Languages, CI/CD, Microservices',\n",
       " 'Well Data Analyst, Data Management, Data Analysis, MFiles, Cloudbased Document Management Technology, Metadata, Data Lifecycle Management, Data Quality Monitoring, Data Governance Frameworks, Data Regulations, Ontology, Business Glossary, Communication Skills, Interpersonal Skills, Leadership Skills, Teamwork, 401(k) Plan, Paid Vacation, Holidays, Benefit Hours, Family Medical Insurance, Dental Insurance, Vision Insurance, Educational Assistance, Matching Gift Program',\n",
       " 'Agile, AWS, Big data, Cassandra, Cloud computing, Distributed data/computing tools, EMR, Google Cloud, Hadoop, Hive, Java, Kafka, MapReduce, Microsoft Azure, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux, Unit testing',\n",
       " 'AWS Glue, AWS Lambda, Snowflake, Selenium, Python, PySpark, SQL, Test automation frameworks, qTest, Jira, QTest, Data Quality Testing Engineer, Excellent problemsolving skills, Attention to detail',\n",
       " 'ML Ops, MLOps Engineering, Workflow orchestration, Monitoring, A/B testing, Data engineering, ETL pipeline, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Highdegree of autonomy, Technical authority, Flexibility, Teamfirst mentality, Mentoring, Inperson environment, Hiring, Financial workflows, Product ownership',\n",
       " 'DeFi, NFTs, DAOs, Web 3.0, Experimentation, Analytics, Advanced modeling, Statistical concepts, A/B testing, Causal inference, Machine learning, SQL, Python, Data engineering skills, Data foundations, BA, BS degree, Quantitative field, Math, Stats, Physics, Computer Science, PhD degree, Programming, Modeling, Communication skills, Positive energy, Continuous learning, Efficient execution',\n",
       " 'Data Warehouse Project Management, Data Engineering, Analytics, Automation, Reporting, AWS Data Warehouse, Snowflake, Mulesoft, PowerBI, Enterprise data warehouse, Project Management, Project Planning, Project Scheduling, Time Management, Budget Management, Data Quality, Data Security, Data Privacy, Data Modeling, ETL Processes, Data Pipelines, Data Automation, Data Visualization, Data Reporting, Industry Trends, Technology Trends, Computer Science, Information Systems, Project Management Experience, Data Warehouse Experience, Data Engineering Experience, Analytics Experience, Automation Experience, Reporting Experience, Enterprise Data Warehouse Experience, ProblemSolving Skills, Communication Skills, Interpersonal Skills, Teamwork, Attention to Detail, Medical Industry Knowledge, Data Regulations',\n",
       " 'Data science, Statistical and causal inference, User modeling, Social network analysis, Machine learning, Heterogeneous language environments, ML usage, Adhoc exploration, Preparing training data, Model development, Robust production deployment, Data modeling, Product trends, Product opportunities, Technical discussions, Core metrics, Visuals, Dashboards, Reports, Project initiation, Project completion, Guidance, Snapchat, Product sense, Product understanding, Statistics, Mathematics, Economics, Computer science, Data science, Quantitative analysis, Machine learning, Algorithms, Statistical methods, Analytics, Python, R, Architect, Tensorflow, Caffe2, PyTorch, Spark ML, Scikitlearn',\n",
       " 'Data Analysis, Data Management, Data Analytics, Data Extraction, Data Interpretation, Data Mining, Data Visualization, Data Query Language, Data Governance Framework, Microsoft Office Suite, SQL, DMBOK, Business Administration, Information Technology, Computer Science',\n",
       " \"Java, Python, RDBMS, NoSQL, AWS, Microsoft Azure, Google Cloud, Redshift, Snowflake, Data lakes, Realtime data, Streaming applications, Mongo, Cassandra, Cloud native data flow, Airflow, Agile engineering, Tableau, Hadoop, Spark, Kafka, ETL, Machine learning, Data warehousing, Cloud computing, People management, Agile teams, Agile, Unit testing, Code review, Enterprise standards, Bachelor's Degree, 8+ years of experience in application development, 2+ years of experience in big data technologies, 1+ year of experience with cloud computing, 4+ years of people management experience, AWS certification (AWS Developer AWS Architect), 4+ years of data warehousing experience, 4+ years of experience working on realtime data and streaming applications, 4+ years of experience with NoSQL implementation, 4+ years of experience with Cloud native data flow/movement in a large enterprise, 4+ years of experience with scheduling and monitoring workflows, 2+ years of experience with Agile engineering practices, 6+ years of people management experience\",\n",
       " 'Fintech, Data engineering, Software development, Scala, Spark, Java, Golang, Python, SQL, Hadoop, S3, Bash, Agile, Python, Shell scripting, Test Driven Development, Linux, Open Source, Big Data, Analytical Problem Solving, Systems Administration',\n",
       " 'Syspro, Business Administration, Information Systems, Inventory Management, Data Analysis, Strategic Planning, Team Building, Communication, Microsoft Office, Data Organization, Risk Assessment, Forecasting, System Optimization, Data Validation, Data Reconciliation, Cycle Counting, Standard Operating Procedures, ERP systems, Picklist Extraction, Kit Management, Inventory Planning, Inbound and Outbound Inventory Flow, Procurement, Warehouse, Logistics, Sales, Finance',\n",
       " 'Data Analysis, Campaign Execution, Database Management, Predictive Modeling, Business Intelligence, Problem Solving, Collaboration, Traffic Analysis, Conversion Trends, Visitor Pathways, Abandonment Analysis, Event Performance, Adobe Analytics, Tagging, Usability, Data Extraction, Derived Variables, Data Quality Control, Analytical Solutions, Data Science, Process Improvement, Statistical Programming, Data Visualization, Presentation Skills, Communication Skills, Storytelling, PowerPoint, SAS, R, Python, SQL, Power BI, Excel, Multivariate Testing, Target, Optimizely, Big Data Technologies, Hadoop, Cloud Computing, Spark, Customer Data Platforms, Agile, Email Analytics, A/B Testing, Voice of Customer, Digital Analytics, Data Management, Data Tagging, Media Analytics, Reporting, Data Capture, Data Cleaning, Business Requirements, Technical Requirements, Data Architecture, Data Security, Data Governance, Data Ethics',\n",
       " 'Software development, System software development, Linux, Operating systems, Program Management, Software engineering principles, Enterprise system architecture, Productivity tools, Process automation, Matrix environment, Communication skills, Technical presentation skills, Strategic thinking, Tactical thinking, Problem solving, Systematic solutions, BS in EE or CS, 7+ years of experience, Modern programming language, Handson experience',\n",
       " \"Data Standardization, Semantics, Ontology, Data Modeling, Business Strategy, Analytics, Enterprise Semantic Solutions, Linked and Canonical Data, JSON, XML, RDF, RDFS, OWL, SKOS, SPARQL, Protg, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, SQL, Graph Databases, Python, R, JSON, OpenAPI/YAML, AVRO, Agile Principles, Processes, Methodologies, Amazon Web Services, DetailOriented, ProblemSolving, Strong Reading, Writing Skills, Project Management, Communication Skills, Teamwork, Bachelor's Degree, Computer Science, Engineering, Library Science, Ontology, Semantics, Computational Linguistics, Master's Degree, PhD\",\n",
       " \"Machine Learning, Data Science, Statistics, Applied Math, Quantitative Analysis, Big Data, NoSQL, Realtime Processing, Optimization algorithms, Objectoriented design, Data structures, Algorithms, Problem solving, Complexity analysis, AdTech, MarTech, AWS, Python, Scala, R, SQL, Hadoop, Spark, Kubernetes, Docker, Git, Jira, Confluence, Tableau, Power BI, Looker, Bachelor's degree in computer science, Engineering, Mathematics, Physics, Statistics, 10+ years of software engineering experience, Handson technical experience working with large data sets, Deep knowledge in machine learning, Deep learning, Statistical modeling, Excellent communicator both to technical and business audiences, Ability to deal with ambiguity in business and technology, Strong interpersonal skills to work with crossfunctional and globally distributed teams, Strong sense of ownership urgency and drive. High on bias for action\",\n",
       " 'Data visualization, Data integration, Data analytics, Report writing, Business Intelligence, SQL, Power BI, Data reconciliation, Data accuracy, Verbal communication, Written communication, Time management, Priority setting, Excel, Presentation skills, Project management, Relational databases, SharePoint, Microsoft Office, Programming languages, Scripting languages, Tableau, Technical documentation',\n",
       " 'Data analytics, Data visualization, SQL, ETL, Google Big Query, Funnel.io, Google Analytics, Google Data Studio, PowerBI, Tableau, Domo, Google Tag Manager, Dataroma, G Suite, Spreadsheet management, GTM, DTM, Tealium, Signal Tag, LeadsRX, Rockerbox, Multichannel attribution reporting, AWS, Azure, Cloud computing',\n",
       " 'ArcGIS, Marine Science, Environmental Contracts, Data Recording Software, Microsoft Excel, Online Database Applications, Power BI, Data Quality Assurance and Quality Control (QAQC), Data Analysis, Statistical Analysis, Reporting, Analytical Skills, Attention to Detail, Data Management, Protocol Adherence, Sampling Initiatives, Instrument Calibration, Residency Requirement, Equal Opportunity Employer',\n",
       " \"Medical Lab Technician (MLT), Hematology, Clinical Chemistry, Blood Bank, Serology, Coagulation, Urinalysis, Microbiology, Microscopy, Immunology, Biology, Bacteriology, Hematology, Chemistry, Phlebotomy, Basic Cardiac Life Support (BLS), Associate's Degree, Medical Technology, Clinical Lab Sciences, MLT registry, ASCP, COVID19 vaccination, LevelII trauma center, Cardiovascular services, Openheart surgery, Cardiac catheterization, Oncology services, Radiation therapy, Outpatient infusion, Labor and delivery, Neonatal intensive care unit, Inpatient behavioral health unit, Intensive care, Inpatient rehabilitation units, Ethical behavior, Charitable care, Uninsured discounts, Uncompensated expenses, Equal opportunity employer, Diversity\",\n",
       " 'Database Business Analyst, Business Systems Analyst, Data Warehouse, SQL, Banking Industry',\n",
       " 'Data analysis, Healthcare data, Data extraction, Data transformation, Data visualization, Reporting, Epic Clarity, SQL, Analytic methods, Statistical methods, Data modeling, Project management, Agile development, Python, R, Clinical informatics, Clinical experience, Communication skills, Organizational skills, Analytical skills, Technical skills',\n",
       " 'Data analysis, Data visualization, Data reporting, Database applications, Analytical skills, Critical thinking, Problemsolving skills, Administrative skills, Organizational skills, Team leadership, Collaboration, Communication, Diversity and inclusion, Facilities management',\n",
       " 'Casefinding, Data abstraction, Case report form completion, Database use, Data management, Data organization, Data analysis, Data submission, Regulatory compliance, Ethical obligations, Collaboration, Communication, Multitasking, Prioritization, Organization, Medical terminology, Protocol interpretation, Clinical information systems, Health research, Healthcarerelated field, Diversity, Equity, Inclusion, Equal opportunity, Engineering, Science, Talent solutions, Workforce solutions',\n",
       " 'MLOps, AzureML, Azure DevOps, Azure Kubernetes Service (AKS), Azure Databricks, Machine Learning frameworks, Scikitlearn, Azure Resource Manager templates, Infrastructure as Code (IaC), Git, CI/CD pipelines, Python, PySpark, PowerShell, Azure CLI, Azurespecific certifications, Problemsolving, Troubleshooting, Cross global location experience, Microsoft Certified: Azure AI Engineer Associate, Microsoft Certified: Azure DevOps Engineer Expert, Data pipelines, Data governance, Data versioning, Data lineage tracking, Data management, Security, Regulatory standards, Application performance',\n",
       " \"Senior Data Engineer, Data Management, Data Engineering, Data Warehousing, ETL/ELT, Enterprise Data Pipelines, Data Analysis, Data Acquisition, Data Transformation, Data Storage, Data Quality, Data Consistency, Data Effectiveness, Data Privacy, Data Protection, Agile Software Development, Data Tagging, Metadata Management, Business Requirements Gathering, Data Pipelines Architecture, Performance Analysis, Performance Optimization, Data Modeling, Bachelor's Degree in Computer Science Information Systems or Engineering, 4+ Years of Experience in Data Management, 3+ Years of Experience in Data Engineering, Experience with Data Warehouse Architectures, Experience with ETL/ELT Tools and Technologies, Experience with Data Quality and Data Governance, Experience with Data Modeling and Data Visualization, Experience with Agile Software Development, Experience with Cloud Computing and Big Data Platforms, Data Engineering Certification, Previous Experience in the Wine and Spirits Industry\",\n",
       " 'Data Engineering, Data Warehousing, Data Lake Infrastructure, Data Analytics, Data Modeling, ETL/ELT Processes, SQL Optimization, Python, PySpark, Scala, Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, Redshift, Synapse, SnowFlake, Kubernetes, Github, DevOps, DataOps, PowerBI, Communication, Leadership, Mentoring, Change Management, Business Requirements, Project Management, Organizational Skills, Team Culture, Accountability, SelfManagement, Negotiation, DecisionMaking, Remote Team Management',\n",
       " \"Revenue Cycle Analytics, Data Analysis, Data Visualization, Healthcare Data, Epic Clarity, SQL, Python, R, Statistics, Project Management, Agile Development, Communication, Organizational Skills, Analytical Skills, Technical Skills, Epic Revenue Cycle, Certified Health Data Analyst (CHDA), Bachelor's Degree, 4+ Years of Experience, 2+ Years of Data Analysis in Healthcare, Proficiency in Epic Clarity and Revenue Cycle Modules\",\n",
       " 'Data Developers, Database design, SQL, C#, SSIS packets, TSQL, .NET, Visual Studio 2015 or later, Data Tools, SQL Server 2014 or later, Data modeling, Data conversions, ETLs, Data migration, Data integration, Data warehousing, Data marts, Data retention rules, Transaction databases, Data analysis, Data optimization, Reporting, Debugging, Agile, Scrum, Software development cycle, Technical writing, Verbal communication, Team player, Problem solving, Attention to detail, Quality assurance, Time management, SQL Server administration, Experience in developing and working knowledge of .NET and SQL development languages, Proven ability to deliver products with the highest quality and on time',\n",
       " 'Machine Learning, Software Development, Large Language Modeling (LLM), Clinical Data Analysis, Python, PyTorch, TensorFlow, NLP, Generative AI, ProblemSolving, Analytical Mindset, Persistence, Communication, Collaboration, Distributed Parallel Training, LargeScale MultiModal Foundation and Generative Models, ParameterEfficient Tuning Techniques, Reinforcement Learning from Human Feedback (RLHF), Prompt Engineering Techniques, CloudBased Infrastructure, Deploying LargeScale Machine Learning Models, Healthcare Domain, Health Provider Led Data Platform, Collaborative Research, Postdoctoral Research, Natural Language Processing, Language Modeling, Generative Modeling, Data Analysis, Research, Software Engineering, Big Data, Machine Learning, AI, Clinical Informatics, Medicine, Healthcare, Computer Science, Electrical Engineering, Deep Learning Frameworks, Strong Programming Skills, Publications, Contributions',\n",
       " 'Data Science, Data Analytics, Business Intelligence, Predictive Analytics, Machine Learning, Artificial Intelligence, Tableau, Python, SQL, R, Tableau Prep Builder, Statistics, Mathematics, Operations Research, Computer Science, Information Systems, Engineering, Economics, MS Forms, Power Automate, SQL, Statistical modeling, Predictive analysis, Machine learning',\n",
       " 'Machine Learning, Deep Learning, Data Science, Speech Processing, Natural Langauge Processing, Audio Analysis, Computer Vision, Cloud Computing, Python, NumPy, Pandas, Numba, PyTorch, TensorFlow, Jupyter, Scikitlearn, Keras, MLflow, WandB, DataBricks, Agile Methodology, FDA Approval, SaMD, GCP, AWS, Azure, MLOps',\n",
       " 'SQL, Jira, Python, Agile, Freddie Mac, Fannie Mae, Command line, Log analysis',\n",
       " 'Oracle Database administration, Oracle Database Security, Backup and recovery, Oracle Data Guard, Performance Tuning, Oracle Cloud Infrastructure (OCI), Oracle Autonomous Database, Unix, Linux, Windows, Network administration, Oracle Grid Control, RAC, MySQL, PostgreSQL, SqlServer, DB2, Oracle Golden Gate, Oracle Application Express (APEX), Service Requests, Product improvement programs, Product expertise, Database Security, General IT Security, Computer Science, Engineering, Database administrator, Network administrator, System administrator, Verbal communication, Written communication, English',\n",
       " 'Data engineering, Data analytics, Data warehouse, Python, SQL, Airflow, Redshift, AWS, GCP, Looker, LookML, REST, APIs, Pytorch, Tensorflow, Hugging Face, SageMaker, Fintech, Machine learning',\n",
       " 'Data Analysis, SQL, Microsoft Access, Relational Database, Data Reporting Tools, Statistics, Mathematical Models, Healthcare, Claims Payment Procedures, Health Insurance Business, Health Care Government Programs, Data Warehouse, Coding, ETL, Encounters, Supervisory Experience',\n",
       " 'Data Science, Machine Learning, Cash Advance Underwriting, Collections Methodology, User Retention, Churn Reduction, Data Analytics, Advanced Analytics, Data Visualization, SQL, Python, R, Looker, Vertex AI, Google Cloud Platform, BigQuery, DataDriven DecisionMaking, ProblemSolving, FastPaced Environment, SQL, DataSets, Data Science Solutions, Data Analytics, Emerging Technologies, Underwriting Processes, Collections Processes, Complex Datasets, Actionable Insights',\n",
       " 'Data Platform Modernization, DataDriven Decision Making, Data Governance, Data Evangelizing, Data Engineering, Python, Modern Datastack, dbt, Dagster, Airflow, Snowflake, RDBMS, NoSQL, Kafka, Pulsar, Communication, Data Governance Program, Analytics, Engineering, Business Intelligence, Product Management, Security',\n",
       " 'Data Management, Reporting, Debugging, Data Loading, Shared Components Development, Data Extraction, Data Staging, Data Transformation, Data Loading, Data Ingestion, Apache Spark, Scala, Continuous Data Listening, Data Extract Generation, Email Publishing, Kafka, PostgreSQL, Hive, OnCall Support, Offshore Team Collaboration',\n",
       " 'Data Loss Prevention (DLP), Symantec, SaaS, IaaS, Cyber security, Data protection, Agile frameworks, Cloud computing, Virtualization, Cloud security, AWS, Jira, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, Cyber technical challenges, URL filtering, Proxy, Network DLP, Threat thinking, Design thinking',\n",
       " 'Agile engineering practices, Java programming language, Scala programming language, Python programming language, SQL programming language, Open Source RDBMS, NoSQL databases, Cloudbased data warehousing, Distributed data/computing tools, Realtime data and streaming applications, Data warehousing, UNIX/Linux system, Performance based incentive compensation, Cash bonus, Long term incentives, Comprehensive health benefits, Financial benefits, Other benefits, Equal opportunity employer, Diversity and inclusion in the workplace, Drugfree workplace, Criminal background inquiries, Reasonable accommodations for disability, Technical support for recruiting process',\n",
       " 'Project Management, Engineering Management, Supervisory Experience, Electrical Engineering, Engineering Technology, EPMS, BMS, Power Distribution Systems, Industrial Control Programmable Logic Controllers, Commissioning, Qualification, Change Control, HVAC, Chiller Plants, Domestic and Process Water Systems, Building Automation Systems, Electrical Diagrams, Control Diagrams, Single and 3Phase Circuits, AWS, QA, SOPs, Budgeting, Performance Management, ClientFacing Communication, Internal Communication, Organizational Skills, DetailOriented, MultiTasking Skills, SelfStarter, Quick Learner, Adaptable to Software Applications, Computing Infrastructures',\n",
       " 'Project Management, Program Management, Microsoft Office Suite, MS Project, SharePoint, Visio, Waterfall, SCRUM, Business and technology consulting, Industry and digital solutions, Applications development and management, Managed edgetocloud infrastructure services, BPO, Systems integration, Global data centers, PMP Certification, Canadian Federal Government  Secret Clearance',\n",
       " 'Manufacturing, Aerospace, CNC machining, Quality control, Team work, Basic computer skills, SPC & Shop Floor System, Blueprint reading, Communication, Adaptability, Planning, High school diploma or GED, 13 years experience in Aerospace fastener manufacturing, Ability to read and write simple instructions, Basic math skills, Ability to lift up to 30 pounds, Ability to work in a loud environment, Ability to wear personal safety equipment, Medical, Dental, Vision, FSA/HSA, 401K Matching, Vacation Pay, Sick Pay, Education Reimbursement',\n",
       " 'Data Analysis, DataDriven Reporting, Financial Analysis, Business Analysis, Critical Thinking, Analytical Skills, Written and Verbal Communication, Attention to Detail, Business Operations, Business Objectives, Business Strategies, Process Flow, Information Flow, ObjectOriented Programming, Pivot Tables, Graphs, Charts, Data Tools, Project Prioritization, Time Management',\n",
       " 'Machine Learning, Voice Biomarker Software, Python, Numpy, Pandas, Numba, Torch, Tensorflow, Jupyter, Cloud Services (GCP AWS Azure), MLFlow, WandB, Sprint Management, Agile Methodologies, DataBricks, MLOps, Speech Processing/Recognition, Audio Classification, Experiment Tracking, Model Training, Agile Planning, FDA approvals, MVP, Feedback, Acknowledgments',\n",
       " 'Distributed Antenna System (DAS), Small Cell, 4G/5G Private Wireless Networks, Construction Drawings, Scope of Work Documents, Tabulated Test Matrices, PIM, Line Sweep, Fiber Optic Field Tests, Proprietary Software, Return Loss, Insertion Loss, Distance to Fault, Key Performance Indicators (KPIs), MIMO A Stream, MIMO B Stream, Time Stamp, Labeling, Variances, Thematic Maps, Proofreading, Statistical Analysis, Data Matrices, Data Validation, Attention to Detail, Quantitative Reasoning, Computer Skills, MS Office Suite (Word Excel PowerPoint), Email, MultiTasking, Independent Work, Verbal Communication, Written Communication, PIM & Sweep Testing, SeeHawk, iBwave, Kaelus Unify, Line Sweep Tools, Exfo Fast Reporter',\n",
       " 'Data Analysis, Healthcare Data, Epic Clarity Reporting, Clinical Data Analysis, User Observation, Data Informatics, SQL Proficiency, Statistical Forecasting, Usability Testing, Project Management, Software Design, Data Extraction, Data Transformation, Data Visualization, Clinical Workflows, Clinical Informatic Foundation, User Experience Design, Digital Health Solutions, Epic Clarity Data Model, Epic Clinical Data Model, Certified Health Data Analyst, Python, R Programming, Agile Development, Written Communication, Oral Communication, Organizational Skills, Analytical Skills, Technical Abilities, Data Extraction, Data Transformation, Data Analysis, EHR systems, Healthcare data analysis, Clinical research, Data visualization, Datadriven solutions, Clinical informatics, Analytics, Data science',\n",
       " 'AMOS Database, Materials Management, WIP Management, Production Work, High School Diploma, English Proficiency, Verbal and Written Communication, Physical Strength, Construction, Maintenance',\n",
       " 'Python, SQL, R, Data Science, Hadoop, Hive, Tableau, PowerBI, ThoughtSpot, Looker, Marketing Mix Modeling (MMM), MultiTouch Attribution Models (MTA), Modeling, Machine Learning, Cloud technologies, Predictive modeling algorithms, Optimization techniques, Statistics, Agile environment, Project management, Data visualization, Communication, Curiosity, Attention to detail',\n",
       " \"Antimoney laundering (AML), CAMS (Certified AntiMoney Laundering Specialist), Business development, Financial reporting, Internal controls, People management, Regulatory compliance, Risk assessment, Teamwork, Bachelor's degree in business, CAMS designation, 46 years of advisory or consulting experience, Extensive knowledge of antimoney laundering processes, Experience with internal controls design and implementation, Business and practice development experience, Proven people management relationship building and leadership skills\",\n",
       " 'EL/ML, MN teaching license, Tier 1 or 2 license, Education Minnesota Osseo, Equal Employment Opportunity',\n",
       " 'Data Processing, Data Entry, Online Keying, Reject Correction, Transaction Reconciliation, Adjustment, File Reconciliation, File Transmission, Microsoft Office, Outlook, Communication, Error Correction, Balancing, Transit Cash Letter Preparation, Online Keying, Data Entry, Reject Repair, Import File Verification, Export File Processing, Out of Proof Condition Reconciliation, Work Product Comparison, PC Software Package Usage, Computer Output Classification, Software Package Utilization, Encoding, Banking Experience, Billing Experience, Accounting Experience, Written Communication, Oral Communication, Software Usage, Computer Operation, Input Speed',\n",
       " 'Data science, Artificial Intelligence, Predictive modeling, Statistical modeling, Machine learning, Natural Language Processing, Generative AI, Data wrangling, Data matching, ETL, Data visualization, Model testing, Model selection, Performance monitoring, Software development, GitHub, Realtime model deployment, Containerizing models, Statistical techniques, Parametric statistical modeling, Nonparametric techniques, Regularization techniques, Variable selection, Feature creation, Validation, Model performance, Programming languages (R Python SPARK SQL), Disparate impact testing, Communication, Teamwork, Presentation skills, Mentoring',\n",
       " 'Cloud Native Data Engineering, Apache Spark, Scala, Performance Tuning, Spark/Scala Programming, Databricks Framework, Azure Cloud Services, Azure Data Factory, Azure Data Lake Storage, Azure Even Hub, Azure SQL, Azure Analytical Services, Azure Cosmos DB, Massively Parallel Processing (MPP) Architecture, Microbatch Streaming, Continuous Streaming Process, Apache Spark Modules, Data Ingestion, SQL, HQL Queries, Hive Data Warehouse, Relational Modeling, Dimensional Modeling, Star Schema, Snowflake Schema, OLTP, OLAP, Normalization, Fact Tables, Dimensional Tables, Technical Design Documents (TDD), DevOps Principles, CD/CI Implementations, Azure Native Services, Customer Relationship Management, Stakeholder Management, Communication Skills, Presentation Skills',\n",
       " 'Configuration Management (CM), Data Management (DM), Configuration Control Boards (CCBs), Technical Review Boards (TRBs), Configuration Identification, Change Control, Physical Configuration Audits (PCAs), Functional Configuration Audits (FCAs), CDRL, Contract Data Requirements List (CDRL) packages, Data Calls, Data Requirements Review Boards (DRRBs), IT Configuration Management, Configuration Audits, Configuration Control, Baseline Management, Data Analysis, Troubleshooting, Configuration Management Plans, Contract Data Requirements List (CDRL), Statement of Work (SOW), Statement of Objectives (SOO), Performance Work Statement (PWS), DoDI 5230.24, DODD 5230.25, DoD 5010.12M, DoD 5000.02R, DFARS 252.2277013, DFARS 252.2277014, ASSIST database, Defense Department Environment, Software Configuration Audits, Hardware Configuration Audits, Configuration Management Tools, Release Notes, CM Reports, Change Control Process, Change Control Board, Software Development Life Cycle, Database Management, MS Office Suite, Secret Clearance',\n",
       " 'Public Safety, Information Technology, Computer Aided Dispatch (CAD), Police Records Management Systems (RMS), SharePoint, MS Office 365, VBA, Java, HTML, JavaScript, PowerShell, Application Deployment Architecture, Application Architecture, Analytical Tools, Problem Solving, Communication, Teambuilding, Management, Cloud Computing, OnPremises Infrastructure, CJIS, HIPAA, Telework Policy',\n",
       " 'Data Analysis, Statistical Modeling, Data Visualization, Data Mining, Machine Learning, SQL, Python, R, Redshift, Hive, Spark, Tableau, Qlik, Excel, Sheets, Splunk, StoryTelling, Operational KPI Metrics, Reporting Systems, HypothesisDriven Analysis, Clustering, Classification, Regression, Decision Trees, Neural Networks, Support Vector Machines, Anomaly Detection, Recommendation Systems, Sequential Pattern Discovery, Text Mining',\n",
       " 'Privacy law, Data protection laws, Artificial intelligence, Machine learning, Payment ecosystem technologies, GDPR, CCPA, Data policies, Privacy trainings, Legal practice, Project management, J.D. degree, State bar membership, Consumerfacing technology, Financial services, Inhouse legal experience',\n",
       " 'distributed database systems, SQL compiler, data storage systems, distributed systems, C++, C, data structures, algorithms, database internals, operating systems, I/O systems, scalability, performance, microservices, team leadership, concurrency, parallelization, modular design, simplicity, iteration, continuous evolution, communication, collaboration, teamwork, startup, fun, accomplishment',\n",
       " 'Application Development, Sustainment, AWS, Azure, Cloud Environments, Life Cycle Support, Enterprise Logistics Readiness Portfolio, Cargo and Personnel Movement, System/Product Capabilities, Automated Processes, Production Systems, Technical Leadership, Architecture, Planning, Implementation, Cloud Native Services, Migration, Software Platforms, Technical Sustainment, Software Development, Evaluation, Documentation, Software Changes, Deficiencies, Analysis, Design, Testing, Coordination, Configuration Management, Software Architecture, Risk Management, Relationship Management, Test Environments, Production Environments, Cloud Spend, Scrum Master, Continuity of Operations Plans, Wireless Networks, WANs/LANs, Firewalls/Routers, Secure Information Interchange, Public Key Infrastructure (PKI), Data Encryption, Software Instrumentation, Data Warehousing, Data Stores, Failover Software, VPN, VDI, IT, Jira, Confluence, Remedy, BitBucket, Jenkins, Ansible, Docker, DevSecOps, Selenium, JMeter, SonarQube, ACAS, Microsoft Office Suite, CI/CD Pipelines, Agile DevSecOps, DoD Secret Clearance, DoD 8140 Compliance, Software/Cloud Architect, DoD Applications, Development Environments, Test Environments, Production Environments, GovCloud/CloudOne (C1), Database Platforms, Oracle 12c, Oracle 19c, Microsoft SQL 2012, AWSnative Database Solutions, Automated Testing Strategies, Employee Stock Ownership Plan (ESOP), 401k Plan, Paid Time Off, Holidays, Medical Plans, Dental Plans, Vision Plans, Life & AD&D Insurance, Employee Assistance Program, Wellness Resources, Training and Development Programs, Life & AD&D Insurance for Employee Spouse and Children, Shortterm Disability, Longterm Disability, Legal Shield, Identity Theft Protection, Pet Insurance',\n",
       " 'Machine Learning, ML Ops, Engineering, Workflow orchestration, Monitoring, Visibility, Experimentation, A/B testing, Data engineering, ETL, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Teamfirst mentality, Autonomy, Flexibility, Vision, Scrappy',\n",
       " 'Agile, AWS, Cassandra, Cloud computing, Data engineering, Data warehousing, EMR, Gurobi, Hadoop, Hive, Java, Kafka, MapReduce, Microsoft Azure, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, UNIX/Linux',\n",
       " 'IT Security, Data & Systems, Vulnerability Management, IT Operations, Systems Management, Linux, Windows OS, System Administration, Software Development, Public Cloud, Infrastructure, Vulnerability Findings, CVE, CPE, CVSS, NIST, Risk Management, Python, Shell Scripting, DBMS, RDBMS, ETL, Ansible, QlikSense, Risk Management, Computer Science, Engineering',\n",
       " 'C++, AI, Data Lakes, Distributed systems, Communication protocols, Matrix multiplications, BLAS, GraphBLAS, Linux, Kernelbypass, Userspace drivers, SIMD Assembly, CUDA, OpenCL, GPGPU, Parallel algorithms, Graph Theory, Information Theory, Optimization Theory, Combinatorics, Python, Git, GitHub, GitLab, Docker, CMake, Standard tooling, HighPerformance Computing, Artificial Intelligence, Scalability, Efficiency, Intelligence',\n",
       " 'Data Analysis, Artificial Intelligence (AI), Tableau, Splunk, SQL, R, Python, Java, TS/SCI clearance, Polygraph',\n",
       " 'Java, Scala, Python, RDBMS, NoSQL, AWS, Cloud Computing, Redshift, Snowflake, Big Data Technologies, Distributed Data/Computing Tools, UNIX/Linux, SQL, Agile Engineering Practices, Machine Learning, Microservices, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, Data Warehousing, Hadoop, EMR, Kafka',\n",
       " 'Product Management, Data Security, Data Privacy, Encryption, Customerfacing Products, Success Metrics, Customer Inputs, Product Value Propositions, Engineering Leadership, Technical Concepts, Influential Leadership, Partner Teams, Solution Architects, Customer Success, User Experience, Complex Products, Health Coverage, Dental Coverage, Vision Coverage, 401(k) Plan, Equity Awards, Flexible Time Off, Paid Parental Leave, Family Planning, Gym Reimbursement, Personal Development Fund, Employee Assistance Program (EAP), Mental Wellness Resources, Annual Performance Bonus',\n",
       " \"Data analytics, Software engineering, Data engineering, Data science, Machine learning engineering, Python development, Special Operations Community, Briefing military leaders and officers, Automating analytic workflows, Data frames, Quantitative modeling, Supervised and unsupervised machine learning algorithms, Technical requirements, PostgreSQL, Software development, API development, Managing databases, Software architecture for containerization tools, Docker, Kubernetes, Extract Transform and Load (ETL) tools, Data warehousing, Data pipeline development, Pandas, Geopandas, Visualization tools, Tableau, Anaconda, R Studio, DoD and IC domains, Artificial intelligence principles, Bachelor's degree, TS/SCI clearance, Upskilling programs, Tuition reimbursement, Mentoring, Networking, Wellness programs, HSA contributions, Paid holidays, Paid parental leave, 401 (k) match, Flexible schedules, Remote and hybrid locations\",\n",
       " 'Drupal, Data architecture, Data engineering, Data science, Data analysis, Data visualization, Database systems, SQL, Data integration, ETL (Extract Transform Load), Queue based messaging systems, Geospatial data management, Data governance platforms, Continuous integration, Continuous deployment, DevOps, Agile Scrum, User experience, User testing, User flows, Wireframes, Site maps, Mockups, Storyboards, Design, Coding, Deployment, Data challenges, Data modeling, Data warehousing',\n",
       " 'Java, Scala, Spark, Hadoop, SQL, Python, Go, Shell scripting, Linux, Data engineering, JVM, Data architecture, Analytical problemsolving, Systems administration, Hadoop ecosystems, Opensource products, Agile environment, Test driven methodologies, Big Data technologies',\n",
       " 'Data Analysis, Python, Tableau, SQL, AWS, Cloud Computing, Data Visualization, Statistics, Advanced Analytical Methods, Business Communication, Consulting, Project Management',\n",
       " 'Data Architecture, Data Flow Diagram, ER Diagram, Visio, Data Governance, Data Standards, Data Standardization, Data Mastering, Metadata Management, Data Management, Data Visualization, Data Virtualization, Data Modeling, Enterprise Data, MDM, Data Analysis, SQL, Banking, Finance, Data Curation, Data Representation, Data Normalization, Data Integration, Data Persistence, Data Exchange, Business Intelligence, Analytics, System Design, Business Strategy, Technology Strategy',\n",
       " 'PTX, GPU Computing, Deep Learning, Autonomous Driving, ISA, NVIDIA GPUs, CUDA, PTX Compiler Front End, Optimizer, Object files, Debug information, Linkers, Loaders, Driver Compiler Interface, LLVM, MLIR, JIT compilers, Compiler Front end, LLVM IR, C, C++',\n",
       " 'Data Science, Statistical modeling, Forecasting, Statistics, Machine learning, Data analysis, Data visualization, Data preparation, Feature engineering, Predictive analytics, Tableau, PowerBI, R, Python, SQL, Communication, Collaboration, Team work, Problemsolving, Analytical thinking, Strategic planning, Business intelligence, Data mining, Data warehousing, Data governance, Data security, Data ethics, Cloud computing, Big data, Artificial intelligence, Machine learning, Deep learning, Natural language processing, Computer vision, Robotics, Automation, Blockchain, Cryptocurrency, Internet of Things, Augmented reality, Virtual reality, Mixed reality, 5G, WiFi 6, Edge computing, Quantum computing',\n",
       " 'Nursing, Data Analysis, Data Collection, Database Management, Data Interpretation, Clinical Processes, Nursing Practice, Nursing Metrics, Nursing Quality Indicators, Research, Literature Reviews, Healthcare, Basic Life Support, BLS, Scheduling Flexibility, Telework, US Citizenship, English Proficiency, Selective Service Registration, Written and Spoken English, Probationary Period, Background Security Investigation, Online Onboarding Process, PreEmployment Physical Examination, Influenza Vaccination Program, COVID19 Vaccination Program, Accreditation Commission for Education in Nursing (ACEN), Commission on Collegiate Nursing Education (CCNE), MSN Bridge Program, Professional Nursing Registration, State Territory or Commonwealth Nursing Registration',\n",
       " 'Salesforce Data Cloud, Salesforce Marketing Cloud, Apex, Visualforce & Lightning, Web based technologies (SOAP REST), Jitterbit, MuleSoft, Software development, Integration, Analysis, Documentation, Problem solving, Communication, User Acceptance Testing, Project planning, Agile development, Legacy debt, API, Data infrastructure, Customer data platforms, Security, Compliance',\n",
       " 'Data Science, Statistics, Computer Science, Data Collection, Data Processing, Data Analysis, Model Development, Simulation, Python, R, SQL, Scikitlearn, NumPy, Matplotlib, Tensorflow, PyTorch, HuggingFace, Project Management, Technical Leadership, Technical Proposal Writing, Statistical Analysis Methodologies, Design of Experiments, Database Languages, Data Science Tools, AI/ML Development, High Level APIs and Libraries',\n",
       " \"Customer service, Hiring, Scheduling, Training, Performance counseling, Evaluation, Inventory, Menu planning, Menu development, Menu implementation, Food quality standards, Food handling, Cost controls, Budget monitoring, Culinary education, Chef experience, Fine dining, Casual dining, Banquets, Artistic ability, Marketing, Cost control, Wage control, Food products, Standard recipes, Microsoft Office, Verbal communication, Interpersonal skills, Food Handler's Certification, ServSafe Training, MultiUnit Operations, Warehouse Operations, Inventory Tracking Systems, Purchasing contracts\",\n",
       " 'Analytics, AI, Machine Learning, Cloud Technologies, New Ways of Working, Data Engineering, Data Management, Data Lakes, Data Mesh, Data Product, Domain Driven Data, Analytics Engineering, Model Engineering, Model Development, Model Governance, Model Ops, Regression Models, Business Development, Process Improvement, A/B Testing, Statistical Models, Machine Learning Algorithms, Machine Learning Systems, AI Products, AI Solutions, Code Reviews, Algorithms, Models, Experiments, Functionality, Performance, Special Projects, Technology Architect, Undergraduate Degree, Graduate Degree, Cloud Infrastructures, Artificial Intelligence Platforms, Artificial Intelligence Applications, Artificial Intelligence Devices, Deployment Plan, Scalability, Security, Data Analytics, Artificial Intelligence/Machine Learning, Artificial Intelligence Acumen, Latest Technologies, Latest Tools',\n",
       " 'Strategic ownership, Datadriven insights, Digital channel and audience insights, First and thirdparty data sources, Digital consumption best practices, Customer segments, Visual storytelling with data, Subject matter experts, Datadriven marketing, Conversion rate optimization, Customer experience activities, Digital marketing experience, Marketing strategy, Channel strategy, Business math statistics computer science, MBA MS, Problemsolving, Communication, Leadership, Strategic thinking, Critical thinking, Data analysis tools, SQL, Python, R, Data visualization tools, Tableau, Power BI, Customer Relationship Management (CRM) systems, Marketing Automation platforms, Business acumen, Numerical skills, Analytical skills, Executive level presentations, Change management, Complex initiatives, Buyin from leadership and stakeholders, Actionable insights, Business analytics, Complex business questions, Datadriven insights, Commercial impact, Direct and influence without authority, Change implementation, Diverse teams, Statistical resources, Technical resources, Marketing resources, Motivated, Collaborative, Innovative, Data privacy regulations, GDPR, CCPA, Information technology group, Digital technologies, IT infrastructure, Application development, Business systems, Collaborative and social technologies, Information security, Project leadership',\n",
       " 'Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Cloud based data warehousing, Agile, Big data technologies, Cloud computing, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Realtime data, Streaming applications, NoSQL implementation, Mongo, Cassandra, Data warehousing, UNIX/Linux, Shell scripting, Agile engineering practices',\n",
       " 'SAP MDG, S4 Hanna, SAP Business Client, ServiceNow, Excel, SharePoint, SAP Legacy systems, Data governance, Data quality management, Master data maintenance, Collaboration, Data standardization, Documentation, Data remediation, Data auditing, Data migration, Reporting, Training, Analytical skills, Problemsolving skills, Operational master data maintenance experience, Training experience, ERP experience, Crossfunctional collaboration',\n",
       " 'Gameday Compliance Monitoring, Basic Agreement, Major League Rules, Major League Baseball Regulations, Club Compliance, Sign Stealing Enforcement, Electronic Devices Enforcement, Video Hardware, Replay Software, Clubhouse Atmosphere, Live Situations, FastPaced Situations, HighPressure Situations, Communication, Baseball, MLB Rules',\n",
       " \"Product Management, Business Focus, Agile, Technology Driven, Human Centered, Transformational Leadership, Learner, Team Player, Bachelor's Degree, 5+ years in Digital Product Management, Bachelor's Degree in Computer Science or Engineering, MBA or Master's degree, 3+ years in Agile Product Management, Experience translating business strategy into consumer facing digital products\",\n",
       " 'Machine Learning, Software Development, MLOps, AWS, Azure, NLP, GenAI, LLMs, Data Cleaning, Preprocessing, Agile, SaaS, Software Development Mentorship, Technical Problem Solving, Communication, HighTrust Relationships, Total Rewards Program, Hybrid Work Environment, Flexible Time Off, Counseling Benefit, RRSP Matching, RESP Contribution, Clioversary Recognition Program, Legal Software, CloudBased Solutions, Legal Practice Management, Diversity Inclusion Belonging and Equity (DIBE), Accessibility, Equal Employment',\n",
       " 'Data Analytics, Project Leadership, Cloud Analytics, Machine Learning, Business Requirements Gathering, Data Visualization, Data Movement ETL Technologies, Agile, Digital Transformation, Risk/Regulatory Drivers, Business Intelligence, Data Science, Engineering, Statistical Analysis, Software Development, Information Systems, RDBMS, Data Governance, Tableau, PowerBI, Looker, Domo, AWS, GCP, SQL, Python, R',\n",
       " 'SAP, Data Migration, Data Mapping, Data Replication, Master Data Management, ERP Implementation, SAP Functionality, SAP Training, Solutioning Workshops, Financial Data Analysis, Company Financials, P&L Statements, Financial Metrics, Team Leadership, FMCG Industry, Global Food Industry',\n",
       " 'Data architecture, Data governance, Data modeling, Data engineering, Data analysis, Data profiling, Data warehousing, Data transformation, Data visualization, SQL, NoSQL, Hadoop, Hive, Impala, Python, Splunk, Excel, Data security, Data quality, Data integrity, Data lifecycle, Data governance, Data stewardship, Data catalog, Data lineage, Data taxonomy, Data classification, Data masking, Data encryption, Data anonymization, Data governance, Data privacy, Data protection, Data regulations, Data ethics, Data monetization, Data science, Machine learning, Artificial intelligence, Business intelligence, Data visualization, Data storytelling, Datadriven decision making',\n",
       " 'Data Journalism, Data Visualization, Geospatial Mapping, Data Mining, Data Analytics, Risk Management, Insurance, Database Management, Data Analysis, Project Management, Communication Skills, Attention to Detail, Multitasking, Analytical Skills, Word Processing, PowerPoint, Excel, Database Programs, Data Mining Techniques, Data Sets, Innovation, Business Intelligence, Actuarial Science, CPCU, ESG, Sustainability, Resilience, Risk Modeling, Claims Handling, Estimating, Underwriting, Rating, Extreme Events, Life Insurance',\n",
       " \"Database Administration, Data Management, CompTIA Security+CE, Bachelor's in Engineering Computer Science Telecommunications, Remedy Action Request System, DBMS Design Concepts, Table Structures, Indexing Methods, Replication Schedules, Access Controls, Data Queries, Report Generation Formats, Database Usage Monitoring, Data Compatibility Troubleshooting, Data Integrity Troubleshooting, Performance Optimization, Database Administration Policies, Database Administration Procedures\",\n",
       " 'Quantitative Finance, AntiMoney Laundering (AML), Statistical Modeling, Machine Learning, Model Validation, Governance, CAMS Certification, Python, SAS, SQL, Technical Writing, Communication',\n",
       " 'Machine Learning, Generative AI, Python, C/C++, AzureML, GCP Vertex, AWS Sagemaker, Cloud Computing, Datadriven Development, Model Training and Finetuning, Model Testing and Deployment, Feature and Product Metrics, Research and Publications',\n",
       " 'Ontology, Data Modeling, Semantic Enrichment, Machine Learning, SQL, SPARQL, JSON, XML, RDF, RDFS, OWL, SKOS, SHACL, Python, R, OpenAPI/YAML, AVRO, Agile, Amazon Web Services, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World',\n",
       " 'Tesla Manufacturing Engineer, Manufacturing equipment and processes, Data analytics, Process integration, Factory optimization, Data architecture, Software solutions, Datadriven models, Data aggregation pipelines, Python, Apache AirFlow, SQL, ClickHouse, JMP, Minitab, Grafana, Plotly, Tableau, React JS, Pandas, MySQL, HDFS, Matplotlib, Vertica, Kafka, Weibull plots, Design of experiments (DOEs), Statistical analysis, Semiconductor manufacturing, Power electronics manufacturing, Battery manufacturing, High volume manufacturing, Data driven decision making, Mechanical Engineering, Electrical Engineering, Chemical Engineering, Computer Science, Mathematics, Statistics, Physics, Aetna PPO and HSA plans, Dental and vision plans, 401(k) with employer match, Employee Stock Purchase Plans, Company paid insurance, Employee Assistance Program, Sick and Vacation time, Paid Holidays, Backup childcare and parenting support resources, Voluntary benefits, Weight Loss and Tobacco Cessation Programs, Tesla Babies program, Commuter benefits, Employee discounts and perks program',\n",
       " \"Data Analysis, Data Extraction, Data Transformation, Data Visualization, Epic Clarity Reporting, SQL, Statistical Methods, Clinical Informatics, Usability Testing, Project Management, Stakeholder Observation, Python, R, Agile Development, Communication Skills, Organization Skills, Analytical Skills, Technical Skills, Healthcare Experience, Clinical Experience, Inpatient Experience, Outpatient Experience, Bachelor's Degree, Epic Clarity Data Model Certification, Epic Clinical Data Model Certification, Certified Health Data Analyst (CHDA) Certification\",\n",
       " 'Data analysis, Data mapping, SQL, SSMS, TSQL, Data modelling, Stored procedures, Data migration, Data cleansing, Data transformation, Data validation, Data migration requirements, Analytical skills, Problemsolving skills, BODS ETL tool, Extraction, SAP, Financial data migration, Financial data reporting',\n",
       " 'Data Center Infrastructure Management (DCIM), Electric Power Monitoring Systems (EPMS), Building Management Systems (BMS), Quality Assurance (QA), Standard Operating Procedures (SOPs), Industrial Control Programmable Logic Controllers (PLC), BAS/BMS systems, Chiller plants, HVAC, Single and 3phase circuits, Control Drawings, Sequence of Operations, Bill of Materials, Graphical User, Interface requirements, Functional Specifications, Electrical diagrams, Control diagrams',\n",
       " 'Ionizing radiation, Budget control, Liaison, Reporting, Regulation, CT, MRI, Mammography, Diagnostic radiology, NMHS/NMMC Policies/Procedures/Guidelines, Local/State/Federal policies/procedures/guideline/regulations/laws/statues, High School diploma, GED, AMA approved school, ARRT or RDMS, Mississippi Medical Technical License, CPR, Interpersonal skills',\n",
       " 'Machine Learning, Deep Learning, Apache Spark, Python, Statistical Modeling, Reinforcement Learning, Imitation Learning, Generative Adversarial Networks, TimeSeries Analysis, Dynamic Systems, Electric Vehicle Controls, Energy Management, Charging, Vehicle Suspension, Communication, Teamwork, Leadership, ProblemSolving, M.S. or Ph.D. in Machine Learning Engineering Computer Science or STEM, 5+ years of industry experience in ML systems',\n",
       " 'Data Analysis, Data Visualization, Forecasting, Passenger Volume Forecasting, Aviation Industry Research, Presentation Skills, Written Communication Skills, Microsoft Power BI, Microsoft Power Apps, Qualitative Data Analysis, Quantitative Data Analysis, Project Management, Data Collection, Data Management, Data Visualization, Innovative Ideas, Advanced Writing Skills, Hybrid Work, Resume, Cover Letter, Equal Opportunity Employer',\n",
       " 'AML Compliance, Customer Onboarding, Customer Due Diligence, Surveillance, Sanction, Training, Audit, RiskBased Approach, Data Analytics, ACAMS Certification, Communication, Independent Third Parties, Financial Regulations',\n",
       " \"crime analytics, data analysis, data interpretation, data management, data presentation, data visualization, law enforcement, strategic leadership, project management, report writing, public speaking, collaboration, problem solving, decision making, Bachelor's degree, 1 year of specialized experience in law enforcement security and crime analysis, 1 year of prior management/leadership experience, Strong understanding of tools databases techniques and resources available to conduct intelligence operations as well as strategic tactical and operational analysis, Experience supervising others, Valid motor vehicle operator's permit\",\n",
       " 'Data Warehousing, Data Reporting, Model Development, CrossFunctional Data Collaboration, Data Science, Biostatistics, Computer Science, 5+ years experience in data science, SAS, R, Python, ARIMA, Exponential Smoothing, Time Series Decomposition, Machine Learning Algorithms, AWS, Azure, Google Cloud, Data Integration, ETL, Talent, Informatica, SSIS, PowerBI, Tableau, SQL, Data Extraction, Data Governance, HIPAA, Data Visualization, Programming Languages',\n",
       " 'TOP SECRET security clearance, U.S. Citizen, Microsoft Suite, Front Office Leadership, Federal and DoD Acquisitions Knowledge, Project/Program Management, USAF/Cyber Experience',\n",
       " 'Mechanical, Electrical, HVAC, Plumbing, Low Voltage, Fire Sprinkler, Scheduling, Logistics, Quality control, Safety, Performance appraisals, Ethics, Compliance, Coordination, 3D modeling, Punchlist, Subcontractors, Suppliers, Building information modeling, Contract documents, Drawings, Specifications, Microsoft Office, Leadership, Interpersonal skills, Physical ability, Vision abilities, Lifting, Exposure to moving mechanical parts, Exposure to fumes or airborne particles, Exposure to outside weather conditions, Exposure to risk of electrical shock, Noise',\n",
       " 'Data Platform, Software Engineering, Data Warehousing, Data Lakes, AWS, Azure, GCP, DevOps, CI/CD, Kubernetes, Terraform, Data Warehousing Architecture, Data Modeling, ObjectOriented Programming, Functional Programming, Python, Golang, DBT, Apache Airflow, AWS GLUE, Spark, Software as a Medical Device, Relational Databases, Query Authoring, Performance Tuning, Cloud Security, Data Governance',\n",
       " 'Data Management, Data Analysis, Data Input, Data Organization, Data Quality Assurance, Data Troubleshooting, Data Visualization, Data Reporting, Data Interpretation, Data Presentation, Spreadsheet Software, Database Management, Data Analysis Techniques, Industry Best Practices, Attention to Detail, Computer Proficiency, Analytical Thinking, Communication Skills, Continuous Learning, Data Requirements Gathering',\n",
       " 'Artificial Intelligence, Machine Learning, Data Science, Biomedical Informatics, Sustainable Research Clusters, Research Collaborations, Grants Writing, Extramural Funding, Scientific Publication, Teaching, Mentoring, Industry Partnerships, Government Agencies, Conferences, Workshops, Seminars, Curriculum Development, Postdoctoral Supervision, Staff Supervision, Diversity, Equity, Inclusion, Compensation, StartUp Packages, Cover Letter, Curriculum Vitae, Research Statement, Teaching Statement, Professional References, Criminal Background Check, COVID19 Vaccination, Booster, Catholic, Augustinian, Liberal Arts, Science, Engineering, Nursing, Health Sciences, Education, Social Policy, Business, Carnegie Classification, NCAA Division I',\n",
       " 'Datadog, Splunk, Prometheus, Grafana, AWS Fargate (EKS Fargate), Modems, Routers, CMTS',\n",
       " 'Data Engineering, Azure, Data Science, Data Modelling, Data Warehousing, ETL/ELT, Agile, Scrum, Python, SQL, C#, DevOps, Automation, Continuous Integration, Cloud Architectures, Microsoft Certified: Azure Administrator Associate, Microsoft Certified: Azure Data Engineer Associate, Data Factory, Synapse Analytics, Databricks, Azure Functions, SSIS, SAP HANA, Snowflake, Delta Lake, Databricks',\n",
       " 'D360 Application, Data Governance, Data Governance Framework, Metadata Management, Data Quality, Data Engineering, Data Leadership, Risk, Security, Privacy, Data Governance Processes, Data Governance Tools, Data Governance Procedures, Information Security, Cyber Security, Information Management Practices, Information Lifecycle Management, Data Modelling, Master Data Management, Business Audits, Requirements Gathering, System Users, Business Requirements, Business Priorities, Technology Priorities',\n",
       " 'Data Center Technician, Server installation, Data Center Infrastructure Management (DCIM), Infrastructure Management, Information Technology ticket systems, Cat 6 Cat 6e fiber terminations and cabling, Test equipment operation, Assembly drawings diagrams schematics, Structured cabling, Fluke equipment, Telecommunications connectivity, Power provisioning, Power Distribution Units, Environmental monitoring, Schneider Electric DCIM, ServiceNow, Cabinet installation, Crossconnects, Remote hands requests, Vendor maintenance, Project installation, Troubleshooting, Telecommunication and server infrastructure, Compute resources, Cabinets, Power, Fiber, MDF, IDF, Network cabling, Equipment Maintenance, Interpersonal Communication',\n",
       " 'Data Automation, Data Engineering, ETL Development, Data Wrangling, Robotic Process Automation, Machine Learning Algorithms, SQL, MS SQL Server, TSQL, SSIS, Azure, Snowflake, Python, C#, Data Pipelines, Data Architecture, Data Sets, Root Cause Analysis, CrossFunctional Teams, Protected Sensitivity Data Points, Computer Science, Statistics, Informatics, Information Systems',\n",
       " 'Python, Spark, TSQL, Microsoft Fabric, Synapse, Purview, Azure Databricks, PowerBI, Database, Storage, Collection, Aggregation, Data pipelines, Data streams, System integration, Data security, Data integrity, Microsoft Analytics services, SQL, Data handling, Data understanding, Data analysis, Data interpretation, Data warehouses, Data mining, Data generators, Canned reports, Clustered column store tables, Realtime access to technical and skilled resources, Hacking, Innovation contests, Learning, Pivoting',\n",
       " 'Software Engineering, Data Science, Machine Learning, Distributed Computing, Statistical Methods, AI, Open Source Programming Languages, Data Engineering, CI/CD, Orchestration, Deployment, Scalable Tools, Infrastructure, MLOps, Algorithms, Annotation, Explainability, Experimentation, Distributed Capability, BS/MS/PhD in Computer Science Engineering Machine Learning or related scientific field, 10+ years of professional experience in data science and/or machine learning, Distributed Systems Architecture, Project Execution, Data Integration, Dynamic Business Environment, RSUs, ESPP, Continuous Professional Development, Product Training, Career Pathing, Inclusion Talks, Mental Health Benefits',\n",
       " 'Python, Data Engineering, SQL, Data Pipelines, Data APIs, Data Analysis, Reporting',\n",
       " \"Data Analysis, Data Visualization, SPSS, Excel, Graphs, Charts, Evaluation Report, Accountability Report, Juvenile Justice Reform, Children's Mental Health Crisis, Marginalized Communities, Data Warehousing\",\n",
       " 'Quality Data Specialist, Registered Nurse, Clinical information extraction, Data entry, Data validation, Data analysis, Retrospective review, Concurrent review, Clinical data abstraction, Data reporting, Data integrity, Data security, Data confidentiality, Communication, Training, Microsoft Office Suite, Excel, Word, PowerPoint, Spreadsheet, Database, Clinical experience, Computer skills',\n",
       " \"Sales strategy, Data analysis, Campaign management, Pricing strategies, Performance marketing, Subscription sales, Group sales, Telemarketing, Direct mail, Email marketing, Social media marketing, Paid digital advertising, Traditional advertising, Data extraction, Brand development, Communications, Budget management, Tessitura CRM, Microsoft Office products, Prospect 2, Event sales, Time management, Problemsolving, Communication skills, Grammar skills, Proofreading skills, Multitasking, Basic computer skills, Bachelor's Degree\",\n",
       " 'Software Development, Cloud Development, Cloud Management, Engineering Management, MongoDB, Distributed Systems, Database, Performance Tuning, Operational Experience, Hiring and Developing Teams, Open Source Databases, Remote Team Management, Distributed Team Management, CrossGeographical Collaboration',\n",
       " 'Clinical Research, Data Management, Data Analysis, Data Entry, Regulatory Compliance, Protocol Adherence, Electronic Medical Records, Case Report Forms, Study Management, Patient Recruitment, Monitoring Visits, Source Data, Clinical Issues, Study Requirements, Logs, Site Initiation Visits, Site Disease Group, PI Oversight Meetings, Multidisciplinary Teams, Study Binders, University Policies and Procedures, Patient Data, Screening, Enrollment, Adverse Events, Deviations, Tumor Measurement, Concomitant Medication, Delegation, Sponsor Logs, Protocol Logs, Work Environment, Teamwork, Professional Practice, Medical Center, Confidentiality, Attention to Detail, Time Management, Prioritization, Deadlines, Independence, Collaboration, Communication, Problem Solving, High School Diploma or Equivalent, No Experience Required',\n",
       " 'Data Science, Data Analytics, Algorithms, Business Process, Time Series Analysis, Forecasting, Multivariate Regression, Bayesian Inference, Minimax Procedures, Subjective Probability, Expected Utility Analysis, Operational Risk Analysis, Uncertainty Evaluation, MultiCriteria Decision Analysis, Qualitative Comparative Analysis, Causal Mapping, Game Theory, Decision Architecture, Systems Architecture Analysis, Organizational Analysis, Structural Reform, Supply Chain Analysis, Multinational Best Practices Evaluation, Logistics, AgentBased Modeling, Simulation, Business Process Analysis, Optimization, Use Cases, Software Program Change Requests, Technology Insertion, Software Development, Requirements Elicitation, Stakeholder Analysis, Communication, Data Architecture, Data Munging, Data and Feature Engineering, Predictive Analytics, Unstructured Text, Natural Language Processing, R, Python, SAS, MATLAB, Anaconda, IBM Blue, Oracle Big Data, Machine Learning, Data Mining, Statistical Network, GraphBased Algorithms, CloudBased Computing, Reports, Visualizations, Statistical Techniques, Heuristic Techniques, Descriptive Analytics, Predictive Analytics, Prescriptive Analytics, Statistical Tests',\n",
       " 'Financial Data Analyst, Freight and Logistics, Business Drivers and Trends, KPIs, Financial Reporting, Accounting/Finance/Business, Manufacturing, Supply Chain, Data Transformation, Business Insights, Relationship Building, Communication, Teamwork, Project Management, Planning, Organization, Microsoft Office, Power Query, Power BI, ERP Systems, Business Applications',\n",
       " 'Data Science, Machine Learning, Data Mining, Statistical Modeling, Data Visualization, Data Analysis, Data Engineering, Databricks, Python, R, Azure Synapse, Azure Blob, Azure ADF, SQL Server, Azure Key Vault, Azure App Insights, Power BI, Tableau, Big Data, Cloud Computing, Leadership, Mentoring, ProblemSolving, Communication, Analytical Thinking',\n",
       " 'Database administration, Geospatial Database, Data Modeling, Quality Control, Quality Assurance, Process Automation, Scripting, ETL platforms, Esri Product Suite, IT Customer Support, Documentation, Technical Writing, Conceptual Modeling',\n",
       " 'Agile, Configuration Management, Data Management, Quality Assurance, Document Control, Technical Writing, Program Operations, Systems Support, Software Engineering, Integration and Test, Specialty Engineering, Customer Service, Microsoft Office Suite, Product Lifecycle Management, Agile PDM, SolidWorks, ClearCase, Subversion, Confluence, Windchill, EIA649C, MILSTD973, CMPIC, NDIA, Statements of Work, Drawings, Schematics, Parts Lists, Lean Six Sigma',\n",
       " 'User administration, Standard Operating Procedures, Oracle Functional Setup Manager (FSM), Oracle Fusion Configuration Package Management, Oracle HCM cloud, Oracle Fusion HCM administration, Linux, Oracle SaaS applications, Oracle SaaS environments, TS/SCI clearance, FS polygraph, Database Administrator (DBA), Columbia Technology Partners, Equal Opportunity Employer, JazzHR',\n",
       " 'Network Engineering, Network Security, Routing, Switching, LAN/WAN/Datacenter Solutions, IP Routing Protocols (BGP OSPF RIP), Documentation, Verbal Communication, Microsoft Office, Visio Professional, Agile, Atlassian Jira Products, QoS Configurations, Cisco and Arista Routers/Switches, Layer 2 Switching Protocols (Spanning Tree Trunking Etherchannel), HSRP, CBWFQ, DSCP, NAT/SNAT, TCP/IP, Multicast, Ethernet, EVPN, MLAG, CVP, DNS/domain Services, Python/Ansible/GITHUB, Change Control Processes  Service Now, Change Control Process, Capacity Problems, System Related Issues, Risk Programs (Credit Market Financial Crimes Operational Regulatory Compliance), Wells Fargo Policies and Procedures, Risk and Compliance Obligations, Proactive Monitoring, Governance, Risk Identification, Escalation, Sound Risk Decisions, Drug and Alcohol Policy',\n",
       " 'SAP EAM/IAM, SAP QM, SAP MDG, SAP S/4 HANA, Siemens Teamcenter Quality Management System, Data analysis, Master data management, Data visualization, Python, SQL, Power BI, Cloud technology, Azure, AWS, ETL/Middleware, Semiconductor Fab Tool Maintenance, Quality business processes, MDM (Master Data Management) tools',\n",
       " 'Data Science, Machine Learning, Artificial Intelligence, Data Analytics, Business Intelligence, Data Mining, Data Research, Data Management, Data Integration, Data Visualization, Business Strategy, Collaboration, Communication, Problem Solving, Mathematics, Statistics, SQL, PowerBI, MySQL, Python, Tableau, SAS, R, Predictive Modeling, Supervised Learning, Unsupervised Learning, Timeseries Data, Natural Language Processing, Data Ethics, Data Confidentiality, Microsoft Office Suite, Teamwork, Initiative, Integrity, Detailoriented, Resourcefulness, Proactiveness, Strong Work Ethic',\n",
       " 'Data center management, Systems maintenance, Systems installation, Systems updates, Building records, Schematics, HVAC, Electricity, Racking, Cabling, Data center design, Data center monitoring, Customer escalations, Inventory management, Safety, Security, Power configuration, Power outages, Malicious attacks, Compliance, Regulations, Laws, Data center development, Data center installation, Software monitoring, Hardware monitoring, Data center standards, Data center systems',\n",
       " 'Data Analysis, Machine Learning, Data Visualization, Cloud Platforms (AWS GCP Azure), SQL, Python, C C++, Java, Cassandra, Hadoop, Spark, ElasticSearch, Big Data Frameworks, NoSQL, Data Warehousing, Statistics, Predictive Modeling, Clustering, Classification, Tableau, Regression, Simulation, Scenario Analysis, Decision Trees, Neural Networks, Ethical Considerations in Data Science',\n",
       " 'Engineering management (4+ years), Computer science, Software design, Distributed systems, Planning, Hiring, Onboarding, Employee development, Technical leadership, Postgres, Networking, DNS, Kubernetes, Internet performance, Internet security, etcd, Rook, Golang, Ceph, Prometheus, Consul, Vault',\n",
       " 'Embedded Systems, FaultTolerant Distributed Runtime Systems, DomainSpecific Languages, Distributed Database Systems, Systems Engineering, Autonomous Driving Runtime, Operating Systems, LargeScale Distributed Databases, Model Training Systems, Performance Optimization, Software/Hardware System Architecture, Computer Science, Software Development, Systems Engineering, Distributed Systems, OpenSource Data Platforms, CloudBased Systems, Low Level Platform BringUp, Device Drivers, Embedded Operating Systems, Secure Data Transfer, Big Data, Apache Spark, Compilers',\n",
       " 'ServiceNow, Development, Configuration, Administration, Troubleshooting, System performance, Functionality, Collaboration, Requirements gathering, Business needs, Training, Support, IT service management, Leadership, Mentoring, Project management, Team management, Secret clearance, Presentation skills, Communication skills, Computer literacy, Export control laws, Equal opportunity employer',\n",
       " \"Mathematical modeling, Statistical analysis, Datadriven insights, Inventory management, Demand forecasting, Network optimization, Replenishment, Logistics planning, Forecasting models, Optimization models, Linear programming (LP), Mixedinteger linear programming (MILP), Integer programming (IP), Data analysis, Machine learning (ML), Artificial intelligence (AI), SQL, Python, CPLEX, Cloud technologies, GCP, Azure, Spark, Hive, TensorFlow, Hadoop, Scikitlearn, Tensorflow, Torch, Keras, PyTorch, Pandas, Numpy, Matplotlib, Seaborn, Data visualization, Business intelligence, Big data, Data mining, Data warehousing, Data governance, Data security, Data ethics, Problemsolving, Critical thinking, Analytical thinking, Communication, Presentation, Teamwork, Leadership, Bachelor's degree, Master's degree, PhD\",\n",
       " \"Medical Laboratory Scientist, Clinical Laboratory Expertise, Medical Technicians, Blood Collection, EKG, Automated Laboratory Testing, NonAutomated Laboratory Testing, Quality Control, Quality Assurance, Variance Reporting, Laboratory Information System, Electronic Health Records, New Test Methodologies, New Instrumentation, Cost Effectiveness, Associate Degree, Chemical Science, Physical Science, Biological Science, Medical Laboratory, Bachelor's Degree, Medical Technology, Clinical Laboratory, Work Experience, Clinical Laboratory Experience, Medical Technologist (MT), Medical Laboratory Technician (MLT), Infant, Pediatric, Adolescent, Adult, Geriatric, OSHA Category I, Bloodborne Pathogens, Infectious Diseases, Respirator, Sitting, Standing, Walking, Stooping, Reaching, Repetitive Actions, Lifting, Carrying, Push/Pull\",\n",
       " 'Artificial Intelligence, Data Science, Analytics, Machine Learning, Python, NumPy, Pandas, Matplotlib, Seaborn, ScikitLearn, Keras, TensorFlow, PyTorch, Caffe, Deep Learning, Optimization Algorithms, Computer Vision, Digital Twins, Industrial Processes, Oil & Gas Downstream, Data Manipulation, Data Analysis, Data Modeling, Data Visualization, Project Management, Energy Efficiency, Cost Reduction, Anomaly Detection, Predictive Maintenance, Process Optimization',\n",
       " \"Data Analysis, Statistical Software, Stata, R, Qualitative Research, Research Synthesis, Report Writing, Brief Writing, Blogging, Public Speaking, Stakeholder Engagement, Policy Advocacy, Project Management, Problem Solving, Analytical Thinking, Communication, Presentation, Teamwork, Collaboration, Continuous Learning, Bachelor's Degree in Education Public Policy/Administration, Master's Degree, 4+ Years of Research Experience in Education Policy, P12 Education Policy Expertise, Federal and State Policy Context Understanding, Large Dataset Manipulation, Descriptive Analyses, Equity Focus\",\n",
       " 'Oracle DBA, OBIEE, EBS, SQL, Oracle IPM, EsBase, IPM Imaging, Database troubleshooting, Database design, Database configuration, Database implementation, Database maintenance, Database backups, Database documentation, Database tuning, Database performance monitoring, Database capacity management, Database automation, Database security, Database updates, Database hotfixes, Database code reviews, Linux, MCSA, MCSE  BI, Oracle eBusiness Suite, Technical writing, Documentation, Accountability, Situational Adaptability, Tech Savvy',\n",
       " 'Data Analysis, Financial Services, Statistics, Data Quality, Large Datasets, Stakeholder Management, Change and Integration Projects, Communication, Presentation, Agile Projects, TSQL, Python, R, Power BI, Snowflake, Alteryx, Hybrid Working',\n",
       " 'Data Reporting, Power BI, Power Apps, Reporting, Data Analysis, Data Visualization, Data Integration, Data Manipulation, Data Architecture, Data Quality, Data Science, Python, R, Azure SQL, Data Lake, Data Extraction, DAX, M, Technical Writing, Stakeholder Management, Team Leadership, Communication, Agile, Waterfall',\n",
       " \"Data Architect, Microservices, Automation, Elastic infrastructure, SQL, NoSQL, Data warehousing, Data architecture frameworks, Reference data, Metadata, Master data, Data security, Data encryption, Data privacy, Data scalability, Data resiliency, Azure, AWS, SDLC, CI/CD, DevOps, Agile, Scrum, Microservices, APIs, Messaging, Logging, Monitoring, Integration, Data engineering, Data architecture, Programming, Python, SQL Server, Oracle, NoSQL databases, Data lakes, Data warehouses, ETL, Master data management, Analytics, Business intelligence, Machine learning, Artificial intelligence, Data modeling, Data mining, Data management, Data warehousing, Data cleansing, Data visualization, Data reporting, Data security, Authentication, Authorization, Encryption, Data privacy, Data scalability, Data resiliency, Attention to detail, Analytical skills, Organizational skills, Task management skills, Leadership skills, Collaboration, Communication, Presentation skills, Ability to influence others, Strategic thinking, Accountability, Bachelor's Degree, Computer Science, Healthcare industry, Azure certification, AWS certification, National Agency Check with Inquiries (NACI) background investigation\",\n",
       " 'Machine Learning, Deep Learning, Python, Scikit learn, TensorFlow, PyTorch, Natural Language Understanding, Image Classification, Video Classification, MultiModal Classification, Content Moderation Systems, PowerBI, Plotly, Bokeh, D3.js, Matplotlib, Data Engineering, Data Manipulation, Stakeholder Communication, Teamwork, Collaboration, Adaptability, Flexibility, Experimental Design, Exploratory Data Analysis, Hypothesis Testing, Data Security, Data Integrity',\n",
       " 'Metadata management, Ontology engineering, Data modeling, Knowledge organization, Semantic technology, Machine learning, Data integration, Data mapping, Data standardization, SQL, SPARQL, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, Python, R, JSON, OpenAPI/YAML, AVRO, Amazon Web Services, Agile, Information science, Computer science, Engineering, Library science, Ontology, Semantics, Computational linguistics, Business strategy, Analysis, Enterprise semantic solutions, Graph databases, Project management, Communication skills, Teamwork, Problemsolving, Independent work, Detail orientation, Reading comprehension, Writing skills, Presentation skills, Collaboration, Time management, W3C standards, RDF, RDFS, OWL, SKOS, SHACL',\n",
       " 'SAP, SAP ECC, SAP S/4 HANA, Data Migration, Data Workstream, Factory Model Concept, Project Management, Quality Standards, Project Plan, Status Reports, Project Scope, Change Control, Risk Management, Project Communications, BODS, Syniti, Information Steward, LTMC, LSMW, SAP ADM, AGILE, SAFe, Jira, Prince 2, PMP, ClientFacing Communication, Internal Communication, SAP SolMan, English (written and spoken), Life Sciences/Pharmaceutical, Cloud, Infrastructure, Consulting, Applications, Business Process Services',\n",
       " \"Machine Learning, Model Development, Data Preparation, Feature Engineering, Algorithm Selection, Model Evaluation, Deployment, Monitoring, Bachelor's Degree, 5+ Years Experience, Python, Machine Learning Libraries, Deep Learning, Data Handling, Model Deployment, Docker, Kubernetes\",\n",
       " 'ML Ops, Engineering, Data engineering, ETL pipelines, Workflow orchestration, Monitoring, Visibility, Experimentation, A/B testing, Systems engineering, Systems ownership, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Product ownership, Hiring, Technical authority, Autonomy, Flexibility, Teamfirst mentality, Passion for vision, NYCbased, Longterm thinking, Fast shipping, Iterative development, Large accounting firm experience, Financial workflows experience, Corporate financial data, Financial products',\n",
       " 'Java, OOPS concepts, Multithreading, Cloud Data stack, Kafka, Spark, Cassandra, HBase, DynamoDB, Elastic Search, PCI Data, Design principles and patterns, Performance tuning, Relational Modeling, Dimensional Modeling, Unstructured Data Modeling, Design architecture review, Banking Financial domain, Cluster and parallel architecture, Highscale distributed RDBMS, SQL, Big Data tools and solutions, Data scientists, Data Lake, EDW, Data applications, Programming/scripting languages, Infrastructure configuration, Software engineering',\n",
       " 'Data Analysis, Tableau, SQL, MS Excel, Communication Skills, Data Analytics, Quality Outputs, Crossfunctional Collaboration, Financial Services, Requirements Gathering, Technical Investigations, Automation Tools, Process Monitoring, Financial Reporting, Dataled Controls, Automation, Business Analysis, Billing Processes',\n",
       " 'Medical Technologist, Medical Laboratory Technician, MT/CLS (ASCP), MLT/CLS (ASCP), Blood Bank, Clinical Lab Experience, Shift Work, Day Shift, Night Shift, Shift Differentials, SignOn Bonus, Relocation Assistance, Clinical Laboratory Tests, Laboratory Equipment, StateoftheArt Equipment, HighVolume Lab',\n",
       " 'Data Analysis, Visual Storytelling, Querying, SQL, Tableau, Python, AWS, Troubleshooting, Data Distillation, Narrative Writing, Collaboration, Communication, Seniority, Keynotes',\n",
       " \"Oracle database, Oracle portfolio, Troubleshooting, Server Admin, Storage, Network, Backup, Application Schema deployment, Data Guard, SQL Tuning, AWR, TSM, TDPO, Veeam, ZDLRA, OS, DB Security, RAC, ExaCC, Unix shell scripting, Interpersonal, Verbal, Written, Bachelor's, Computer Science\",\n",
       " 'Machine learning, Data analysis, Python, Java, Scala, Data visualization, AWS, Tableau, Azure, Data pipelines, EMR, S3, Spark, Databricks, Snowflake, Automated deployment, Docker, Statistical analysis, Forecasting, Planning, Pricing, Targeting, Ad delivery, Generative AI, CTR/CVR modeling, Multidisciplinary collaboration, Datadriven services, Applied research, Industry experience: digital video advertising digital marketing inventory forecasting planning pricing targeting decisions and efficient ad delivery',\n",
       " 'Data Science, Machine Learning, Statistical Analysis, Python, Java, R, MATLAB, Spark, PySpark, ScikitLearn, Data Architecture, Data Munging, Data Engineering, Predictive Analytics, Databases, SQL, Unstructured Text Processing, Natural Language Processing, Databricks, Qlik, Advana Analytics Platform, Supply Chain Management, Maintenance Data, Algorithm Implementation, Statistical Modeling, Heuristic Techniques',\n",
       " 'Data Science, Statistical Analysis, DataDriven Insights, Storytelling, Machine Learning, Text Analysis, Python, R, SQL, Git, Experimental Methods, Regression, Cluster Analysis, HLM, Social Network Analysis, Longitudinal Methods, Multivariate Analysis, Data Visualization, Business Intelligence, Decision Making, People Data',\n",
       " 'Project Management, Data Analysis, Report Writing, Finance Management, Stakeholder Engagement, Problem Solving, Communication Skills, Attention to Detail, Office Suite, Databases, Spreadsheets, Word Processing',\n",
       " \"Senior Project Manager, Data Warehouse Projects, Integration Projects, Project Management, Resource Allocation, Project Reporting, Healthcare Sector, NHS, Project Management Methodologies, Analytical Skills, Leadership Skills, Interpersonal Skills, Communication Skills, Presentation Skills, Strategic Thinking, ProblemSolving Skills, Multiple Project Management, Attention to Detail, Bachelor's Degree, Project Management Certification\",\n",
       " 'Snowflake, AWS S3, Lambda, DBT cloud, SQL, ETL, Data Models, Star schema data modeling, Unix, Linux, CI/CD pipelines, Data warehouses, Data lakes, Data applications, Big data technologies, AWS, Spark, Data extraction, Data transformation, Data loading, Data analytics, Data architecture, Data design, Reference architectures',\n",
       " 'Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Distributed Databases, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, ETL Pipelines, Data Compliance, Data Classification, Data Retention, Data Governance, Security, Scalability',\n",
       " 'Machine Learning, Data Science, Statistics, Computer Science, Python, R, TensorFlow, Pandas, Numpy, Scikitlearn, Data mining, Business Intelligence, Analytical thinking, Problemsolving, Research, Prototyping, Production environments, Scalability, Business acumen',\n",
       " 'Data Science, Machine Learning, AI, Gen AI/ML algorithms, H2O Driverless AI, H2O open source, Python, R, Supervised learning, Unsupervised learning, Clustering, Decision tree learning, Neural networks, Statistical techniques, Concepts (regression properties of distributions statistical tests), Data mining techniques, GLM/Regression, Random Forest, Boosting, Trees, Text mining, Social network analysis, H2O Wave, Tableau, PowerBi, H2O ML Ops, MLFlow, Generative AI, Large Language Models, Prompt Engineering, Retrieval Augmented Generation, LLMOps, Problemsolving skills, Communication skills, Presentation skills, Technical concepts',\n",
       " \"Data Management, Data Governance, Power BI, SQL, Excel, Python, Microsoft Dynamics, Dashboard Creation, Data Analysis, Reporting, Data Protection, People Management, Bachelor's Degree, Written Communication, Verbal Communication\",\n",
       " 'Food and Beverage Management, Leadership, Customer Service, Beverage Knowledge, Teamwork, Communication Skills, Responsible Service of Alcohol certification, Food Safety Course, Paid Parental Leave, Paid Wellness Days, Employee Discounts, Career Development Programs',\n",
       " 'SQL Server Development, Backend design, Legacy system optimization, Performance tuning, Data modeling, Data analytics, ETL, AWS Experience, AWS Lambda functions, AWS databases, AWS storage services, Technical evangelism, Communication, Cloud Formation, Rapid solution prototyping, AWS Glue, Scala, Lambda/Step Functions, State Machines, Control Flow, Data Lake experience, Project management, Requirements management, Version control',\n",
       " 'Data management systems, Data architectures, Data structures, Data throughput, Query performance, Backend database technologies, ETL, Operational suitability test support, Logistics and suitability experience, Space systems, Ground systems, Ground segments, Manufacturing and maintenance, Mission planning, Reliability, Availability, Compatibility, Transportability, Interoperability, Maintainability, Safety, Human factors, Manpower supportability, Logistics supportability, Environmental effects, System documentation, Training requirements, Test design, Test planning, Data collection, Data processing, Data evaluation, Data reporting, TS/SCI clearance, Polygraph clearance, Masters degree',\n",
       " 'Kubernetes, Kubeflow, MLOps, Data science, Data engineering, Machine learning, Artificial intelligence, Data pipelines, Model training, Model deployment, Monitoring, Management tools, DevOps, CI/CD, Containerization, Container orchestration, Docker, Spark, S3, Hadoop, scikitlearn, LGBM, Tensorflow, PyTorch, Java, Scala, Python, Kotlin, Communication, Collaboration, Teamwork, Code simplicity, Performance, Troubleshooting, Debugging',\n",
       " \"Data extraction, Data transformation, Data analysis, Healthcare data, Clinical data, Data visualization, Data reporting, Epic Clarity, SQL, Statistical methods, Clinical informatics, Usability testing, User experience, Project management, Agile Development, Python, R, Clinical experience, Communication skills, Organizational skills, Analytical skills, Technical skills, Bachelor's degree, 4 years of relevant experience, 2 years of data analysis in healthcare, Epic Clarity reporting proficiency, SQL proficiency, Strong grasp of analytics and statistics, Epic Clarity Data Model certification, Epic Clinical Data Model certification, Certified Health Data Analyst (CHDA) certification\",\n",
       " 'Database Marketing, Marketing Operations, Program Management, Database Management, Sales Enablement, PowerBI, Tableau, Project Management, Cross Functional Leadership, Sales Enablement, Competitive Profiles, Public Speaking, Business Communications, Team Collaboration, Diversity and Inclusion, Telecommunications Industry',\n",
       " 'Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Data security, Database table indexing, SQL technologies, Data handling, Data understanding, Data analysis, Data interpretation, Data manipulation, Error identification, Data modeling, Entity and relationship extraction, Data warehouses, Data storage, Relational databases, Data mining, Storage and extraction, Metadata, Information repositories, Metrics, Monitoring processes, Business intelligence tools, Report forms, Formats, Information dashboards, Data generators, Canned reports, Information portals, Resources',\n",
       " 'Java, Python, Distributed systems, Cloud computing, Technical lead, Data Factory, SQL DW, Cosmos DB, Power BI, Ataccama, Talend, Collibra, Snowflake, Stream Sets, Enterprise cloud data architecture, Azure, SAFe, Jira, Confluence, Azure DevOps, CI/CD principles, Spark, Databricks, Data science, Machine learning, Statistical modeling, Deep learning, Business context',\n",
       " 'Data analysis, Problemsolving, Data visualization, Strategic planning, Project management, Facilitation, Coaching, Communication, Interpersonal skills, Microsoft Office Suite, Oracle, Crystal Reports, Budget applications, Agile methodologies, Scrum methodologies, Budgeting, Human resources, Public administration, Public policy',\n",
       " 'Machine Learning, Statistical Methods, Geospatial Analytics, Changepoint Detection, Clustering, Graph Analytics, Bayesian Programming, Python, Java, C/C++, AWS, Hadoop, Spark, Dask, MapReduce, Git, Agile, Data Science, Computer Science, Mathematics, Statistics, Security Clearance',\n",
       " 'Securities Data Analysis, Investment Data Quality, Investment Data Ecosystem, Investment Data Management, Data Flows, Investment Data Consumers, Securities Life Cycle, Data Ingestion, End User Reporting, Vendor Management, Project Management, Process Optimization, Team Management, Communication Skills, Collaboration Skills, Excel, SQL/Oracle, Microstrategy, Reporting Tools, Agile, Prioritization, Decision Making, Eagle PACE, Eagle STAR, Degreed',\n",
       " 'Cloud computing, Data warehousing, Data migration, Analytics, Reporting, Sales, Business development, Client relationship management, Team leadership, Communication, Interpersonal skills, Problem solving, Decisionmaking, Integrity, Credibility, Eventdriven architectures, Domaindriven design, Humancentric solutions, Diversity, Inclusion, Equal opportunity',\n",
       " \"Leadership, Interpersonal skills, Analytical skills, Bachelor's degree, Material handling operations, Cycle counts, Inventory replenishment, Material returns, Safety measures, Problemsolving, Accuracy, Neatness, Policy adherence, Procedure improvement, Efficiency, Quality, Safety, Subordinate assistance, Healthcare, Vacation, Holidays, Sick days, Health insurance, Dental insurance, Vision insurance, Prescription drugs, Career advancement, Training, Development, Protective gear, Health advocacy, Life insurance, Shortterm disability, Longterm disability, 401(k), Vision coverage, Dental coverage\",\n",
       " 'Quantitative Finance, AntiMoney Laundering (AML), Machine Learning, AI, Python, PySpark, SQL, Hadoop, Statistics, Math, Economics, Team Player, Communication Skills, Programming Skills, SAS, MATLAB, AML Modelling Techniques, BitBucket, Horizon, PyCharm, JIRA, GRADOC, Risk Models, US Regulatory Environment',\n",
       " \"Data Engineering, Data Pipeline Frameworks, Data APIs, Data Delivery Services, Hadoop, Spark, Postgres, Angular JS, NoSQL, Scala, Python, SQL, Java, Big Data Technologies, Public Cloud (AWS Microsoft Azure Google Cloud), Distributed Data/Computing Tools (MapReduce Hadoop Hive EMR Kafka Spark Gurobi MySQL), Realtime Data and Streaming Applications, NoSQL Implementation (Mongo Cassandra), Data Warehousing (Redshift Snowflake), UNIX/Linux, Shell Scripting, Agile Engineering Practices, Bachelor's Degree, 4+ years experience in Application Development, Master's Degree, 6+ years of experience in Application Development, 4+ years of experience in Big Data Technologies, 4+ years of experience with a Public Cloud, 4+ years experience with Distributed Data/Computing Tools, 4+ year experience working on Realtime Data and Streaming Applications, 4+ years experience with NoSQL Implementation, 4+ years of Data Warehousing Experience, 4+ years of experience with UNIX/Linux, 4+ years of experience with Agile Engineering Practices\",\n",
       " 'Data reporting, SAP BI/Business Objects, Business requirement documents, Test plans, Universe design, Data definitions, Data governance, Data stewardship, Project management, Requirements definition, Requirements management, Communication skills, Analytical skills, Problemsolving skills, Planning skills, Prioritization skills, Teamwork skills, Organization skills, Business acumen, Technology acumen, Medicaid Claims adjudication, Health benefits, PTO, 401K, Paid family leave, Tuition reimbursement, Eyewear discounts, SAP Business Objects developers',\n",
       " 'TensorFlow, Signal Processing, Machine Learning, Deep Learning, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long ShortTerm Memory (LSTM), Spectrograms, Audio Formats, Python, Numpy, Pandas, Matplotlib, Scikitlearn, LibROSA, PyAudio, Wav2Vec, AudioKit, Agile, Agile, MLOps, Natural Language Processing (NLP), SpeechtoText, Pretrained Models, HuggingFace Hubs, Artificial Intelligence (AI), Machine Learning (ML), Educational Technology',\n",
       " 'Machine learning, Java, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Cloud computing, AWS, Microsoft Azure, Google Cloud, Cloud warehousing, Data Lakes, Realtime data, Streaming Applications, NoSQL implementation, Mongo, Cassandra, Cloud native data flow/movement, Airflow, Agile development, People management',\n",
       " \"AIpowered video games, Social networking, Web search, IC design, Product design, Medical diagnosis, Scientific research, Deep learning, ChatGPT, HPC, InfiniBand, Enhanced Ethernet, RAS/Resilience, Manufacturing test requirements, Test methodology, Test plan, Test flow, AI system RAS/Resilience models, Benchmarking, Risk assessment, Troubleshooting, Rootcause analysis, FIT rates, Data analysis, Manufacturing flows, Software tools/infrastructure, Architecture, Hardware, Software, Product engineering, NVIDIA's AI hardware, NVIDIA's AI software architecture, EE, CE, CS, Mathematics, Compute System RAS/Resilience model theory, HPC or AI system architecture, Cluster Interconnect technologies, Test equipment, Linux commands, Benchmark utilities, HPC or MLPerf benchmarking, Strong problemsolving, Troubleshooting, Rootcause analysis, Selfinitiative, Interpersonal skills, Flexibility\",\n",
       " 'Machine learning, Data science, Python, Linux, Image processing, Image classification, Object detection, Image segmentation, Tensorflow, PyTorch, Multiprocessing, Multithreading, Docker, Kubernetes, OpenCV, Git, Supervised learning, Unsupervised learning, Classification, Regression, Model selection, Regularization, Agile methodology, DevOps, MLOps, Software deployment, Confluence, Jira, Github, Mining, Processing plant operations',\n",
       " 'Data analysis, SQL, Data analytics, Machine learning, Python, R, Tableau, Data visualization, Databases, Business intelligence, Microsoft SQL Server, Statistics, Analytical skills, Microsoft Power BI, Data representation, Data mining, Data science, Business analysis, Java',\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL, Redshift, Snowflake, Machine learning, Distributed microservices, Full stack systems, Cloud computing, AWS, Microsoft Azure, Google Cloud, Agile, Unit tests, Code review, Distributed data, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Realtime data, Streaming applications, NoSQL implementation, Mongo, Cassandra, Data warehousing, UNIX/Linux, Shell scripting',\n",
       " 'Statistical analysis, Machine learning, Deep learning, Computer vision, Image processing, Python, Scikitlearn, TensorFlow, PyTorch, Survival analysis, Bioinformatics, AWS, Azure, MLflow, Histopathology, Tissuebased image analysis, Digital pathology, Biomarkers, Precision medicine, Bioinformatics, Cloud computing, Machine learning lifecycle management, Scripting',\n",
       " 'Registered Nurse, Intensive Care Unit (ICU), Interdisciplinary team, Quality of life, Evidencebased practice, Clinical competence, Research projects, Rotating shifts, Single beds, Negativepressure isolation rooms, Family entrance, Waiting area, Quiet room, Infection control, Support space, LPN support',\n",
       " 'Product Management, Agile, Data Management, Communication, Leadership, Strategic Thinking, Problem Solving, User Experience, Stakeholder Management, Product Development, Product Life Cycle, Business Requirements, Operational Management, Problem Management, User Focus, Working Within Constraints, Community Building, External Standards Assessment, Civil Service Pension, Flexible Working, Diverse and Inclusive Work Environment',\n",
       " 'Business Execution, Strategy Development, Data Analytics, Reporting, Tableau, Power BI, Service Now, SQL, SAS, Project Management, Issue Management, Exam Management, Research and Analysis, Communication, Visualization, Data Mining, Risk Management, Compliance, Automation, Dashboards, Executive Reports, Trend Analysis, Decision Making, Remediation, Policy Development, Process Improvement, Requirements Gathering',\n",
       " 'Data Visualization, Power BI, Data Analytics, Business Intelligence, Microsoft Office Suite, Written and Oral Communication, Team Management, Customer Presentations, New Technology Socialization, CrossFunctional Collaboration, Business Administration, Finance, Economics, MIS, Data Science, Mathematics',\n",
       " 'Atmospheric physics, Chemistry, Oceanography, Trace gases, Gas chromatography, Spectrometry, MATLAB, Igor, R, Python, Boundary layer processes, Data analysis, Interpretation, Research aircraft, Field measurements, Collaborative work, Communication skills, Detailoriented, Organization, Time management, Travel, Ph.D. degree, 2+ years of experience, Federal background check, Ability to work in a U.S. government facility, SkillSurvey questionnaire, Professional references, Degree verification',\n",
       " 'Data Engineering, Data Warehouse Development, ETL, Data Modeling, Data Governance, Big Data Technologies, Hadoop, S3, Cloud Data Warehouse Solutions, Snowflake, Redshift, Databricks, SQL, Spark (PySpark), Python, Java, Scala, Tableau, PowerBI, Mode, Superset, Machine Learning Algorithms, AI, Flink, Spark Stream, Data Security Protocols, GDPR, CCPA',\n",
       " 'R, Python, SQL, AI, Machine Learning, kNearest Neighbors, Naive Bayes, SVM, Decision Forests, Statistical Tests, Distributions, Regression, Maximum Likelihood Estimators, Statistical Tools, Data Visualization, Reporting, Strong Critical Thinking, ProblemSolving, Time Management, Prioritization, English',\n",
       " 'Network Intrusion Analysis, Log Analysis, System Image Analysis, Packet Capture Analysis, Incident Triage, Vulnerability Assessment, Signature Creation, Correlation Rules Creation, Threat Analysis, Forensic Collection, Intrusion Correlation Tracking, System Remediation, Incident Response Guidance, Incident Reporting, SOP Development, Workflow Development, Training, Heuristic Analysis, Interactive Searches, DrillDown Reports, Incident Handling, Security Monitoring, Vulnerability Management, Task Order Processing, Policy Development, Cybersecurity Certification, TS Clearance',\n",
       " 'Machine Learning, Deep Learning, AI, Computer Vision, Object Detection, Python, C++, Linux, Git, Automotive AI, Medical Device, Data Pipelines, Data Annotation, Ph.D., Research, Deployment',\n",
       " 'Java, Scala, Python, Apache Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Unix, Linux, Data Visualization, Tableau, Shell Scripting, Agile, AWS, Microsoft Azure, Google Cloud, MongoDB, Cassandra, Redshift, Snowflake',\n",
       " 'Data management, Data strategy, Data operations, Data analytics, Data privacy, Data governance, Business alignment, Data workflows, Data transformation, Master data management, Datadriven strategy, Data syndication, Product lifecycle management, Data visualization, SQL, Tableau, PowerBI, Python, Relational databases, Data models, Data structures, Industry standards, Microsoft Office, Project management, Communication, Data compliance, Regulation, IT',\n",
       " 'Agile, AWS, Batch Systems, Big Data, Cassandra, Cloud Computing, Data Modeling, Distributed Computing, EMR, Hadoop, Hive, Java, Kafka, MapReduce, Microservices, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, Streaming Systems, UNIX/Linux',\n",
       " \"Medical Laboratory Technician, Medical Technologist, ASCP Certification, Bachelor's Degree in Medical Technology/Medical Laboratory Science, 2 Years Clinical Experience, Laboratory Computer System, Quality Control, Quality Assurance, Specimen Collection and Processing, Data Input, Troubleshooting, Communication, Safety, Infection Control, Regulatory Compliance, Continuing Education, Procedure Development, Method Evaluation, Team Leadership, Scheduling, Delegation, Tuition Reimbursement, 401(k) Savings Plan, Health and Welfare Benefits, PTO Accrual\",\n",
       " \"Data Science Consulting, Data Analysis, Data Visualization, Tableau, Power BI, Python, R, SQL, IT Risk, Cyber, Communication, Organizational skills, Problem Solving, Secret federal security clearance, Bachelor's Degree, Master's Degree\",\n",
       " 'Data Science, Machine Learning, CausalML, Optimization, NLP, Explainable AI, Linux, SQL, Hive, SparkSQL, Python, Scala, Java, R, Git, Scrum, Software engineering, Data Mining, Bayesian Learning, Reinforcement Learning, Deep Learning, A/B testing',\n",
       " 'Software Development Technologies, Salesforce, Data Science, Amazon Redshift/Snowflake, API, REST/SOAP, SQL, Virtualization, Denodo, Data Pipelines, Visualization, Tableau, Scripting, Python, Data Modeling, DBA, Cloud Data Warehousing, Curiosity, SelfStarter, Proactive Problem Solving',\n",
       " 'Data Engineering, Azure Data and Analytics, Data profiling, Data cataloguing, Data mapping, Data pipelines, Data streams, System integration, Data warehouses, Databases, Data visualization, Data architecture, Data science, Business intelligence, Lakehouse, Data mesh, Cloud analytics, Team management, Program management, Project management, Software development methodology, Microsoft Data and Analytics technologies',\n",
       " 'Machine Learning, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Workflow Orchestration, Monitoring, A/B Testing, Systems Ownership, EndtoEnd ML Ops, Engineering Culture and Practices, Hiring and Team Building, LongTerm Thinking, Iterative Shipping, Autonomy, Flexibility, TeamFirst Mentality, Mentoring and Being Mentored, Passion for Vision, NYCBased, 95 Work Ethic, Generosity Compensation',\n",
       " 'Python, AWS, Airflow, Snowflake, Hadoop, Scala, Spark, Postgres, Angular JS, NoSQL, Data pipeline frameworks, Data APIs, Data delivery services, Analytical models, On premise, Cloud Production environments, Agile engineering practices, Unix/Linux, Shell scripting, MapReduce, Hive, Kafka, Gurobi, MySQL, Redshift, Tableau, Power BI, QlikView, SAS, R, SPSS, Data visualization, Data storytelling, Data mining, Machine learning, Data science, Data engineering, Data architecture, Data governance, Data quality, Data security, Data privacy, Data ethics, Big data, Cloud computing, Distributed data/computing, Realtime data and streaming applications, NoSQL implementation, Data warehousing, Agile development, Scrum, Kanban, DevOps, Testdriven development, Continuous integration/continuous delivery (CI/CD), Software development lifecycle (SDLC), Version control, Git, Jira, Confluence, Slack, Microsoft Teams, Zoom, Webex, Tableau, Power BI, QlikView, SAS, R, SPSS, Data visualization, Data storytelling, Data mining, Machine learning, Data science, Data engineering, Data architecture, Data governance, Data quality, Data security, Data privacy, Data ethics, Big data, Cloud computing, Distributed data/computing, Realtime data and streaming applications, NoSQL implementation, Data warehousing, Agile development, Scrum, Kanban, DevOps, Testdriven development, Continuous integration/continuous delivery (CI/CD), Software development lifecycle (SDLC), Version control, Git, Jira, Confluence, Slack, Microsoft Teams, Zoom, Webex',\n",
       " 'SQL, CCL, ETL, Data Engineering, Data Architecture, Machine Learning, Artificial Intelligence, Oracle Health Millennium, PACS, Syngo, REDCap, API, RDBMS, Clinical Applications, Healthcare Databases, Computer Science, STEM, Information Technology',\n",
       " 'ETL, Database design, Data architecture, Databricks, Data Lake, PySpark, Spark cluster management, DataWarehouse design, AWS, Azure Data Factory, Web Apps, Synapse, Python, Scala, SQL, C#, Java, Regression testing, AWS technology, Amazon S3, ECS, Lambda, Step Functions, AWS Glue, SQS, DynamoDB, Event Bridge/Kinesis/MSK, EC2, Elastic MapReduce, Glue, Redshift, CloudFormation, Azure, GCP',\n",
       " 'Business intelligence, Data analysis, Data governance, Data management, Data modeling, Data mining, Data science, Data visualization, PowerBI, SQL, TSQL, Business intelligence, Team management, Data analytics, Reporting, Microsoft Office, SQL Server, PowerBI, Statistics, Forecasting',\n",
       " 'Cyber security, Data protection, Information security, Cloud computing, Agile, Symantec Data Loss Prevention (DLP), JIRA, AWS, Scripting, Docker, Kubernetes, Jenkins, CI/CD, Threat analysis, Risk management, Web proxy, Email security, Endpoint security, Network security, Virtualization, Databases, Operating systems, Linux, Windows, Networking, Data loss prevention, DLP infrastructure engineering, DLP Controls, SaaS, IaaS, URL filtering, Proxy, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, CISSP, GIAC, CISM, CCSP, CISA, Security+',\n",
       " \"Risk Adjustment, Data Processing, Data Storage, Data Architecture, Reporting, Analytics, Subject Matter Expert, Business Operations, Process Improvement, Data Extraction, Data Reconciliation, XML, 837 Data, SQL Server, Visual Studio 2012, Oracle, Microsoft Access, Excel, MS Word, Adobe Reader, PowerPoint, Bachelor's Degree, 5 Years of Experience in Risk Adjustment, Ability to Consult with Senior Management, Ability to Provide Expertise on Analytic Changes, Ability to Query from Various Databases, Knowledge of Risk Adjustment Processes, Reporting and Analytics Experience, Knowledge of MS Office Suite\",\n",
       " 'Enterprise Data Governance, Master Data Management, Collibra, Data governance use case requirements, Technical design documents, Analysis, Data relationships, Data aggregation, Summarization, Data consolidation, Dimensional modeling, ETL design, SQL programming, Oracle, Microsoft SQL Server, Data Quality, Metadata analysis, Python programming, Software product configuration, Requirements capture, Software implementation methodologies, Agile, SCRUM, Software Development Lifecycle, Environment promotion, ETL/ELT, Data warehousing, Data marts, Data services, API development, REST architecture, Epic electronic health record, Flowsheets, Notes, Encounters, Lab results, Billing, Paid claims, Engineering, Computer science, Information systems, Healthcare data, Healthcare data implementation, Communication, Attention to detail',\n",
       " 'Data architecture, Data engineering, Data pipelines, DataOps, Data Mesh, Data governance, Data privacy, Machine learning, Data visualization, Data analysis, Data strategy, AWS, Azure, Google Cloud Platform, SQL, Snowflake, MySQL, SQL Server, NoSQL, DynamoDB, CosmosDB, Cassandra, MongoDB, Data Lake design, Delta Lake, Lakeformation, Iceberg, Data pipeline management tools, Airflow, Dagster, AWS Step Functions, Azure Data Factory, Kubernetes, Docker Swarm, Metadata management tools, Collibra, Atlas, DataHub, SparkStreaming, Pulsar, MLOps platforms, Sagemaker, Azure ML, Vertex.ai, MLFlow, Python, Java, C++, Scala',\n",
       " 'Senior Data Scientist, Data Science, Machine Learning, Python, R, SQL, Apache Spark, AWS, NLP, Statistics, Data Engineering, Experimental Design, Knowledge Graphs, Vector Similarity Search, Large Language Models',\n",
       " 'Social Work England registration, Eligible to work in the UK, 2+ years postqualified experience, Degree level or equivalent in Social Work, Complex decision making, Professional judgement, Service user involvement, Needs led assessment, Critical reflection, Analysis, Risk assessment, Safeguarding investigations, Protective services, Supportive services, Care planning, Commissioning, Direct work with adults and families, Multiagency collaboration, Statement preparation, Representation to judiciary',\n",
       " 'Construction, HVAC, Plumbing, Electrical, Low Voltage, Fire Sprinkler, Scheduling, Logistics, Quality Control, Trade Labor Staffing, Project Operation, Safety, Performance Appraisals, Ethics and Compliance, Recovery Strategies, Communication, Billing, Contract Documents, 3D Modeling, Project Inspections, Startup, Commissioning, Turnover, Punchlist, Coordination, Monitoring, Subcontractor Management, Project Changes, Daily Records, Preconstruction, Building Construction, Means and Methods, Scheduling, Cost Control, Contract Documents, Drawings, Specifications, Scopes of Work, Project Schedule, Leadership, Interpersonal Skills, Physical Demands, Climbing, Vision, Lifting, Working Environment, Exposure to moving mechanical parts, High precarious places, Fumes or airborne particles, Outside weather conditions, Risk of electrical shock, Noise, Microsoft Office Suite',\n",
       " 'Big Data, ETL, Data Warehousing, Data Engineering, GCP, Hadoop, HDFS, Hive, PySpark, Python, SQL, Hive/PySparkdataframes, Programming, Python/Scala, Cloud Native Principles, Architectures, Cloud Native System Architecture, Low Latency, High Throughput, High Availability, Cloud Native Applications, Docker, Kubernetes, OpenShift, Hadoop Architecture, Spark Architecture, UNIX Shell Scripting, NoSQL, HBase, Couchbase, MongoDB, Teamwork, MultiTasking, Communication',\n",
       " 'Software Engineering, Data Engineering, Data Applications Engineering, Apache Spark, Kafka, Spark Streaming, SQL, Python, Scala, Hadoop, AWS, Azure, GCP, CI/CD, Unit and Integration testing, Automation, Orchestration, REST API, BI tools, Jenkins, Cloud security, Networking, Data analytics, Computer Science, Information Systems, Engineering',\n",
       " 'Data Management, Analytics, Digital Product Development, Data Manipulation, Coding, Automation, Standards, Best Practices, Architectural Design, Citizenship, Autonomy, Stakeholder Management, Change Management, Communication, Agile Project Management, Product Delivery, Continuous Learning, Business Acumen, Commercial Acumen, Data Analysis, Data Visualization, Data Governance, Data Structures, Digital Security, Data Ingestion, SQL, Python, EDM, Rushmore',\n",
       " 'Laboratory Technologies, Clinical Trials, Pharmaceutical Industry, Clinical Research, Clinical Data Management, SOPs, Good Clinical Data Management, Data Governance, Data Management Processes, Data Collection, Data Cleaning, Data Reporting, Data Quality, Data Analysis, Regulatory Compliance, Oncology, Infectious Diseases, Therapeutic Areas, Scientific Communication, Interpersonal Skills, Problem Solving, Analytical Skills, Attention to Detail, Resourcefulness, Proficiency in Computer Applications, Life Sciences, Bioinformatics, Computer Science',\n",
       " 'Data Science, Analytics, Statistics, Biostatistics, R, Python, SAS, Interdisciplinary Collaboration, Consulting, Industry Experience, Teaching Experience, Research Experience, Research Interests, Research Plans, PhD, Doctorate, Faculty Credentials, Equal Employment Opportunity, Background Check, Criminal Background Investigation, Motor Vehicle Report, Credit Check, Preemployment Drug Screening, Verification of Academic Credentials',\n",
       " 'Sociology, Public Administration, Political Science, Statistics, Data Analysis, Query Writing, Business Intelligence, Healthcare, Data Collection, Data Reporting, Government, Research, Community Engagement, Project Management, Data Visualization, AntiRacism, Social Justice, Equity, Leadership, Collaboration, Stakeholder Engagement, Diversity, Inclusion, Microsoft Word, Microsoft Excel, Microsoft Outlook, Microsoft PowerPoint',\n",
       " 'AI, Machine Learning, Deep Learning, GPUs, HPC, InfiniBand, Ethernet, Linux, Benchmarking, Root Cause Analysis, Test Equipment, Test Methodology, Test Plan, Test Flow, Risk Assessment, Software Tools, Infrastructure, Manufacturing Flows, Architecture, Hardware, Mathematics, CE, EE, CS, Cluster Interconnect Technologies, Compute System RAS/Resilience Model Theory and Methodology, ProblemSolving, Troubleshooting, Institutionalizing RootCause Analysis, HPC or MLPerf Benchmarking, Equity, Benefits, Diverse Work Environment, Equal Opportunity Employer',\n",
       " 'Data Analysis, Data Visualization, R, Python, Excel, Tableau, Power BI, SQL, MS Office Suite, Agile Project Management, Statistics, Mathematics, Economics, EEO1s, AAPs, Regression Analysis, Classification Analysis, Pay Equity, ReductioninForce, Employment Discrimination, Affirmative Action Plans, Audits, LitigationRelated Exposure, Damages Models, Payroll Data Analysis, Exposure Analyses, Data Extraction, Data Cleaning, Data Interpretation, Data Presentation, Client Interaction, Attorney Interaction, Presentation Skills, Technical Writing, Teamwork, Customer Service, Multitasking, Prioritization, ProblemSolving, Continuous Improvement, Technology Innovation, Learning New Platforms',\n",
       " 'Pharmacy experience, Pharmacy technician license, Brand name/generic medications, Sig codes, Problem solving, Attention to detail, Accuracy, Ability to work in a fastpaced environment, Ability to work independently, Ability to effectively communicate professionally, Advanced computer skills, Time management skills, Team oriented, Ability to work with minimal direction, Ability to solve problems with minimal direction, Quality minded',\n",
       " 'Data Analysis, Visualization, Data Reporting, Data Governance, Statistical Analysis, Trend Analysis, Pattern Analysis, Business Intelligence, Data Interpretation, Data Extraction, Automated Tools, Data Visualization Tools, Data Management, Data Communication, Data Presentation, Problem Solving, Critical Thinking, Analytical Skills, Communication Skills, Teamwork Skills, Leadership Skills, Project Management, Data Modeling, Data Science, Databases, Programming Languages, Software Development, Software Engineering, Systems Design, Systems Architecture, Systems Integration, System Implementation, System Testing, System Deployment, System Administration, System Maintenance, System Security, Network Engineering, Network Design, Network Implementation, Network Management, Network Security, Cloud Computing, AWS, Azure, Google Cloud Platform, DevOps, Agile, Scrum, Kanban, ITIL, COBIT, ISO 27000, GDPR',\n",
       " 'Database Administration, Database Servers, Database Systems, Database Performance Monitoring, Database Troubleshooting, Database Repair, Database Backup, DISA STIG Settings Testing, NetApp Cert Dat Admin, MSSQL, ACCESS, Oracle, Sybase, Informix, ITIL Foundations, Software Development Life Cycle (SDLC), Cloud+, SSCP, Army Network Enterprise Center Operations, Data Compatibility, Data Integrity',\n",
       " \"Data Science, GEOINT Analysis, Python, JavaScript, SQL, Pig, R, Elasticsearch, Spatial Analysis, Data Mining, Database Structures, Analytic Information Extraction, Visualization, Top Secret/SCI (TS/SCI) Security Clearance, Bachelor's and/or Masters Degree, 5 Years of Relevant GEOINT Analysis Experience, Ability to Identify Retrieve Manipulate Relate and/or Exploit Multiple Structured and Unstructured Data Sets, Ability to Interpret and Evaluate the Results of Data Science Community's Methods Models and/or Algorithms, Ability to Initiate the Efficient Implementation of Methods Tools and Algorithms, Ability to Lead Work Unit Proactively Addressing and Responding to the Most Difficult Data ScienceRelated Challenges, Strong Writing and Briefing Skills, MS Office Suite, Strong Time Management Skills, Ability to Work Independently, Prior Military Service or Familiarity, Ability to Work in a Dynamic and Demanding Work Environment, Ability to Brief Staff Officers to Affect Decisions, Ability to Work Cooperatively as a Team Member and Provide Guidance to Customer Analysts\",\n",
       " \"Generative AI, Data Analysis, Data Management, Data Quality, SQL, Python, Excel, Data Documentation, Data Lineage, Data Flow Diagrams, Communication, Collaboration, Problem Solving, Teamwork, Bachelor's Degree in Computer Science or Information Systems, 5+ Years of Experience in Data Management and Analysis, HR Specialty Operations, Employee Data Analysis\",\n",
       " 'Data Warehousing, Data Marts, Data Stores, ETL, Data Visualization, Data Standards, Data Models, Business Solutions, Technology Policies, Data Mining Tools, Meta Data Management Tools, Data Collection, Master Data, Database Management, Report Generation, Data Modeling, SharePoint, Teams, SQL Server, Azure, PowerBI, MS Windows, Office 365, Zachman Framework, TOGAF Framework',\n",
       " 'Data science, Data analysis, Machine learning, Artificial intelligence, Algorithms, Predictive modelling, Regression, Optimization, Deep learning, Reinforcement learning, NLP, Data visualization, Python, R, SQL, Spark, PowerBI, AWS, Statistical techniques, Cloud platform, Communication skills, Presentation skills, Negotiation skills, Stakeholder management, Problemsolving skills, Creative thinking, Analytical discipline, Tertiary degree',\n",
       " 'Digital marketing, Marketing analytics, Datadriven insights, Digital consumption best practices, Audience acquisition, Engagement opportunities, Customer experience activities, Data analysis, SQL, Python, R, Tableau, Power BI, CRM systems, Marketing automation platforms, Business acumen, Numerical and analytical skills, Executive level presentations, Change leadership, Complex initiatives, Data privacy regulations, GDPR, CCPA, Data visualization',\n",
       " 'Java, Scala, Python, Redshift, Snowflake, Machine Learning, Microservices, Fullstack Systems, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, NoSQL Databases (Mongo Cassandra), UNIX/Linux, Shell Scripting, Agile Engineering Practices, Data Warehousing, Cost Optimization, FinOps, Data Collection, Data Storage, Data Access, Data Analytics, Big Data',\n",
       " 'Machine Learning, Speech Analysis, Data Strategy, Model Training, Model Deployment, Experimentation, ProductionGrade Models, Cloud Services, R&D, Communication, Leadership, Collaboration, Data Requirements, Jupyter, Numpy, Pandas, Numba, Torch, Tensorflow, Python, GCP, AWS, Azure, Multidisciplinary Skills, Agile Planning, MVP, Project Management, FDA Approvals, SoftwareasaMedical Device (SaMD), MLFlow, WandB, DataBricks',\n",
       " 'Data center maintenance, Data center management, Data center operation, Hardware maintenance, Hardware inspection, Server maintenance, Storage maintenance, Network maintenance, Hardware troubleshooting, Hardware diagnostics, Server installation, Server configuration, Server maintenance, Network switch management, Network router management, Network firewall management, Network traffic monitoring, Security protocol implementation, Security protocol enforcement, Power distribution monitoring, Battery backup systems monitoring, Equipment inventory management, Maintenance activity logging, System configuration management, Disaster recovery planning, Data center certifications, Data center technologies, Operating systems (Linux Windows), Server virtualization, Data center management tools, Troubleshooting, Problemsolving, 24/7 operational environment, Lifting (50 lbs), Climbing, Ladder work, Medical benefits, Dental benefits, Vision benefits, HSA contribution, FSA, 401(k), Stock purchase plan, Life insurance, Disability insurance, Employee assistance program, Sick leave, Vacation time, Paid holidays, Childcare support, Voluntary benefits, Weight loss program, Tobacco cessation program, Tesla Babies program, Commuter benefits, Employee discounts',\n",
       " 'Scala, Python, Java, Agile, Unix/Linux, MySQL, MapReduce, Hadoop, Hive, EMR, Kafka, MongoDB, Cassandra, Tableau, Redshift, Snowflake, AWS, Microsoft Azure, Google Cloud, SQL, RDBMS, NoSQL, Data warehousing, Data visualization, Machine learning, Distributed microservices, Full stack systems, Gurobi, Spark',\n",
       " 'Apache, SQL, Data Engineer, AWS, GCP, Azure, SnowFlake, Data Modeling, Data Warehousing, Kinesis, Kafka, Python, Java, Scala',\n",
       " \"Machine learning system, Platform, Software, System, Monitoring, Management, Platform infrastructure, Services, GPU clusters, Kubernetes, Kubeflow, YARN orchestrations, Problem solving, Data analysis, Communication, Golang, Python, Shell, Bachelor's degree, Computer Science\",\n",
       " 'Clinical laboratory skills, Specimen processing, Instrumentation, Quality control, Laboratory computer systems, Data input, Training, Continuing education, Medical laboratory technology, Chemistry, Biology, ASCP MLT certification, Personal protective equipment, Lab coat, Gloves, Protective eyewear, Radiation safety, Chemical safety, Physical demands, Lifting, Standing, Computer skills',\n",
       " \"Data Loss Prevention (DLP), Cyber Security, Software Engineering, Agile Frameworks, Cloud Computing, Symantec Data Loss Prevention (DLP), JIRA, AWS, CISSP, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, GED, Bachelor's Degree in Cybersecurity Systems Engineering or Computer Science\",\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Agile engineering practices, Machine learning, Distributed microservices, Full stack systems, SQL, UNIX, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, Redshift, Snowflake',\n",
       " 'Data Science, Business Analytics, SQL, R, Python, Data Visualization, Data Modeling, Experimental Design, Machine Learning, Data Storytelling, Hypothesis Testing, Causal Inference, AB Testing, Data Ethics, Cloud Computing, Big Data Platforms, Agile Development, Jira, Confluence, Looker, Tableau, Data Warehousing, Data Engineering, Experimentation, Product Management, Data Collaboration, Python, SQL, R',\n",
       " 'Data Analytics, Data Architecture, Big Data Platforms, ETL/ Data Warehouse, Data Storage, Data Integration, Data Consumption, Enterprise Data, Business Intelligence, Data Pipelines, Dimensional Databases, Data Acquisition, Data Transformation, Data Processing, Data Management, Data Recovery, Data Archiving, Data Migration, Data Security, Data Governance, Data Visualization, Data Warehousing, SQL, Python, Java, Hadoop, Spark, Hive, Pig, Tableau, Power BI',\n",
       " 'Data Center Construction, Project Management, Transmission Engineering, Data Center Architecture, Engineering',\n",
       " 'Data Science, Product Development, Engineering, Design, Experimentation, Analytics, Advanced Modeling, Growth Strategies, Scalable Processes, Data Visualization, SQL, R, Python, Data Models, Experimental Design, Business Intelligence, Fintech, Structured Data, Unstructured Data',\n",
       " \"Data Mining, Data Processing, Data Manipulation, Data Analysis, Data Presentation, Financial Modeling, CRM Data Management, PowerPoint Presentations, Tableau, Salesforce, Dealogic, Bloomberg, Thomson Reuters, Capital IQ, Excel, Macros, Pivot Tables, Corporate Banking, Capital Market Products, Investment Banking, Bachelor's Degree in Finance/Accounting, 13 Years of Financial Reporting Experience, Strong Data Reporting Experience, Strong Presentation Skills, ResultsOriented, Strong Work Ethic\",\n",
       " 'Warehouse management systems, Hand tools, Power tools, Laser measuring devices, Cabling, Rack hardware, Straddle stackers, Rack movers, Electric pallet jacks, High school diploma, 46 years of experience, Interpersonal skills',\n",
       " 'Data Protection, Backup, Cohesity, Data Domain, Avamar, Veeam, Commvault, Cloud Native, SQL, Oracle, VMWare, NAS, PowerShell, Python, API Development, Automation, Encryption, Databases, Networking, Hardware, Software, Interfaces, Operating Systems',\n",
       " 'Master Data Management (MDM), Data Enrichment, Informatica MDM/DG, SAP MDM, Data Quality, Data Governance, Agile, SDLC, SQL, Data Transformation, ETL, Data Profiling, Data Scorecard, Exception Management, SAP, Business Process Workflow, Data Cleansing, Data Ingestion Pipelines, Data Matching & Deduplication, Standardization, Program Support, Thought Leadership, Data Harmonization, HighLevel Design',\n",
       " 'Project management, Microsoft Project, 5G telecommunication system design, Internet of Things (IoT) systems, Technical data packages, Engineering drawings, Testing and evaluation, Safety compliance, NIST SP 800171, PMP Certification, Federal contract, DoD Secret Security Clearance, Troubleshooting, Communication, Interpersonal skills',\n",
       " 'Data Analysis, Data Interpretation, Data Mining, Statistical Techniques, R, Python, SQL, Tableau, Power BI, Hypothesis Testing, A/B Testing, Data Visualization, Analytical Skills, Critical Thinking, Problem Solving, Communication Skills, Collaboration Skills, Data Management, ETL Processes, Data Quality Assessment, Data Integrity, Data Accuracy, Data Completeness',\n",
       " \"SQL, Python, Java, Scala, Data engineering, Software engineering, Data modeling, Data warehousing, Agile development, Cloud providers, AWS, Azure, System testing, Unit testing, Technical documentation, Software design, Component level architectures, Software module construction, Bachelor's degree, Computer Science, Engineering\",\n",
       " 'Vendor Master Data Analyst, Analysis, Financial Data, Feasibility, Profitability, Master Data Governance (MDG), System Support, Subject Matter Expertise, Process Improvement, Business, Finance, Accounts Payable (AP), Accounts Receivable (AR), Professional Consulting, Custom Projects, Professional Resource Augmentation, Standardization, Specialization, Collaboration',\n",
       " \"Data Management, Data Strategy, Data Analytics, Data Operations, Workflow Management, Master Data Management, Data Governance, Data Privacy Compliance, SQL, Tableau, PowerBI, Python, Relational Database Concepts, Industry Standards, Microsoft Office Suite, Project Management, Team Collaboration, Communication, Computer Skills, Bachelor's in Information Systems Computer Science Statistics Mathematics or Data Analytics\",\n",
       " 'Data engineering, Data science methodologies, Data warehousing, Data modeling, Data architecture, Data curation, Data analysis, Data visualization, TSQL, Stored procedures, Window functions, Common table expressions, Derived tables, Dynamic TSQL, Query optimization, Refactoring, Design patterns, Abstraction, Encapsulation, Azure data factory, Data lake, Star schema, Data lineage, Clinical trial, Pharmaceutical development, Data privacy, Anonymization, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP, CDISC, FHIR, OMOP, MS Office',\n",
       " 'Machine Learning, MLOps, Data Science, Computer Science, Deep Learning, Python, NumPy, Pandas, Numba, PyTorch, TensorFlow, Jupyter, Cloud Services, NLP, Clinical Research, Product Development, Project Management, Team Leadership, Communication Skills, Interpersonal Skills, Research Skills, Agile Methodologies, FDA Approvals, Speech Processing, Audio Classification',\n",
       " 'Data Analysis, Data Warehousing, ETL, BI Architectures, RDBMS, DDL, SQL, Semantic Layer Reporting Views, Data Discovery Sessions, Policy & Claims Insurance, Enterprise Data Warehouse',\n",
       " 'Medical Terminology, Clinical Laboratory Techniques, Sample Collection and Analysis, Equipment Maintenance, Patient Data Management, Sterile Techniques, Infection Control, ASCP Certification, AMT Certification, HEW Certification, AAB Certification, BLS Certification (AHA), State License',\n",
       " 'Natural Language Processing, Machine Learning, Data Science, Python, SQL, Text representation, Semantic extraction techniques, Data structures, Supervised learning, Unsupervised learning, Software skills, Kubernetes, Transformerbased models, Large language models, Data analytics, Taxonomy experts, Software engineers, Data scientists, Data engineers, QA engineers, U.S. security clearance, Startups, Entrepreneurial environment',\n",
       " 'Data analytics, Data management, Data storage, Programming, Python, R, C/C++, GitHub, Azure DevOps, Agile software development, Software engineering, Software development lifecycle, Unit testing, Component testing, Integration testing, System testing, Release management, Opensource software, Software design, Software distribution, Scientific research, Infectious disease modeling, Public health, Global health, Health equity, Poverty reduction, Lowresource settings',\n",
       " 'Data Architect, Data Governance, Data Warehousing, Cloud Technologies, Databricks Architect, Unified Data Analytics, Spark SQL, Delta Lake, Spark Core, Spark Architecture, Spark API, Degree in Computer Science or related field',\n",
       " 'SQL, Data Mining, R (Programming Language), Data Visualization, Databases, Machine Learning, Statistics, Python (Programming Language), Tableau, Analytical Skills, Data Science, Business Analysis, Business Intelligence (BI), Microsoft SQL Server, Data Representation, Data Analytics, Analytics, Data Analysis',\n",
       " 'SAP S/4HANA, Data Design, Network and Supply Chain Planning, Data Governance, Data Standards, Business Data Requirements, Conceptual Models, Business Glossary, Ontologies, Data Migration, Data Quality, Data Cleansing, Data Enrichment, Data Governance Framework, Global Data Standards, Data Lifecycle Management, SAP Transactions, SAP Reporting, SAP Architecture, Business/IT Partnering, Global Team Leadership, CrossFunctional Collaboration, Global Working, Cultural Sensitivity, Life Sciences, Healthcare, Data Quality Management, Data Process Improvement, Key Business Processes',\n",
       " \"Data Management, Data Quality Assurance, Data Governance, SAP Enterprise Asset Management System, Water Equipment, Wastewater Equipment, Maintenance Plan Development, Condition Assessment, Project Management, SQL, GIS, Microsoft Office Suite, Field Work, Driver's License\",\n",
       " 'Big Data, A/B testing, SQL, Tableau, Looker, R, Python, Statistics, Data visualization, Data management, Analytics, Data science, Reporting, Communication skills, Ownership mindset, Problemsolving, Innovation, Collaboration, Trust',\n",
       " 'Agile, AWS, Cassandra, Cloud computing, Data engineering, EMR, Hadoop, Hive, Java, Kafka, MapReduce, Microservices, Mongo, MySQL, NoSQL, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux',\n",
       " \"Computer literacy, Verbal communication, Written communication, Maintain, Create systems, Analytical ability, Facility working with numbers, Attention to detail, Relational database software, Raiser's Edge, Luminate, Omatic, Data reporting, Data analysis, Project management, Data entry, Reporting\",\n",
       " 'Machine Learning, Generative AI, PRIME system, Cloud Risk Management, Business Process Management, Change Management, Risk Management, Risk Certifications (CRISC CISM CRCM CIPP ABA Risk Mgmt Certification), Project Management, Compliance, Legal, Regulatory, Operations, Technology, Cybersecurity, SaaS, Risk Guide',\n",
       " 'Clinical laboratory procedures, Blood group type and compatibility tests, Quality control testing, Data analysis, Solutions and stains preparation, Specimen collection, Patient instructions, Equipment handling, Confidentiality, Communication, Customer service, San Carlos Apache Healthcare Corporation policies and procedures',\n",
       " 'Digital marketing strategy, Channel strategy, Datadriven insights, Data analysis, SQL, Python, R, Tableau, Power BI, Customer Relationship Management (CRM), Marketing Automation, Business acumen, Analytical skills, Presentation skills, Complex Initiatives, Business Analytics, GDPR, CCPA, IT infrastructure, Application development, Business systems, Collaborative and social technologies, Information security, Project leadership',\n",
       " 'Data science, Machine learning, Deep learning, Hadoop, Azure, SQL, ETL, Data visualization, Big data architecture, Data analytics, Apache Hadoop, Sqoop, Flume, Kafka, Oozie, Zookeeper, HCatalog, MPP, NoSQL, Data warehouse design, BI reporting, Dashboard development, Programming languages, Python, R, Data lakes, Data lake governance, Data Security, HITRUST, Data Integration & Interoperability, Agile development, DevOps',\n",
       " 'Medical Laboratory Scientist (MLS/MT), Hematological, Blood banking, Chemical, Microscopic, Microbiology, Quality control, Customer service, Communication skills, Teamwork, Problem solving skills, Leadership skills, Computer skills, Laboratory equipment maintenance, Data analysis, Quality assurance, Safety procedures, Infection control, Patient care, Attention to detail, Time management, Organizational skills, Ability to work independently, Ability to work under pressure, Ability to lift and carry heavy objects, Ability to stand for long periods of time, Ability to work in a fastpaced environment, Ability to work with a diverse population, Ability to work in a variety of settings, Ability to work with a wide range of equipment',\n",
       " \"Leadership, Interpersonal skills, Analytical skills, Bachelor's degree, Material handling operations, Safety measures, Inventory replenishment, Cycle counts, Server racks assembly, Server racks installation, Healthcare, Vacation, Holidays, Sick days, Full health insurance, Career advancement, 401(k), Life insurance, Shortterm disability, Longterm disability\",\n",
       " \"Product Management, ML/AI, Data Science, Machine Learning, Information Retrieval, AB Testing, Experimentation, Agile, Communication, Leadership, Stakeholder Management, Business Analysis, Requirements Gathering, Product Development, Product Lifecycle Management, Market Analysis, Competitive Analysis, Industry Trends, AdTech, MarTech, Sponsored Product, Sponsored Search, Paid Search, Sponsored Ads, Retail Media, Retail, Media, Advertising, Technology, Bachelor's Degree, Computer Science, Engineering, Master's Degree, Business Administration\",\n",
       " 'Data Engineering, SQL, Python, Tableau, Spotfire, Power BI, SSIS, dbt, Data API, Matilion, Boomi, Snowflake, Databricks, Delta Lake, Azure, AWS, Google, VM, kubernetes, Azure ACI, Kafka, Docker, Docker Compose, Linux, Agile, Data Visualization, Data Architecture, Data ETL, Cloud Architecture, Cloud Data Tools, Data Cleansing, Data Delivery, Data Models, Data Quality, Data Governance, Data Education, Data Security, Software Engineering, Web App Development, Applied Math, ML Frameworks, R, .Net, React, Vue, Streamlit, Ambiguity Management, Innovation, Agile Methodology, Computer Science, Engineering',\n",
       " 'Python, SQL, Relational database, Data warehousing, AWS Redshift, Athena, RDS, BigQuery, Big data processing systems, Distributed computing, Databricks, Spark, Sagemaker, Kafka, ETL design, Hadoop, Hive, EMR, MySQL, NoSQL, MongoDB, Cassandra, Airflow, UNIX/Linux',\n",
       " 'Project management, Program management, Technical project support, Network infrastructure delivery life cycle, Change management, Service delivery, Jira, Confluence, LeanAgile, Scrum, Kanban, SAFe Agile methodologies, Initiative, Execution focus, Commitment to dates, Analytical skills, PMP certification, ITIL certification, CCNA certification, Risk management, Decision making, Strategic thinking, Ownership, Accountability, Communication skills, Detail orientation, Technical acumen, Partnership, Automation, Process improvement, Financial services experience, Nimble, Flexibility, Proactive',\n",
       " 'Project Management, Data Science, Analytics, Client Relationship Building, Agile Methodology, Data Analysis, Data Visualization, Public Speaking, Data Science Research, Data Science and Analytics Consulting, Quantitative Analysis, Computational Analysis, Python, R, SAS, SQL, Tableau, Power BI, Scrum Master, Agile Product Management, Agile Product Owner, Scaled Agile Framework (SAFE), PMIAgile, ICAgile, EWM (IBM Engineering Workflow Management), Ability to obtain Public Trust Clearance, Secret Clearance, Top Secret Clearance, Federal Defense & Security Experience, Business Development, RFP (Request for Proposal), RFQ (Request for Quotation), RFI (Request for Information), White Paper Writing, Presentations, Demonstrations, US Citizenship',\n",
       " 'Java, AWS, CloudFormation, Terraform, ARM templates, SQL, NoSQL, REST, GraphQL, Scrum, Kanban, Agile, AWS lambda, Prometheus, Data management tools, Relational databases, Distributed systems, Domaindriven design, Data curation, Data quality, Data warehousing, ETL, Data processing, Data architecture, Data ingestion, Data distribution, Low latency infrastructure, Risk management, Machine learning, Financial engineering, Big data, Mobile development, Quantitative analysis, Business Applications, Software Engineering, Problem Solving, Analytical Skills',\n",
       " 'Critical facility engineering, MEP (mechanical/HVAC/electrician) experience, Troubleshooting, Data center experience, Root cause analysis, Single point of failure identification, Maintenance coordination, Vendor management, Building monitoring system supervision, Plumbing and fire suppression system operation, Safety system maintenance, Process improvement, Energy efficiency, Problem solving, Physical dexterity, Attention to detail, Teamwork, Communication',\n",
       " 'Agile, AWS, Big Data, Cassandra, Cloud Computing, EMR, Gurobi, Hadoop, Hive, Java, Kafka, MapReduce, Microservices, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux, Unit Testing',\n",
       " 'Data Engineering, Data Architecture, AWS, Python, SQL, Scala, Machine Learning, Big Data, Ontology Standards, Financial or Banking, Data Modeling',\n",
       " 'Java, Backend development, Distributed systems, Design patterns, Communication skills, Apache Flink, Samza, DataFlow, SQL, NoSQL, Time series, GraphDB, Cloud environments, GCP, Linux fundamentals, Networked computing environment concepts',\n",
       " 'Mental healthcare, Voice biomarker software, Machine Learning, Speech analysis models, MLOps, Team leadership, Data collection strategies, ML experimentation, Architectural design, Model training, Business impact, Cloud services, Agile methodologies, Speech processing/recognition, Audio classification, Experiment tracking, Reproducibility tools, FDA approvals, Softwareasamedical device (SaMD), Empathy, Mentoring, Multidisciplinary skills, Strategic planning, Agile planning, MVP, Communication skills, Feedback, Acknowledgments, Project management, Timelines, Computer Science, Python, Numpy, Pandas, Numba, Torch, Tensorflow, Jupyter',\n",
       " 'HRIS data entry, HR data management, Human resources, Employee file maintenance, Data entry, Employee onboarding, Offboarding, Employee benefits, PowerPoint, Microsoft Excel, HRIS system, Employee relations, Background checks, Drug screens, MVR checks, New Hire processing, Termination processing, Record filing, Digital records, Invoices reconciliation, HR forms creation, HR documents creation, Organizational skills, Planning skills, Data management skills, Data management skills, High integrity, Professionalism, Confidentiality, Tech savvy, Excel skills',\n",
       " 'Data analysis, Data science, Statistics, Computer science, Programming, SQL, Snowflake, Oracle, SPSS, APEX, Tableau, Python, Informatica, ETL, ELT, Data governance, Data quality, Regulatory compliance, Data warehousing, AWS, GCP, Azure, Git, Scrum, Kanban, Power BI, Qlik, R, Stata',\n",
       " 'Data extraction, Data transformation, Data analysis, Healthcare data, Clinical operations, Data analysis reports, Data visualizations, Epic Clarity reports, Clinical informatics, Clinical workflows, Usability testing, Strategy documents, Data dictionaries, Project management, Agile Development, Written communication, Oral communication, Organizational skills, Analytical skills, Technical skills, SQL, Python, R, Epic Clarity reporting, Epic clinical modules, Epic Clarity Data Model certification, Epic Clinical Data Model certification, Certified Health Data Analyst (CHDA) certification',\n",
       " 'Machine Learning, Data Science, Natural Language Processing, Speech Recognition, Audio Classification, Cloud Services, Agile Development, Sprint Management, Model Training, Model Deployment, Model Efficiency Optimization, Experiment Tracking, Reproducibility Tools, FDA Approvals, SoftwareasaMedical Device (SaMD), Python, Numpy, Pandas, Numba, Torch, Tensorflow, Jupyter',\n",
       " 'Underground Hard Rock Mining, Supervisory Common Core, Strong communication skills, Organizational skills, Time management skills, Ability to work in a crosscultural environment, Frontline supervision, Mucking, Drilling, Hauling, Safety interactions and administration, Production reports, Crew lineup meetings, Jumbo activities, Bolt, Screen, Boring, Rapid development mining experience, Shift Supervisor',\n",
       " 'Data Analysis, Data Visualization, IPEDS, Surveys, SQL, R, Python, PowerBI, Tableau, Qlik, Data Administration, Statistical Programming, Data Manipulation, Data Management, Data Maintenance, Data Storage, Data Retrieval, Data Entry, Data Processing, Relational Databases, Data Extraction, Business Intelligence, Higher Education, Time Management, Customer Service, Analytical Skills, Organizational Skills, Oral Communication, Written Communication',\n",
       " \"Clinical Trial Data Analysis, Data Extraction, Data Visualization, Analytics, Reporting, Communication, Problem Solving, Critical Thinking, Teamwork, Customer Service, Microsoft Excel, English Proficiency, Bachelor's Degree in Scientific Field, 35 Years of Experience in Clinical Trial Data Analysis, Strong SelfOrganization Skills, Robust SelfOrganization, Ability to Manage Conflicting Priorities, Advanced Level Knowledge in Excel\",\n",
       " 'Biological Science, Medical Technologist (ASCP), Medical Technologist (AMT), MLS (ASCP or ASCPInternational), CLS (ASCP), MLT (ASCP), CLA (ASCP), Blood Banking, Chemistry, Hematology, Microbiology, Immunology, Urinalysis/Body Fluids, TJC, FDA, CAP, AABB, CMS, HCFA, CLIA, PI process',\n",
       " 'Azure Data Architect, Snowflake, Data and Analytics, AWS, GCP, Azure, Presales, Communication, Presentation, Data Strategy, Data Model Design, Database Development, Data Warehousing, Data Analytics, Data Management, Hadoop, Big Data',\n",
       " 'Machine Learning (ML), Artificial Intelligence (AI), Python, Data Gathering, Data Quality, System Architecture, Coding Best Practices, Lean Development, Agile Development, Data Visualization, Tableau, SQL, Hive, Spark, Scala, HDFS, Applied Statistics, Distributions, Statistical Testing, Regression, Cognitive Services, AWS, GCP, Azure, IBM Watson, Azure Chatbot, Google DialogFlow, Alexa, RASA, Amazon Lex, Perception, Computer Vision, Time Series Data, Text Analysis, Large Language Models, Generative AI, Deep Learning, CNNs, RNNs, LSTMs',\n",
       " 'Data Science Consulting, Data Analytics, Data Visualization, Business Intelligence (BI), Predictive Analytics, Machine Learning, Artificial Intelligence, Data Wrangling, Data Querying, Data Assets, Technical Solutions, Tableau, SQL, R, Python, Tableau Prep Builder, Tableau Desktop Certification, Tableau Server Certification, MS Forms, Power Automate, Statistical Modelling, Data Science (MS/MA), Data Analytics (MS/MA), Statistics, Mathematics, Operations Research, Computer Science, Information Systems, Engineering, Economics, Polygraph',\n",
       " \"Data Architect, Data Strategy, Data Migration, Data Architecture, Data Governance, Data Management, Data Modelling, Data Quality Assessment, Enterprise Architecture, Business Strategies, Data Flows, Data Transformation, Data Structures, Data Extraction, Data Platform Concepts, Data Integration, Data Analytics, Artificial Intelligence, Machine Learning, Data Science, Agile Mindset, TOGAF Certification, Stakeholder Engagement, Communication Skills, ProblemSolving Skills, Analytical Skills, Conceptual Skills, Reporting Skills, Bachelor's Degree in Computer Science, Computer Engineering, Information Systems, Salary Sacrificing, Salary Continuance Insurance, Professional Development, Training Courses, Webinars, Professional Coaching, External Education Support, Hybrid/Flexible Working Arrangements, Volunteer Day, Additional Leave Options, Employee Assistance Program, Collaborative Culture\",\n",
       " 'Medical Laboratory Technician, Compassion, Attention to detail, Acute care experience, Certifications, Degree, Lifetime opportunities, Career development, Nationwide opportunities',\n",
       " 'Senior Data Engineer, Data Migration, ETL, SQL, AWS, Python, Java, Node.js, AWS DMS, Glue, Athena, AWS Aurora PostgreSQL, Data Quality, Data Deduplication, Data Validation, Data Audit, Reconciliation, Exception Reporting, Public Cloud, Oracle, PostgreSQL, NoSQL, DynamoDB, Mongo, Cassandra, Realtime Data, Streaming Applications, Agile Software Development, DevSecOps',\n",
       " \"Contract Management, Data Analysis, MMIS System Maintenance, Item Master Management, PAR Management, Template Location Management, Supplier File Management, Product Building, Product Switching, Product Inactivation, GPO Contract Management, Local Contract Management, Distributor Contract Management, Manufacturer Contract Management, Vendor Contract Management, GPO Catalog Management, Contract Review, Contract Renewal, Contract Database Management, Contract Analytic Tools, Usage History Analysis, Contract Tier Pricing, Contract Pricing Uploads, Contract Discrepancy Resolution, GPO Tracking Reports, Usage Information Spreadsheets, Impact Analysis, Validation, Report Creation, Query Creation, MMIS Data Analysis, Personalized Reports, Supply Chain IT Support, Bachelor's Degree in Business or Healthcare Management, Associate Degree in Business or Healthcare Management\",\n",
       " 'Machine Learning, Protein Generation, Computational Biology, Deep Learning, Generative Biology, Protein Modeling, Python, R, Numpy/Scipy, Probabilistic Machine Learning, Deep Generative Models, Autoregressive Models, Variational Autoencoders, Normalizing Flows, Generative Adversarial Networks, EnergyBased Models',\n",
       " 'ESRI ArcGIS, Microsoft 365, Microsoft Azure, SQL, GIS, Azure, Background in Environmental Services, Data Management, Web Technologies, Budgeting, Roadmapping, Product Management, Service Owner, Technical Leadership, BSc in Computer Science, BSc in Engineering',\n",
       " 'Machine Learning (ML) Ops, ML Systems, Complex Workflows, Workflow Orchestration, Monitoring, A/B Testing, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Software Development, Design, Evaluation, Experimentation, Ownership, Reporting, Culture Building, Practice Setting, Team Building, LongTerm Thinking, Iterative Development, Production ML Systems, Systems Ownership, Scrappiness, Autonomy, Flexibility, TeamFirst Mentality, Mentoring, Passion for Vision, NYCBased, AllIn Commitment, Hiring Knack, Financial Workflows Experience, Product Ownership Experience',\n",
       " 'Medical Technologist, Medical Laboratory Technician, Blood Bank, Clinical laboratory tests, Equipment operation and maintenance, ASCP certification, Highvolume lab experience, Day or night shift availability, New grads with clinical experience, Hourly range of $23.50$38.90, Shift differentials, Sign on bonus, Relocation assistance, Stateoftheart equipment, Fastpaced lab system',\n",
       " 'Computer operations, Systems administration, Network administration, Database administration, Applications analysis, Applications development, Information security, Hardware, Software, Active Directory, Local Area Network, VMWARE, Backup systems, IIS web application server, Databases, Data dictionaries, Repositories, Computer science, Management information systems, IT Professional II, Background investigation, Specialized certification',\n",
       " 'Data analytics, Robotics, AI, Cognitive technologies, Analytical capabilities, Statistical programming, Data visualization, Machine learning, SQL, NoSQL, Data extraction, Transformation, Loading, Natural language processing, Optimization, Simulation, Strategic communication, Microsoft Azure, Agile project management, Cloud computing, Storytelling',\n",
       " 'Data Architect, Azure, Azure Data Lake, Databricks, Matillion, Snowflake, PowerBI, Eventdriven architecture, Analytical system data architecture, ETL design, Dimensional data modeling, Enterprise data model maintenance, Metadata management, Information security, GDPR, TSQL, Data science principles, Agile methodologies, Scrum',\n",
       " \"Database development, Data analysis, Data visualization, Data analytics, R code development, Statistical analyses, Chemical forensics evaluations, Quality assurance/quality control, Health sciences modeling, Custom application development, Project Management, Environmental Chemistry, Chemical fate and transport, Statistical programs (R), Code/script development, Evaluation of environmental ecological or health data, Statistical and spatial analyses, Report preparation, Written communications, Evaluation of technical documents, Consulting assignments management, Multidisciplinary teams work, Information analysis and communication, Agencies organizations and individuals cooperation, Project management, Mentoring and developing staff, Collaboration with colleagues, Critical thinking, Communication skills, Organization skills, Willingness to travel, Bachelor's degree (ecological environmental), Data science emphasis, Master's or PhD (preferred), 710 years of experience, Experience in environmental chemistry chemical fate and transport, Experience with statistical programs (R), Flexible work arrangements, Generous paid time off, Health and retirement benefits, Investment in development\",\n",
       " 'Data Management, Public Health, Microsoft Office Suite (Word Excel PowerPoint Outlook), Data Analysis, Collaboration, Sentry database, SDWIS database, Microsoft Teams, SharePoint, Bluebeam, Trello, Box.com, DD214, NGB 22, Veterans Preference',\n",
       " 'Artificial Intelligence, Natural Language Understanding, Machine Learning, Automated Speech Recognition, Conversational Systems, Large Scale Production Systems, Software Development, Algorithms, Cloud Solutions, Proof of Concept, Programming, Algorithms, Intellectual Challenges, Teamwork, Collaboration, Initiative, Ownership, Technical Decision Making, Machine Learning Fundamentals, Regression, Classification, Ranking, Neural Networks, Sequence Models, Data Systems, Spark, EMR, S3, Airflow, Big Data, Java, Python, C++, NLP, ASR, Computer Science, MS in Computer Science, Ph.D in Computer Science, Problem Solving, Pragmatism, Innovation',\n",
       " 'Network Engineering, Network Security, Routing and Switching, LAN/WAN/Datacenter Solutions, IP Routing Protocols (BGP OSPF RIP), QoS Configurations, Cisco and Arista Routers/Switches, Layer 2 Switching Protocols (Spanning Tree Trunking Etherchannel), HSRP CBWFQ DSCP NAT/SNAT TCP/IP Multicast Ethernet EVPN MLAG CVP, DNS/Domain Services, Python, Ansible, GITHUB, Change Control Processes  Service Now, Flexibility, Ability to travel, Hybrid work schedule',\n",
       " \"Data Architect, Team Lead, Postgres, Database Structural Design, Development, Metadata, Data Standards, Data Definitions, HTML, XML, Data Modeling, Data Stewards, Logical Database Structures, Classification Schema, Bachelor's Degree, Computer Science, Information Systems, Engineering, Mathematics, Scientific Discipline, Technical Discipline, TS/SCI, FSP, U.S Security Clearance, U.S Citizenship, Affirmative Action, Equal Opportunity Employer\",\n",
       " 'Data Analytics, Software Metrics, Integrated Functional Capability, DevOps, Agile, Lean, Architecture, Model Based Engineering, Microsoft Office, Key Performance Indicators, DoD Secret clearance, Team leadership',\n",
       " 'Python, AWS, Apache Airflow, Snowflake, Hadoop, Spark, Postgres, AngularJS, NoSQL, Scala, UNIX, Linux, MapReduce, Hive, EMR, Kafka, MySQL, Redshift, Agile, Data pipeline frameworks, Data APIs, Analytical models, Data warehousing, Data streaming, Distributed data tools, Cloud computing, Big data technologies',\n",
       " 'Agile engineering, Application development, Big data, Cloud computing, Data engineering, Data mining, Data science, Data visualization, Distributed data/computing tools, Hadoop, Java, Kafka, Machine learning, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, Spark, SQL, UNIX/Linux, Unit testing',\n",
       " 'Data Analytics, BI Reporting, Data Visualization, A/B Testing, Product Strategy, Communication, Data Quality, Data Analysis, Product Development, Business Development, Teamwork, Collaboration, Engineering, Computer Science, Mathematics, Startup, Entrepreneurship, Equal Opportunity, Inclusion, Diversity',\n",
       " \"DB2 for z/OS, DB2 RACF, DB2 utilities, IBM DB2 database software, REBIND, Sysplex operation, Coupling Facility, Data sharing group standing operating procedures, Top Secret Clearance, Bachelor's degree in Science Technology Engineering or Math, Government and DOD contract work, DOD Cyber Security requirements, Authority to Operate (ATO) preparation, Security audits\",\n",
       " 'Rivian, Electric Adventure Vehicles, Insystem architecture, SOC components, SOC compute, Hardware accelerators, Neural networks frameworks, ML Architectures, Compiler backend, Deep Neural Network (DNN) architectures, Stateoftheart DNN architectures, Objectoriented design, Automotive requirements, Autonomous vehicle requirements, Deep learning techniques, Contract manufacturing, Supply chains, Highvolume products, Aggressive design environments',\n",
       " \"Database Analysis, Database Design, Data Decomposition, Data Extraction, SQL, Data Mining, Data Modeling, Data Management, Data Visualization, Data Warehousing, Research, Statistics, Clinical Informatics, Survey Design, Veteran Health, Health Care, Outcomes Analysis, Project Management, Manuscript Preparation, Presentation Materials, Training, Supervision, Communication, Problem Solving, Critical Thinking, Analytical Skills, Attention to Detail, Bachelor's Degree, 46 Years of Experience, Health Sciences Research, Survey Research, Database Research, Engineering, Math, Computer Science\",\n",
       " \"Power engineering, Machine learning, Data science, Power systems modeling, Simulation, Analysis, Anomaly detection, Classification, Pattern recognition, Unsupervised learning, Supervised learning, Deep reinforcement learning, Time series data processing, Programming languages, Data analytics, Visualization, Algorithm development, Collaboration, Presentation, Documentation, Leadership, Complex technical information, Electrical engineering, Computer science, Utilities, Department of Energy (DOE), U.S. DOE Qlevel security clearance, U.S. citizenship, Master's degree, Ph.D., Research\",\n",
       " 'Public health, Epidemiology, Surveillance, Data collection, Case investigation, HIV, AIDS, STIs, Viral hepatitis, Data analysis, Research, Evaluation, Training, Technical assistance, Community engagement, Social marketing, Policy advocacy, Racial equity, Social justice, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Statistical software, Database management, GIS, SAS, SPSS, R, Python',\n",
       " 'Data governance, Data engineering design patterns, Cloud architecture patterns, Data ingestion, Scripting, Data visualization, Agile, Budget management, Python, SQL, Looker, Google Cloud, Computer Science, Statistics, Mathematics, Program management, Data science, Software engineering, Cloudbased AI deployments, Datadriven decisions, Crossfunctional collaboration, Analytics, Problemsolving',\n",
       " 'Data Center Equipment Supply Planning, Inventory Planning, Supply Chain Management, Problem Solving, Metrics Tracking, Process Improvement, Scalability, Communication, Collaboration, Project Management, Material Requirements Planning, Spreadsheets, Database Management, Analytics, SQL, Big Query, Supply Chain Certification, Financials, Cost Accounting, S&OP, ERP Systems, PLM Systems, New Product Launches, Transitions',\n",
       " 'R, Python, SAS, Stata, MATLAB, SQL, Writing queries, Scripts, Data Analysis, Data validation, Campaign optimization, Data gathering, Automated reports, Dashboards, Statistics, Experimental design, Hypothesis testing, Statistical analysis, Regression, Timeseries analysis, Problemsolving, Communication, Statistics, Experimental design, Hypothesis testing, Statistical analysis, Regression, Timeseries analysis, Product questions, Data pulling, Statistical measurement, Analytical 8020 assessment, Machine learning',\n",
       " 'Oracle PL/SQL, SQL Server, Database Development, Written Communications, Oral Communications, Data Integration, PL/SQL Coding, Data Sets Conversion, System Enhancements, Reports Production, Transaction Editing, Requirements Analysis, Data Modeling, Data Integration Tools, IBM DataStage, Web Services, Messaging, Data Warehousing',\n",
       " 'Algorithms, Software application capabilities, Data analysis, Machine learning algorithms, Heuristics, Technical knowledge of piston engine architecture, Statistics, Analytics, Python, R, Design for Six Sigma, Oral communication, Written communication, Interpersonal skills, Project management, Reporting',\n",
       " \"Oracle Database, Microsoft SQL Server, IBM Maximo, IBM WebSphere, DoD 8570/8140: IAT II (i.e. Security +), Oracle DBA certification, Microsoft Office, Bachelor's Degree in Computer Science or similar discipline, 5+ years of experience as an Oracle Database Administrator, 5+ years of database backup and recovery procedures/systems, Strong application development understanding, DoD Database Security technical implementation, DoD 8570/8140: IAT II (i.e. Security +), Secret US Government Clearance, US Citizenship\",\n",
       " 'Data Analytics, Financial System Innovations, Data maintenance, Cognos, Banner, Microsoft Excel, SQL, Data integrity, Banner Finance, Data security, Dashboard, Physical inventory data, Business model, Business initiatives, Adhoc projects, Statistical reports, Validation tables, Rules, Forms, Banner control reports, Policy and Procedure Manual, Finance website, Expense coding',\n",
       " 'Computer Networks, Systems Engineering, Data Engineering, Cloud Architecture, Data Integrity Analysis, Data Pipelines, RESTful API, Data Management, Data Architect, Data Governance, Data Dictionaries, Data Access Controls, Security Clearance: TS/SCI with CI poly, STEM education, 10+ years experience, Systems Architect, Data Structures, Data Integration, Data Engineering concepts, Data Integrity analysis, Data Ingestion, Data Access via API, Data Pipelines',\n",
       " 'Technical Architecture, Consulting, Databricks, AWS/Azure, Data Warehouse, Delta Lake, Analytical Data Lake, MLOPS Data Pipeline, AIML Model Development, AIML Model Training, AIML Model Implementation, Banking Domain Expertise, Data Classification, Data Profiling, Python, PySpark, SQL, Offshore/Onsite Coordination, Business Needs Translation',\n",
       " 'AML (AntiMoney Laundering), BSA (Bank Secrecy Act), SAR (Suspicious Activity Report), Enhanced Due Diligence, Risk Scoring, Financial Crimes Investigations, Trend Analysis, Data Analysis, Transaction Monitoring, Reporting, Microsoft Office Suite, Internet Navigation, Spreadsheet Applications, Database Reporting Applications, Communication Skills, Writing Skills, Analytical Skills, MultiTasking, DetailOriented, SelfStarter, Teamwork, Fundamental Banking Principles, General Banking Operations',\n",
       " 'Data Engineering, Python, AWS, Airflow, Snowflake, Hadoop, Streaming Data Hub, Data APIs, Data Delivery Services, Scalable Solutions, On Premise Production Environment, Cloud Production Environment, Scala, Spark, Postgres, Angular JS, NoSQL, Agile, SQL, Java, Big Data Technologies, Cloud Computing, MapReduce, Hive, EMR, Kafka, Gurobi, MySQL, RealTime Data, Streaming Applications, Data Warehousing, UNIX/Linux, Shell Scripting, People Management Experience',\n",
       " 'Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Cloud computing, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX/Linux, Agile engineering practices, Machine learning, Distributed microservices',\n",
       " \"Machine learning model development, Data preparation, Feature engineering, Algorithm selection, Model evaluation, Deployment, Monitoring and maintenance, Bachelor's degree in computer science machine learning artificial intelligence, 5+ years of experience in machine learning model development and deployment, Proficiency in Python, Experience with machine learning libraries like TensorFlow and PyTorch, Deep learning frameworks and architectures, Data manipulation preprocessing and feature engineering, Model deployment using containerization and orchestration tools like Docker and Kubernetes\",\n",
       " 'Medical Laboratory Science, Clinical Laboratory Standards, Medical Laboratory Equipment, Specimen Collection, Computer Medical Records, International Air Transport Association (IATA) Regulations, Water Testing Methods and Standards, Performing Laboratory Tests, Collecting Specimens, Analyzing, Observing, Interviewing, Controlling Laboratory Documents, Supervising, Overseeing, Managing Laboratories, Training, Organizing, Multitasking, Working with others of diverse backgrounds, Managing Resources, Budgeting, Contracting, Using Computers, Reading Comprehension, Active Listening, Active Learning, Oral and Written Communication, Public Speaking, Complex Problem Identification, Critical Thinking, Social Perceptiveness, Interpersonal Skills, Persuasion, Service Orientation, Monitoring, Coordination, Process Operation and Control, Equipment Selection and Installation, Compassionate, Selfsufficient, Independent, Integrated with the department, Interest in continuous learning and change, Attention to detail, Dependable, Tolerance of travel, Worldwide available, Tolerance of living away from family, Tolerance of working and living in difficult and/or isolated or highthreat locations, Foreign Language Skills, Superior Oral and Written Communication Skills, Vision, Speaking, Mobility, Dexterity, Physical Endurance, Mental Endurance, Hearing, Microsoft Office',\n",
       " 'Clinical Research, Case Report Forms (CRFs), Data Management, Adverse Event Reporting, Clinical Trials Management System, Data Queries, Protocol Compliance, Laboratory Techniques, Radiology Imaging, Medical Records, Patient PreScreening, Screening, Data Analysis, Statistics, Microsoft Office Suite, Research Ethics, Clinical Research Coordinator, Allied Health, Biological Sciences, Chemistry, Economics, Mathematics, Psychology, Sociology, Demography, Geography, Anthropology, Data Science',\n",
       " 'Data Analytics, Data Architecture, High Throughput Data Processing, Big Data Platforms, Data Warehousing, ETL, Data Integration, Data Consumption, Data Pipelines, Dimensional Databases, Data Acquisition, Data Transformation, Data Processing, Data Actionability, Database Structures, Data Recovery, Data Implementation, Prototyping, Proof of Concepts, Solution Architectures, Data Performance Optimization, Technical Guidance, Data Engineering, Data Administration',\n",
       " 'ML Ops, Machine Learning, Workflow orchestration, Data engineering, ETL pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Autonomy, Flexibility, Teamfirst mentality, Passion for vision, Hiring skills, Financial workflow experience, Product ownership, NYCbased, Inperson work environment, Longterm thinking, Smart abstractions, Shipping fast, Iterative development',\n",
       " 'Machine Learning Operations, Workflow Orchestration, Monitoring and Visibility, Experimentation and A/B Testing, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Autonomy, Adaptability, Techincal Leadership, Mentoring, Passion, Ownership of EndtoEnd ML systems, Programming Languages (not mentioned), Software Frameworks (not mentioned), Software (not mentioned), NYCbased, InPerson Environment',\n",
       " 'Data Visualization, DataDriven Design, Agile Development, UserCentered Design, UI Design, Visual Storytelling, Data Analysis, Descriptive Analysis, Diagnostic Analysis, Predictive Analysis, Prescriptive Analysis, Communication, Creative Direction, Brand Guidelines, Adobe Creative Suite, Photoshop, Illustrator, InDesign, Microsoft Office, PowerPoint, Word, HTML, Video Editing, After Effect, Premiere Pro, Prototyping, Adobe XD, Tableau, Ceros, Adobe Workfront, Jira, Arts, Graphic Design, User Interface Design, Digital Design, Portfolio, Organizational Design, Process Modeling, Deck Building, Presentation Skills, PowerPoint Designs, Attention to Detail, Time Management, Interpersonal Communication',\n",
       " \"Computer Logic, Objectoriented Programming, C++, C#, Java, Python, Classroom teaching, Online teaching, Graduate degree in CIS/MIS/CS/science/math/engineering, Bachelor's degree in CIS MIS computer science business math science or engineering, 3 years of relevant professional programming experience, 1 year of professional experience with an objectoriented language, Prior teaching experience (preferred), Resume, Cover letter, Official transcripts, Criminal background check, Semester by semester adjunct pay, Tuition waivers, Sick and personal business leave, Comprehensive Adjunct Faculty Development Program\",\n",
       " 'Material Master, Bill of Materials, Recipes, Resources, PurchasingInfo Records, Warehousing data, SAP (ECC and APO), Manufacturing Execution System (PMX), Extended Warehouse Management System (EWM), Standard Operating Procedures, GMP/pharmaceutical/manufacturing environment, SAP Master Data, Batch Management, Quality Assurance, Supply Chain Management, Project Management, Communication, Collaboration, SAP ECC, SAP APO, SAP PMX, SAP EWM, Master Data Management, Power User forums',\n",
       " 'Data Processing, Inspection, Performance Management, Career Development, Offshore Personnel Development, Mentoring, Coaching, Training, Offshore Operations, Marine Asset Service Lines, Onboarding, Induction, Guidance, Competency Assessment, Training Resources, Training Matrix, Supply Chain, Technological Developments, HSSEQ, Incident Reports, Interview Support, Field Staff Management, Offshore Personnel Management Team, Hybrid Work Environment, Strategic Thinking, Survey Sector, Industry Qualifications, Offshore Field Working, Communication Skills, HSSEQ Knowledge',\n",
       " \"Antimoney laundering, CAMS designation, Business development, Coaching, Compliance, Consulting, Controls, Financial reporting, Leadership, People management, Regulatory reporting, Risk management, Team leadership, Bachelor's degree, Experience (46 years), Master's degree (asset)\",\n",
       " 'Data Center Design, Construction, Operations, TIA942, Uptime Institute Tier Standards, Electrical Systems, Cooling Systems, Fire Suppression Systems, Security Systems, Commissioning, Capacity Planning, Change Management, Incident Response, DCIM, BMS, PE, CDCP, CDCS',\n",
       " 'Clinical data management, Patient data accuracy, Clinical research trials, Data capture/retrieval systems, Health care delivery systems, Data abstraction, Data organization, Data presentation, Data publication, Federal/state/local regulatory requirements, Institutional operational research objectives, Ethical obligations, Microsoft Word, Microsoft Excel, Spreadsheet management, PDF file creation/use, Online database use/maintenance, Internetbased research, Network & online file maintenance & management, Custom report generation, Medical terminology, Complex protocols, Subject records, Healthcare based clinical information systems, Health research, Healthcarerelated field',\n",
       " \"Master Data Entry, ERP System, Item Master/Item Branch integrity, Bill of Materials Structure, Maintenance, Planning Parameters, Data Integrity monitoring and revision, Standard Operating Procedures (SOPs), MRPrun, Forecast/MPS upload, Planning schedule, Reporting, Analysis, Supply Chain organization, Global Master Data Standard, Item Master Maintenance, Item Branch plants, Item Master SetUp, Branch Plant specific fields, Master Data items, Obsolete raw materials, Item Branch Master Raw Material set up review, Master Data accuracy/integrity measurement, Bill of Material (BOM) structure, New BOM structure, Standard Branch Plant requirements, BOM's, Obsolete BOM's, Item Branch Master BOM review, MRP system, Forecast/MPS upload, Planning schedule, Sales, Inventory levels, Master Data, SOX/GMP requirements, Key metrics, Supply Chain, Department Manager, Longterm growth strategies, Tactical initiatives, Quality, Regulatory, Manufacturing, Finance, Production Planners, Material Planners, Analytics, Reports, Dashboard visualizations, Written and oral English, Supply Chain operations, Material Management, Inventory Control, Product Flow, High School Diploma or GED, Bachelor's degree in science engineering business operations Supply Chain or other related technical field, JD Edwards enterprise system, Healthcare Medical Dental and Vision, Financial Planning & Stability 401(k), Health & Wellness programs, Vacation, Community Outreach Programs, Family Planning Support, Professional training and development opportunities, Tuition reimbursement\",\n",
       " \"Python, Statistical Modelling, Master's Degree in Statistics, Pre Deployment Model Evaluation, Post Deployment Experiments, RoI for Model, AB Testing, Campaign Assessment, Communication Skills, Team Coaching in Statistics, Sequential Testing, Block Design, Uplift Modelling, Contextual Bandits\",\n",
       " \"Advanced analytics, Criminal justice data analysis, Crime analysis, Tableau, Power BI, ArcGIS, SPSS, R, Stata, Python, Data visualization, Statistical techniques, Data mining, Law enforcement databases, Record Management Systems, ComputerAided Dispatch, Analytic products, Analytic tools, Member of academic or professional association, Background investigation, Driver's license, Hybrid position, Comprehensive benefits package, Salary range: $47.85  $71.80 per hour, Fulltime (1.0 FTE) regular position, FLSA exempt (not eligible for overtime), Discretionary pay band, Online application at http://www.seattle.gov/jobs, Resume and cover letter required, Incomplete applications will not be considered\",\n",
       " 'Cybersecurity, Data analysis, System audit, General control reviews, Integrated audits, Risk assessment, Risk management, Compliance with regulations, IT contract compliance, Agency policies and procedures, System data analyses, Data integrity testing, Documenting and presenting test assessment results, System control audits, Researching analyzing and evaluating risks, Controls relevant to cybersecurity, Risk reduction recommendations, Project lessons learned, Performance improvement opportunities, Conducting internal reviews, Department’s general system controls, Access security, Change management, System development life cycle, Disaster recovery, Data center operations, HIPAA Security and Privacy rules, General and application control issues, Implementation of corrective actions, Preparing and maintaining complete work paper documentation, Memorandums, Letters, External audits, Liaison between the Comptroller’s Office, Third party auditors, Audits and reviews, Division and bureau being audited, Selfimprovement, Education, Certification, Training, Keeping abreast of current and emerging technologies, IT risk management, Audit concepts and methods, Microsoft Office Suite, Word, Excel, PowerPoint, Access, ACL, Commonly used operation systems, Databases, Network structures, ISO 2700X, COBIT 5, NIST, Security+, CISSP, CISA, CISM',\n",
       " 'Image understanding, Computer vision, Machine learning, Object selection, Matting, Inpainting, Color adjustment, Fast, Accurate, Interpretable, Infrastructure as Code, Terraform, Cloud computing, DevOps, MLOps, Software engineering, Problem solving, Communication, Collaboration, Creativity, Photography, Video editing, VFX, Artistic expression, Product design, User experience, Research, Data science, Mathematics, Statistics',\n",
       " \"Data Science, Machine Learning, Applied Mathematics, Statistics, Computer Science, Big Data Analysis, Data Mining, Data Visualization, Programming Languages, Hadoop, Spark, Python, R, SQL, NoSQL, Data Structures, Algorithms, Software Engineering, Software Development, Software Testing, Data Modeling, Databases, Cloud Computing, Data Security, Data Governance, TS/SCI Clearance with Polygraph, Bachelor's Degree in Mathematics Applied Mathematics Statistics Applied Statistics Data Science Operations Research or Computer Science, Master's Degree in Mathematics Applied Mathematics Statistics Applied Statistics Data Science Operations Research or Computer Science, Doctoral Degree in Mathematics Applied Mathematics Statistics Applied Statistics Data Science Operations Research or Computer Science, Active TS/SCI Clearance with Polygraph, Ability/willingness to work fulltime onsite in secure government workspaces\",\n",
       " 'Machine Learning, Data Science, Python, Tensorflow, PyTorch, Jupyter, Deep Learning, NLP, Model Optimization, Production Deployment, Cloud Computing, GCP, AWS, Azure, Experiment Tracking, MLFlow, WandB, Agile Methodologies, Communication, Leadership, Teamwork, Problem Solving, Analytical Skills, Research Skills, Speech Processing, Audio Classification, FDA Approvals, Medical Device Software',\n",
       " 'Medical Laboratory Scientist, Medical Lab Technicians (MLT), High Complexity Testing, Moderately Complex Tests, Waived Tests, Quality Control, Quality Assurance, Proficiency Testing, Instrumentation, Methodology, Equipment Evaluation, Selection, Implementation, Patient Confidentiality, Biohazard, Chemical, Sharps Safety, Compliance, Communication, Dependability, Reliability, Independent Judgment, Bachelor of Science Degree, CAHEA Accredited Medical Technology Program, ASCP Categorical Certification, Clinical Laboratory Science, Medical Terminology, Laboratory Information Systems, Associate Degree, Laboratory Science, Licensure, Certifications, Clearances, New York License, ASCP Certifications Preferred, Equal Opportunity Employer',\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Cloudbased data warehousing services, UNIX/Linux, Shell scripting, Agile engineering practices, Data warehousing, NoSQL implementation, Distributed data/computing tools, Realtime data, Streaming applications, Cloud computing (AWS Microsoft Azure Google Cloud), Big data technologies, Application development, Unit tests',\n",
       " 'CAD Data Engineering, PLM Systems, CAD Data Management, Data Exchange, Data Quality Assurance, Data Standards, Data Integration, Data Modeling, Data Governance, Data Lifecycle Management, System Optimization, Troubleshooting, Data Analysis, Data Reporting, EndUser Training, Computer Science Degree, Engineering Degree, Automotive Background, Manufacturing Background, PLM Systems Experience, CAD Data Formats, CAD Standards, Data Management Best Practices, Data Modeling, Data Integration, Data Quality, Data Governance, Analytical Thinking, Problem Solving, Written Communication, Verbal Communication',\n",
       " 'ML Ops, ML Systems, Workflow Orchestration, Monitoring, Visibility, Experimentation, A/B Testing, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Product Ownership, Engineering Leadership, Hiring, Technical Authority, Autonomy, Flexibility, Teamfirst Mentality, Mentoring, NYCbased, Inperson Environment, Passion for Vision, Tech for Accounting Finance and Economy, Generous Compensation',\n",
       " \"Machine Learning, Generative AI, Risk Management, Compliance, Regulatory, Operations, Cybersecurity, Process Management, Project Management, Cloud Risk Management, Business Process Management, Lean, Green Belt Certification, Change Management, Knowledge of PRIME system, Risk Governance, SQL, Python, Data Visualization, Data Analysis, Communication, Collaboration, Problem Solving, Critical Thinking, Attention to Detail, Bachelor's Degree, 3+ years of experience in Process Management Project Management Risk Management or Cloud Risk Management, 3+ years of experience in Compliance Legal Regulatory or Operations, 2+ years of experience in Technology or Cybersecurity, Business Process Management certification Lean or Green Belt Certification, Risk Certifications (CRISC CISM CRCM CIPP ABA Risk Mgmt Certification)\",\n",
       " \"Data Engineering, Machine Learning, Distributed Microservices, Java, Scala, Python, RDBMS, NoSQL, Cloudbased Data Warehousing, AWS, Microsoft Azure, Google Cloud, Agile, Hadoop, Spark, Kafka, MongoDB, Cassandra, Redshift, Snowflake, UNIX/Linux, Agile Engineering Practices, Bachelor's Degree, 6+ years of application development experience, 2+ years of big data technologies experience, 1+ year of cloud computing experience, 2+ years of people management experience, 7+ years of experience in application development with Python SQL Scala or Java, 4+ years of experience with a public cloud, 4+ years of experience with Distributed data/computing tools, 4+ years of experience working on realtime data and streaming applications, 4+ years of experience with NoSQL implementation, 4+ years of experience with data warehousing, 4+ years of experience with UNIX/Linux, 2+ years of experience with Agile engineering practices\",\n",
       " 'Data Engineer, ETL pipelines, Cloud computing, BigQuery, Dataflow, Data Proc, Data Fusion, Cloud Composer, GSUTIL, GCS, Kafka, Pub/Sub, Data Catalog/Dataplex, Python, Unix, Linux, Relational database management systems, Teradata, Oracle, SQL Server, SQL, BTEQs, Stored procedures, Unstructured Data, Agile application development, DevOps, CI/CD pipelines, Git, Problemsolving, Verbal communication, Written communication, Interpersonal skills, Healthcare Domain, Patient Data, GCP Cloud Professional Data Engineer, Prolonged sitting, Standing, Computer workstation, Mouse, Keyboard, Monitor',\n",
       " 'Data architecture, Data computation, Data governance, Data management, Data processing, Data replication, Data services, Data storage, System administration, Data science, Scala, Java, Python, Bash, Hadoop, Spark, SQL, Linux',\n",
       " 'CaseWorthy, SQL, Microsoft Office, Excel, Database Structure, Reporting, Data Extraction, Documentation, Client Management, Administrative tasks',\n",
       " nan,\n",
       " 'Ontology, Data Modeling, Data Standardization, Semantics, RDF, RDFS, OWL, SKOS, SHACL, JSON, XML, SPARQL, Python, R, JSON, OpenAPI/YAML, AVRO, Agile principles processes and methodologies, Amazon Web Services, SQL, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, Graph databases, Machine Learning, NLP, Linked Open Data',\n",
       " 'Machine Learning, Deep Learning, NLP, Computer Vision, Audio, Data Mining, PyTorch, TensorFlow, Python, Data Structures, Algorithms, Java, KDD, IJCAI, WWW, WSDM, ICML, NeurIPS, CVPR, ECCV, ICCV, ACL',\n",
       " \"Azure SQL, Azure Cloud Environments, Blob Storage, Queue Storage, Table Storage, Files, PowerShell, Spark, Scala, Lambda, Python, Python/R, USQL, Azure Data Lake Store(ADLS), Integration Run Time(IR), Azure Data Factory(ADF), File System Data Ingestion, Relational Data Ingestion, Data Lake, Data Factory, Azure native programming, Custom programming, SQL Server Integration Services (SSIS), OLTP, Data warehouse systems, Hadoop, MongoDB, NoSQL platforms, SQL Server environments, Microsoft .NET framework, C#, Service Oriented Architecture (SOA), Web services, Agile/Scrum, K12 data warehousing, Analytics, Bachelor's degree, Computer Science, Communication skills, Teamwork, Collaboration, Bias for action, Optimal database solutions, Application requirements, Business requirements, SQL Server jobs, SSIS packages, BIDS, Performance, Security, Availability, Agile/SCRUM model\",\n",
       " 'Business Administration, Supply Chain Management, Finance, Program Management, Operations Demand Planning, Project Lifecycle Management, Strategic Leadership, Data Analysis, Problem Solving, Communication, Presentation, Project Scheduling, Risk Management, Data Extraction, Inventory Management, Portfolio Management, Forecasting, Supplier Relationship Management, Customer Relationship Management, Risk Mitigation, Resilience and Continuity, Global Perspective, CrossProduct Area Experience, CrossIndustry Experience, Affirmative Action Employer, Equal Employment Opportunity Employer, Accommodation for Applicants',\n",
       " 'Data Engineering, Machine Learning, Distributed Microservices, Full Stack Systems, Java, Scala, Python, Open Source RDBMS, NoSQL Databases, CloudBased Data Warehousing, Redshift, Snowflake, Agile Development, Unit Testing, Code Review, People Management, AWS, Microsoft Azure, Google Cloud, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Shell Scripting, SQL',\n",
       " 'VMware Virtual Machines, VMware application management servers, Backup Appliances, Hypervisor, Server Blade Chassis, Disk Arrays, Storage Switches, VMware vRealize Operations Manager, Virtual infrastructure, Data services enclaves, Installation, Configuration, Maintenance, Monitoring, Storage, Virtual Storage Systems, Virtual environments, Disaster recovery, Incident handling and reporting, Patching, System hardening, STIG application, POAM maintenance, ITIL best practices, SOPs, TTPs, SmartBooks, Data Domain, NetBackup, EMC Unity, EMC VNX, SAN hardware and software, Problemsolving skills, Decisionmaking ability, Analytical skills, Communication skills, Team skills, Initiative, Resultsoriented approach, Flexibility, Tenacity, Ability to work in a dynamic crossorganizational team environment, SECRET clearance',\n",
       " \"Data analysis, Financial analysis, Data categorization, NY State Medicaid Managed Care Operating Report, Encounter Intake System, NY State Mandated Encounter Data to Cost Report Reconciliation Supplemental E, Internal claim line assessment, Data reconciliation, Score card oversight, Encounter Data Quality Performance Measures, Encounter data submission monitoring, State liaison, Unpaid Claims Liability data collection, SQL Server Environment, Statistical systems, Data report writers, Microsoft Excel, Microsoft Access, Microsoft Word, Health Insurance financial and data experience, Bachelor's degree, 7 years of industry experience\",\n",
       " 'SQL Studio, SQL Reporting Services, Business Intelligence, Data Visualization Software, IBM Cognos, Problem Solving, Critical Thinking Skills, Relational Databases, Programing Techniques, Analytical Reports, Proficiency in Programming Languages, Hadoop, Data Mining, Java, C++, Python, R Programming, Tableau, Microsoft Excel, AWS, Git, Data Vault, DevOps, Data Integration, Machine Learning, Communication Skills',\n",
       " \"Data Loss Prevention (DLP), Cybersecurity, SaaS, IaaS, Agile frameworks, Customer engagement, Symantec Data Loss Prevention (DLP), High School Diploma GED or equivalent certification, Cybersecurity, Information technology, Symantec Data Loss Prevention (DLP), Bachelor's Degree in Cybersecurity Systems Engineering or Computer Science, Scripting, Agile delivery model, Public cloud security, Multicloud environments, JIRA, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional\",\n",
       " 'Data Engineering, Data Architecture, ETL, Data Warehousing, Team Management, Data Infrastructure, Data Pipelines, Data Lakes, Data Availability, Data Reliability, Data Security, Data Quality, Data Governance, Data Standards, Data Modeling, SQL, Python, Microsoft Technologies, SQL Server, Azure, PowerBI, Data Science, AI Modelling, Data Analytics, Business Intelligence, Project Management, Resource Allocation, Time Management, Budget Management, Documentation, Computer Science, Information Technology, ProblemSolving, Collaboration, Communication, Interpersonal Skills',\n",
       " \"Project Management, Construction Management, Civil Engineering, Industrial Construction, Technical Aspects, Administration, Design Build Projects, OSHA 30 Hour, CPR First Aid, SWPPP Training, LEED Certification, Valid Driver's License, Industrial Design Build Projects, Trust Building, Communication, Leadership, Supervision, Quality Control, Safety, Physical Strength, Mobility, Computer Skills, Bachelor's Degree\",\n",
       " 'Database Management, Team Leadership, SQL Server, DB2 (z\\\\OS and Distributed), Microsoft Office (Word Excel PowerPoint), Communication Skills, Interpersonal Skills, Prioritization and Initiative Management, Analytical and ProblemSolving Skills, Decision Making, Relational Database Systems and Technologies, Data Engineering, Customer Support, Staff Administration and Management, Performance Appraisals, Coaching and Counseling, Resource Planning, Training and Feedback, Project Management, Vendor Relations, ITIL Processes, Process Improvement, Technology Evaluation and Recommendation, Operational and Tactical Planning',\n",
       " 'Product Management, Data Product Strategy, Data Product Development, Global GotoMarket, Product Launch and Adoption, CrossFunctional Leadership, Performance Metrics, Customer Focus, Risk Management, Data Analytics, Data Integration, Data Governance, Data Security, Strategic Thinking, ProblemSolving, Analytical Skills, Effective Communication, Leadership, Market Orientation, Customer Orientation',\n",
       " 'Backend Data Engineering, Data Analytics, Data Infrastructure, Automation, Complex ProblemSolving, Critical Thinking, Agile Development, Data Pipelines, Data Extraction, Data Transformation, Data Loading, Database Schema Design, Data Quality, Data Governance, Scalability, Reliability, Security, Python, SQL, Relational Databases, Airflow, Flask, Docker, Kubernetes, FullStack UI, Communication Skills, Collaboration, Project Management, Time Management, Organizational Skills, Strategic Thinking, Interpersonal Skills, Presentation Skills',\n",
       " 'Machine Learning Engineering, Algorithmic Product Architectures, AWS Services, Python, SQL, PySpark, Docker, Data Science, Data Engineering, Data Architecture, LLM, Feature Store, Data Cleansing, Data Imputation, InfrastructureasCode, RealTime Streaming, Offline Batch Optimizations, Data Processing Workflows, Data Governance, Data Security',\n",
       " 'ElasticSearch, Kafka, MongoDB, MySQL, Site Reliability, Datastores, Automation, Cloud provider managed services, AWS, Terraform, Kubernetes, Infrastructure as Code, Unix/Linux, Clientserver, Python, Go, Good communication and documentation skills, Datastores, Scalability, Performance, Reliability, Availability, Coding, Mentoring',\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL databases, Cloud based data warehousing services, Redshift, Snowflake, AWS, Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, Windows Server, Linux, Unix, Agile',\n",
       " \"Tableau, Business analysis, Data analysis, Data visualization, Financial analysis, Data management, Data quality, Data extraction, Data manipulation, Data compilation, Data reporting, Microsoft Excel, PowerPoint, Survey tools, Data source identification, Backend, Selfstarter, Team player, Communication skills, Organizational skills, Multitasking, Bachelor's degree, Secret clearance\",\n",
       " 'Data Protection Management, Compliance, Data Protection Measures, Data Subject Access Requests, Data Protection Impact Assessments, EU Data Representative, PECR, Register of Processing Activities, Data Map, Risk Assessments, Data Breaches, Sensitive/Higher Risk Data, Domestic/International Data Protection Laws, Data Protection Training, Data Protection Laws (UK), Data Subject Access Requests, Data Focused Policies/Procedures, Data Protection Breaches/Issues, International Data Protection Laws, Data Transfers, Degree Level Education, Legal Risk and Compliance',\n",
       " 'Python, AWS, Airflow, Snowflake, Scala, Spark, Postgres, Angular JS, NoSQL, Hadoop, Hive, EMR, Kafka, Gurobi, MySQL, Redshift, Unix/Linux, Agile practices, Data warehousing, Distributed computing, Realtime data, Streaming applications, Data pipelines, Data APIs, Analytical models, Cloud computing, People management, Mentoring, Crossteam collaboration, Leadership, Problemsolving, Systems analysis, Application development, Big data technologies, Data engineering, Datadriven solutions',\n",
       " 'ETL, Data Development, Dashboards, Reporting, Micro Strategy, Tableau, Power BI, Power Automate, Logic Apps, PowerApps, SQL Server, Stored Procedures, TSQL, SSRS, PowerBI, Power BI Report Builder',\n",
       " 'Information Systems, Data Analytics, Data Science, Accounting, Enterprise Resource Planning (ERP), Teaching, Research, Christian faith, AACSB accreditation, Undergraduate, Graduate, Master of Business Data Analytics, Master of Business Administration, Faculty, Teaching evaluations, Syllabi, Transcript, Statement of research interest, Statement of teaching history',\n",
       " 'Tableau, Socrata, GIS, Spatial mapping, R, Python, Transportation datasets, Travel Demand Modeling/Forecast, Data visualization, Data Science, Data Visualization, Statistics, Communications skills, Climate change mitigation, Adaptation policy, Alternative energy, Transportation statutes, Geography, Social Science, Behavioral Science, Computer Science, Information Systems, Math, Earth Science, Geo Science, Life Science, Physical Science, Environmental Science, Engineering, Business',\n",
       " 'Object Oriented Programming, AWS, NoSQL Datastores, Distributed SQL Query Engines, Workflow Management Tools, Secure Coding Frameworks, Source Control Management, Build Processes, Code Reviews, Testing, Data Privacy Laws and Regulations, Data Retention Policies, Data Deletion Policies, Data Use Policies, Data Exposure Policies, Data Privacy Policies and Procedures, Data Privacy Risk Assessment, Data Privacy Impact Assessment, Data Privacy Incident Response, Data Privacy Awareness Training, Data Privacy Auditing',\n",
       " 'Python, Data Analysis, Statistical Techniques, Optimization Techniques, Econometric Techniques, Metrics Reporting, Communication Skills, System Functionality Mastery, DAMA CDMP Certification, EDM DCAM Certification, Presentation Skills, Technical Training Experience, Leadership Skills, Strategy Development Skills, Data Quality Management Consulting, Data Governance, Data Modeling, Data Profiling Tools, Statistics, Optimization, Econometrics, Data Quality Management, Data Product Reports, Data Analysis Tools, Machine Learning, Artificial Intelligence, Natural Language Processing, Software Development, Web Development, Bloomberg Technologies, Data Management Domains',\n",
       " 'Bioinformatics, Cellular and molecular biology, Genomics, Proteomics, Transcriptomics, Python, R, Data integration, Omics platforms, Tissue processing, Singlecell preparation, Molecular biology, Biochemistry, In vitro culture functional assays, Pharmacology, Molecular target validation, Animal handling, In vivo modeling, Multiomics technology, Data visualisation, Patterns identification, Insights extraction, Innovative approaches, Scientific reporting, Manuscripts, Presentations, Experimental design, Data analysis, Biological and computational approaches, Preclinical experimental design, Human donor samples, Ethical guidelines, Responsible research conduct, Bachelor’s Degree, Master’s Degree, PhD, Effective communication, Organization skills, Crossfunctional teams, Project milestones, Handson experience, Shortterm incentive programs, Longterm incentive programs',\n",
       " 'Software engineering, System design, Application development, Testing, Operational stability, Spark, EMR, ETL, Java, Python, Spring, API, React, Agile methodologies, CI/CD, Applicant Resiliency, Security, Cloud, Artificial intelligence, Machine learning, Mobile, Frontend technologies, AWS Certification, Scrum Master',\n",
       " \"Hadoop, AWS, Azure, GCP, Statistical tools, Data mining tools, Supervised models, Unsupervised techniques, SQL, UNIX, BASH, Python, C++, Java, Statistical packages, Predictive analytics, Prescriptive analytics, Master's degree in mathematics, Master's degree in statistics, Master's degree in computer science, Advanced understanding of data visualization libraries, Advanced understanding of statistical methodologies for modeling and business analytics, Understanding of Variance Inflation Factor (VIF), Exploratory data analysis, Feature engineering, Predictive modeling, Actionable insights, Strategic direction, Data visualization, Data analysis, Machine learning algorithms, Feature selection algorithms, Sampling techniques, Statistical performance metrics, Programming\",\n",
       " 'Machine Learning, Voice Analysis, Mental Healthcare, Product Development, MLOps, ML Experimentation, ML Model Optimization, Speech Processing, Audio Classification, Experiment Tracking, Sprint Management, Agile Methodologies, FDA Approvals, Python, NumPy, Pandas, Numba, PyTorch, TensorFlow, Jupyter, GCP, AWS, Azure',\n",
       " 'DB2 administration, z/OS DB2 administration, Installing/migrating DB2 and IBM or third party DB2 related products, SMPE, data sharing, DR (restoring DB2 and products), Application of maintenance to DB2 and related products, TLS/SSL Secure Client server, Configuring DB2 Connect, Establishing connection to security products RACF or ACF2, Experiencing tuning DB2 systems, Ability to produce performance reports of how the sub system is performing, REXX, COBOL, BMC Control M, BMC Products',\n",
       " 'Data Integration, Informatica Powercenter, Informatica Powerexchange, Informatica MDM, Informatica Data Quality, Tableau, Knime, IBM Cognos, Windows Server, Application Monitoring, ETL Development Standards, SQL, RDBMS, SaaS, PaaS, Cloud Solutions, System Maintenance and Upgrade Procedures, IDMC (Informatica’s Intelligent Data Management Cloud Offering), Medical, Dental, Prescription, Vision, Long Term and Short Term Disability Coverage, Tuition Reimbursement, Paid Time Off and Holidays, 401(k) Plan, Defined Benefit Pension Plan, Subsidized Parking or Bus Subsidies, Free Breakfast and Lunch, Employee Discounts',\n",
       " 'Java, Scala, Python, AWS, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, NoSQL, Mongo, Cassandra, Redshift, Snowflake, UNIX/Linux, Machine learning, Microservices, Distributed computing, Data warehousing, Cloud computing, Data collection, Data storage, Data access, Data analytics, Agile engineering, Big data technologies, Cloud cost optimization, WellArchitected Framework, SQL, Data Engineering, Data Analytics, Cost Optimization, Software Development, Big Data, Cloud Computing, Machine Learning, Distributed Data/Computing, UNIX/Linux, Agile Engineering, Hadoop, HIVE, EMR, Kafka, Spark, Gurobi, MySQL, Redshift, MongoDB, Cassandra, Java, Scala, Python, RDBMS, NoSQL, Snowflake',\n",
       " 'Machine Learning, Deep Learning, NLP, LLM, LLM pretrain, LLM evaluation, Responsible LLM, LLM alignment, Recommendation algorithms, TensorFlow, PyTorch, Jax, Optimization, Sparsity, AI generation, Training infrastructure',\n",
       " 'Data Science, Machine Learning, Natural Language Processing, Generative AI, Data Engineering, Artificial Intelligence, Big Data Processing, Curriculum Development, Research, Grant Writing, Teaching, Scholarly Activities, Data Analytics, ETL Data Pipeline, NoSQL Database, Distributed/Parallel Systems, Cloud Computing, Statistical Analysis, Data Visualization, Programming, Python, R, Java, SQL, Hadoop, Spark, ScikitLearn, TensorFlow, PyTorch, Keras, Tableau, Power BI',\n",
       " 'Data Management, Data Architecture, Data Analytics, Data Science, Data Modeling, Data Integration, Data Warehousing, Data Visualization, Cyber Security, Information Assurance, Software Engineering, Systems Engineering, ModelBased Systems Engineering, Big Data, Artificial Intelligence, Machine Learning, Modeling, Simulation, Communication Skills, Complex Security Environment, Data Flows, DoD Top Secret Clearance',\n",
       " 'Senior Data Engineer, Python, SQL, Cloud infrastructure, AWS, ETL/ELT processes, Data collection and ingestion, CI/CD environments, BI tools, Data warehousing, Dimensional modeling, Star schema, NoSQL, Airflow, Data pipelines, Batch data pipelines, Real time data pipelines, Serverless applications, Workflows, A/B testing, Business insights, Strategic planning, Data integrity, Agile/Scrum environment, Backend development, Message queues, Stream processing, Terraform, dbt, Integrations, APIs, AWS Redshift',\n",
       " 'Data Analysis, Data Compilation, Reporting, Data Management, Data Import, Data Validation, Data Cleaning, Statistical Analysis, Data Visualization, Database Design, Data Security, Data Storage, Communication, Analytical Thinking, Mathematical Mindset, Microsoft Office Suite, Software Learning, Document Management',\n",
       " \"Electrical distribution, Electrical layout, Transformers, PLC's, Generators, Switchgear, UPS systems, STS', ATS', PDU's, Chilled Water Systems, CRAC/CRAH's, PreAction Sprinkler Systems, NEC, NFPA 70E, NFPA 72, NFPA 25, Fiber optics, Cabling infrastructure, Industrial safety best practices, Lockout/tag out, Arc flash protection, OSHA, Data trending, Data tracking, Data analysis, Integrated critical monitoring systems, Installing, Maintaining, Troubleshooting, Centrifugal Chillers, Cooling Towers, Heat Exchangers, Water Treatment Systems, VFD's, Pumps, HVAC equipment, Emergency Standby Diesel Generator Systems, Fuel/Oil systems, Electrical generation, Electrical distribution, Static UPS Systems, Double Interlock PreAction Systems, Leadership skills, Fiber and copper communications cabling, IT hardware, Servers, Network switches, Structured cabling, Communication skills, Microsoft Office Suite (Word Excel PowerPoint Project)\",\n",
       " \"Data Science, Machine Learning, Applied Statistics, Python, SQL, Matplotlib, Seaborn, Tableau, Data Visualization, Data Analysis, Statistical Modeling, Hypothesis Testing, Experimental Design, DataDriven Decision Making, DataDriven Insights, Technical Writing, Communication, Collaboration, Problem Solving, Attention to Detail, Analytical Thinking, Bachelor's or Master's degree in Quantitative Field, Competitive Compensation Package, Medical Dental and Vision Insurance, 401k Matching and Employee Stock Purchase Plan, Dynamic and Inclusive Work Environment, Opportunities for Career Growth, Access to CuttingEdge Technology, CuttingEdge ML Technology, EEO Employer, Equal Opportunity Employer\",\n",
       " 'Data Management, Data Analysis, Analytical Tools, Healthcare Data, UB04 Claim Format, HCFA 1500 Claim Format, Claim Analysis, Data Reporting, Communication, Teamwork, Problem Solving, Curiosity, Learning Agility, Remote Work',\n",
       " 'SAS, Clinical Trials, Clinical Data, Data Analytics, Mathematical Concepts, Statistical Concepts, Oral Communication, Written Communication, Teamwork, Technical Solution Training, Time Management, Resource Management, Accuracy, Attention to Detail, CDISC, Scientific, Technical, 21 CFR Part 11, ICH E6 (GCP), CDISC Standards, MedDRA, WHODRUG',\n",
       " 'Customer service, Observation, Reporting, Rapid response, Writing, Enforcement, High school education, Screening process, Security experience, Military experience, Law enforcement experience, State license, Computer skills, Supervisory experience',\n",
       " \"Machine Learning, Artificial Intelligence, Python, Data Science, Bachelor's Degree, Information Technology, Data Gathering, Data Quality, System Architecture, Coding Best Practices, Lean Development, Agile Development, Programming Languages, Deep Learning, Generative AI, Large Language Models, Cognitive Services, Cloud Platforms, Chatbots, Perception, Time Series Data, Big Data, Data Visualization, SQL, Hive, Spark, Scala, Statistics, Tableau, AWS, GCP, Azure, IBM Watson\",\n",
       " 'Java, Scala, Python, RDBMS, NoSQL databases, Redshift, Snowflake, Machine learning, Distributed microservices, Cloudbased data warehousing services, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Agile engineering practices, Data visualizations, Tableau',\n",
       " 'SQL, Tableau, Excel, Python, R, Data Analysis, Data Visualization, Business Intelligence, Machine Learning, Statistical Modelling, Stakeholder Management, Presentation Skills, A/B Testing, Datadriven Decision Making, Problem Solving, Automation, Communication Skills, Multitasking, Prioritization, Resource Coordination, MBA, Program Management Certifications, Asian Market Experience, Travel Industry Experience, Ecommerce Experience, Tech Experience, Consulting Experience',\n",
       " 'SQL, Python, Power BI, Data Warehouse Design, Redshift, Snowflake, Data pipelines, Data Transformation, Data integration, Data delivery, Automation, Infrastructure, Scalability',\n",
       " 'Machine Learning, Deep Learning, Object Detection, Sensor Fusion, SLAM, Unsupervised/SelfSupervised Learning, Computer Vision, Robotics, Graphics, PyTorch, TensorFlow, MXNet, C++, CUDA, Cloud Pipeline Development',\n",
       " 'Problem Solving, Data Analysis, Systems Analysis, Systems Administration, System Integration, Software Development, Requirements Gathering, User Requirements Analysis, Workflow Analysis, Project Management, Scheduling, Software Evaluation, Database Management, Data Processing, Business Analysis, Engineering Analysis, Secret Clearance, Top Secret Clearance',\n",
       " \"Data Analysis, Data Warehousing, SQL, Data Visualization, Reporting, ProblemSolving, Team Collaboration, Business Intelligence, Data Quality Management, Data Definitions, Business Rules, Performance Management, Talent Selection, Training, Business Objectives, Leadership, Insurance Industry Experience, Bachelor's Degree, Business, Finance, Accounting, Computer Science\",\n",
       " 'Machine learning, Generative models, Deep psychology, Python, PyTorch, TensorFlow, JAX, Software engineering, Data curation, Supervised finetuning, RLHF/RLAIF/DPO, LLMs, Realtime audio models, Zen code, MVP, Firstauthor publications, Machine learning history, Hybrid environment, Collaborative culture, Fastpaced environment, Growth opportunity, Impact on ML engineering, Bias toward action, Authenticity, Strong opinions, Historical perspective',\n",
       " 'Medical Laboratory Technology, Blood Testing, Microscopy, Laboratory Equipment Operation, Quality Reviews, Administrative Support, ASCP Board of Registry Exam',\n",
       " 'Agile engineering practices, Apache Hive, Apache Kafka, Apache Hadoop, Apache Spark, AWS, Cassandra, Cloud computing, Data engineering, Distributed systems, EMR, Gurobi, Google Cloud, Java, Linux, Machine learning, MapReduce, Microsoft Azure, MongoDB, MySQL, NoSQL, Open source RDBMS, Python, Redshift, Scala, Snowflake, SQL, UNIX',\n",
       " \"Data Analytics, Data Management, Reporting, Data Analysis Experience, Document Analysis, Legal Documents Analysis, Microsoft Office Suite, SharePoint, Microsoft Access, Adobe Acrobat, Visual Basic for Applications, Tableau, SQL, SAS, Python, R, Machine Learning, Data Mining, Data Visualization, Communication Skills, Analytical Skills, ProblemSolving Skills, Team Collaboration, Attention to Detail, Ability to Meet Deadlines, Bachelor's Degree\",\n",
       " 'Cybersecurity, Data Protection, DLP (Data Loss Prevention), SaaS (Software as a Service), IaaS (Infrastructure as a Service), Agile, Cloud Computing, Virtualization, Networking, Troubleshooting, JIRA, AWS (Amazon Web Services), Cyber Security Certifications, Cloud Certifications, HTML, CSS, JavaScript, Technical Writing, Problem Analysis, Architecture, Crossfunctional Teamwork, Communication, Leadership Influence, Project Management, Technical Requirements, Design, Testing, Deployment, Delivery',\n",
       " 'Data Analytics, Cloud Development, A/B Testing, Simulation, Regression Modeling, Machine Learning, Data Visualization, Automation, MLOps, Python, Cbased Languages, Azure, GCP, PowerBI, Looker, Google AutoML, Azure AutoML, Databricks AutoML, Dashboarding Platforms, Cloud Architect, Data Engineer, UX Designer, Quality Analyst, AutoML Platforms, Simulation Platforms, Digital Twin Platforms',\n",
       " 'Compliance Review, Contract Management, Policy Development, Collaboration, Expertise Building, Training, Juris Doctor degree, State Bar membership, Privacy law experience, Contract negotiation experience, Analytical skills, Problemsolving skills, Attention to detail, Teamwork',\n",
       " 'Machine Learning, Deep Learning, Python, numpy, pandas, numba, torch, tensorflow, jupyter, Model Optimization, Cloud Services (GCP AWS Azure), Speech Processing, Audio Classification, Experiment Tracking Tools (MLFlow WandB DataBricks), Sprint Management, Agile Methodologies, FDA Approvals (SaMD), Product Documentation, Strategic Planning, MVP, Communication Skills, Feedback, Timelines',\n",
       " 'Data Management, Data Interpretation, Data Analysis, Data Visualization, Report Generation, Timetable Management, Data Input, Microsoft Excel, SIMS, Communication Skills, Project Management, Time Management, Attention to Detail, Experience in a School Environment',\n",
       " \"Machine Learning Model Development, Data Preparation, Feature Engineering, Algorithm Selection, Model Evaluation, Deployment, Monitoring and Maintenance, Bachelor's degree, Programming Skills, Python, TensorFlow, PyTorch, Deep Learning, Data Handling, Model Deployment, Docker, Kubernetes\",\n",
       " 'Data management, Data architecture, Data analysis, MS Power Platform, Power BI, Data sharing agreements, Heat network specialists, Project environment, Industrial or technology sectors, Behaviours, Quality Service, Delivering at Pace, Developing Self and Others, Civil Service Pension Scheme, Flexible working, Criminal record check, Internal Fraud Database (IFD), Nationality Requirements',\n",
       " 'C/C++, Golang, Python, Kubernetes, Kubeflow, YARN, Mesos, TensorFlow, PyTorch, MXNet, Django, Flask, NodeJS, React, GPU, RDMA, Machine Learning, Computer Vision, Deep Learning, Natural Language Processing, Speech, Audio, Knowledge, Data Mining',\n",
       " 'SAS, DataFlux, Loqute, Graduate degree, Public health research, Program evaluation, Data analysis, Data quality, Data management, Confidential healthrelated datasets, AIDS Institute populations, HIV, STI, HepC, OUD, Statistical methodology, Interpretation, Dissemination of data, Surveillance, Evaluation technical assistance, Data products, Visualizations, Reports, Peerreviewed publications, Grants, Data matching, Microsoft Office, Oral communication, Written communication, Communication skills, Data management team, Committee for Matched Data Access (CMDA), AIDS Institute (AI), DataFlux matching jobs, SAS DataFlux',\n",
       " 'Azure Data Architect, Data Warehouse, Data Infrastructure, Data Quality, Data Integration, Data Pipelines, Reports, Data Services APIs, Data Lake, Databricks, Python, Scala, Spark SQL, REST APIs, SOAP APIs, Database Management Systems, OLAP, ETL Frameworks, Azure Data Warehouse Architecture, Data Warehouse Design, Enterprise Service Bus, ETL Pipelines, Azure Data Factory, Data Transformation Queries, Handson Experience, IT Education, Data Warehouse Management, Information Systems',\n",
       " 'Managed Services, Microsoft, Oracle, SAP, Salesforce, Servicenow, Workday, Informatica TDM, Python, PowerShell, TDM tools: Delphix Informatica IBM Optim, ALM, qTest, JIRA, Azure DevOps, SQL Server, DB2, Postgres, Redshift, MongoDB, TDM, Agile, Waterfall, Healthcare, Energy, Utilities, Test Data Generation, Test Data Masking, Subsetting, PII/PHI, Nonfunctional testing management, Functional testing, Integration testing, System testing, Regression testing, Performance testing, Security testing, Endtoend testing, User acceptance testing, Software testing, Test management, Test strategy, Test plans, Capacity planning, Infrastructure, Test data management architecture, Tool usage, Tool selection, Test tools, Test automation, Quality assurance, PMP, Agile Certification',\n",
       " 'Data Science, SQL, Python, R, Tableau, RShiny, Data Manipulation, Statistical Concepts, A/B Testing, Clustering, Probability, Machine Learning, SaaS Business Model, Viral Product Loops, Data Analytics, Data Visualization, Communication, Storytelling, Data Mining, Forecasting, Business Intelligence, Experimentation, Roadmapping, CrossFunctional Collaboration, Microservices',\n",
       " 'Data Engineering, Programming Languages: Java Scala Python Open Source RDBMS NoSQL databases, Cloudbased Data Warehousing Services: Redshift Snowflake, Data Analytics, Big Data Technologies, Data Management, Agile Engineering Practices, Cloud Computing (AWS Microsoft Azure Google Cloud), Data Warehousing (Redshift Snowflake), Distributed Data/Computing Tools (MapReduce Hadoop Hive EMR Kafka Spark Gurobi MySQL), Realtime Data and Streaming Applications, NoSQL Implementation (Mongo Cassandra), UNIX/Linux, People Management, Unit Testing, Code Review',\n",
       " 'Database Administration, Oracle RDBMS, Snowflake, Data Warehouse, SQL, Toad, OEM, Microsoft SQL Server, Change Review Board, Data Migration, Performance Tuning, Troubleshooting, Problem Resolution, Backup and Restore, Critical Thinking, Collaboration, Attention to Detail, Customer Focus, Decision Making',\n",
       " 'Technical Program Management, Civil Engineering, Structural Engineering, Architectural Engineering, Problem Solving, Stakeholder Management, Ambiguity Tolerance, Diversity Appreciation, Data Center Product Development, Project Management, Scope Management, Schedule Management, Budget Management, Quality Management, Risk Management, Critical Path Management, CrossFunctional Team Management, Process Development, Release Schedule Coordination, KPI Definition, SLA Definition, Reporting to Senior Leadership',\n",
       " 'Medical or health analytics, Statistical modeling, SAS, R, Advanced statistical package, Leadership, Biostatistics, Statistics, Public Health, Data Science, Publications, Statistical consulting, Study design, Data extraction and manipulation language, Relational database, Mathematics, Engineering, Social/Physical/Life Science, Business, Data analytics',\n",
       " \"Bachelor's degree, Biological/Physical Sciences, Medical Laboratory Science, American Heart Association BLS certification, American Red Cross BLS certification, Medical Laboratory Courses, Medical Laboratory Scientist/Technologist (MLS/MLT), American Society of Clinical Pathology (ASCP), American Medical Technology (AMT), Laboratory General Supervisor, Medical Laboratory Courses, Team of exceptional medical professionals, Technical knowledge, Aseptic technique, Isolation procedures, Infection control measures, Patient safety and confidentiality, Clean and safe environment\",\n",
       " 'Data Designer, Agile, Data Modeling, SQL, Data Analysis, Communication, Teamwork, Problem Solving, Experience in Data Design',\n",
       " 'Python, AWS, Airflow, Snowflake, Scala, Spark, Postgres, Angular JS, NoSQL, Hadoop, Hive, EMR, Kafka, Gurobi, MySQL, Mongo, Cassandra, Redshift, UNIX, Linux, Agile',\n",
       " 'Scala, HBase, Spark, Data pipelines, Data warehouses, Microservices, Agile development, People management, Software engineering, Product management, Data science, Mobile engineering, Backend engineering, Crossfunctional teams, Business analysis, Customer research, Project management, Communication skills, Problemsolving skills, Analytical skills, Attention to detail, Ability to work independently and as part of a team, Ability to meet deadlines, Ability to handle stress, Ability to travel',\n",
       " 'Data Engineering, ELT, Data Lake, Data Warehouse, Data Cleansing, Data Enrichment, Data Quality Control, SQL, Query Optimization, Python, Apache Spark, Kafka, Apache Airflow, Microstrategy, Tableau, Looker, Alation, Agile Development, Cloud Computing, AWS',\n",
       " 'Data Governance, Data Management, Data Privacy, Data Security, Data Architecture, Data Modeling, Data Warehousing, Data Mining, Data Analytics, Business Intelligence, Data Quality, Data Integration, Data Migration, Data Backup and Recovery, Data Disaster Recovery, Data Governance Policies, Data Standards, Data Procedures, Access Control, Encryption, Data Masking, Auditing, Data Cleansing, Data Archiving, Data Lineage, Data Definition, Data Ownership, Data Handling, Data Governance Principles, Data Privacy Regulations, Data Security Regulations, Data Management Tools, Data Technologies, SQL, NoSQL, Hadoop, Spark, Hive, Pig, Informatica, Talend, SAS, SPSS, R, Python, Tableau, Power BI, Bachelor’s degree in information technology business or related field, Master’s degree in data management, 5+ years of experience in information technology, Experience in data governance or data management, Knowledge of data privacy and security regulations, Strong understanding of data privacy and security regulations',\n",
       " 'Machine Learning, Software Development, Data Analysis, Python, NumPy, Pandas, Numba, Torch, TensorFlow, Jupyter, Cloud Services (GCP AWS Azure), MLOps, Data Requirements, Experimentation, Productization, Reporting, Monitoring, Team Leadership, Communication, Conflict Resolution, Career Development, Project Management, Roadmapping, OKR Management, Budget Allocation, Research, Thought Leadership, Speech Processing, Audio Classification, Experiment Tracking, Reproducibility Tools (MLFlow WandB DataBricks), Sprint Management, Agile Methodologies, FDA Approvals, SoftwareasaMedical Device (SaMD)',\n",
       " 'Machine Learning Operations (ML Ops), Engineering, Python, A/B testing, Data engineering, ETL pipeline, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Troubleshooting, Leadership, Autonomy, Flexibility, Teamfirst mentality, Passion for vision, NYCbased, Hardworking, Hiring',\n",
       " 'Financial data analysis, Reporting, Financial/analytical modeling, Advanced Excel, PowerPoint, Data collection, Data analysis, Evaluation, Tableau, Snowflake, Sigma, Python, Anaplan, Attention to detail, Multitasking, Problemsolving, Communication, Interpersonal skills, Teamwork, Creativity, Adaptability, Commitment, Positive attitude, Performance, Technology environment, Accounting, Finance, Business',\n",
       " 'Data management, Data analysis, Microsoft Office, SIMS, Policy management, Statutory returns, Enhanced DBS check, Customer service, Organizational skills, Ability to meet deadlines, Passion for education',\n",
       " 'Server Management, Storage and Backup, Virtualization, Hyper Converged Infrastructure, Networking, Cabling, Fault Diagnosis, Remote Access, Storage Array Configuration, Tape Management, IT Ticket Management, Troubleshooting, Active Directory, TCP/IP, Backup and Recovery, Soft Skills, Communication, Analytical Skills, Record Keeping, Unsupervised Work, Time Management, Quality Work, Productivity, Engineering or Science Degree, 57 Years Experience',\n",
       " 'Data Engineering, Data Architecture, Cloud Computing, Google Cloud Platform, Spark, SQL, Python, Java, NoSQL, Tableau, Power BI, Looker, Machine Learning, AI, Apache Kafka, Google Cloud Pub/Sub, Amazon Kinesis, Serverless, Cloud Functions, Dataflow, BigQuery, DataProc, DataForm, Microsoft Azure, Amazon AWS',\n",
       " 'Data Science, AI/ML, Algorithms, Statistical Modeling, Supervised Learning, Unsupervised Learning, Regression Analysis, Clustering, Outlier Detection, Decision Trees, Ensemble Methods, Neural Networks, Python, R, SQL, A/B Testing, AWS, Business Intelligence Tools, Epidemiology, Medical Affairs, Market Access, Oncology',\n",
       " 'Data Warehousing, AWS, Cloud Native, Agile, ETL, Big Data, NoSQL, Java, Python, RDBMS, Redshift, Snowflake, Machine Learning, Google Cloud, Microsoft Azure, Hadoop, Data Analysis, System Architecture, ETL Design, Data Visualization, Communication, Leadership, Teamwork, Problem Solving, Analytical Thinking',\n",
       " 'Data Analysis, Data Management, Data Optimization, Marketing Campaigns, CPA Goals, Campaign Optimization, Campaign Scale, Campaign Profits, Campaign Insights, Strategic Improvements, Statistical Analysis, Analytical Thinking, Market Analytics, Data Visualization, Marketing Channels, Lead Generation, Lead Optimization, Handson Experience, Campaign Performance, Campaign Analysis, Financial Services, Communication Skills, Organizational Skills',\n",
       " 'ML Ops, Engineering, Complex workflows, Workflow orchestration, Monitoring, Visibility, Experimentation, A/B testing, Data engineering/ETL, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Product ownership, Financial workflows, Corporate financial data, Hiring, Recruitment',\n",
       " 'Data center operations, Telecom, Cabling, Electrical support systems, Mechanical support systems, Power distribution systems, Emergency power, UPS systems, D/C power, HVAC, Fire alarm systems, Fire suppression systems, Data center operations qualification program, Integrated systems, IT, Site support systems, Maintenance, Troubleshooting, Operation, Site operation, Maintainability, Security, Site physical security, Site compliance, Methods of procedures, Preventative maintenance, Corrective maintenance, Emergency maintenance, Training, Qualification, Budgeting, Forecasting, Vendor management, Contract management, Facility budgeting, Facility forecasting, Project management, Scope development, Critical systems, Critical infrastructure facilities, Mechanical engineering, Electrical engineering',\n",
       " 'SQL, Python, ETL, Data warehousing, Data mapping, Data modeling, SQL tuning, Scripting, Powershell, Snowflake, Talend',\n",
       " 'Machine Learning Operations (ML Ops), Complex ML Workflows, EndtoEnd ML Operations, Culture and Practice, Team Building, Longterm Thinking, Iterative Development, Data Engineering, ETL Pipelines, Workflow Orchestration, Monitoring, Visibility, Experimentation, A/B Testing, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Automation, Accounting Workflows, Economic Impact',\n",
       " 'ML Ops, Engineering, Data engineering, ETL pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep learning, Workflow orchestration, Monitoring, Visibility, Experimentation, A/B testing, Autonomy, Flexibility, Teamfirst mentality, Passion for vision, NYCbased, Hiring, Experience with financial workflows, Product ownership',\n",
       " 'Data platform engineering, Data pipeline design and implementation, Data warehouse and data lake management, Data integration, Data engineering technologies and trends, Data platform blueprint and design, Integration with other systems and data models, Cloud technologies (AWS Azure GCP), Data security technologies (Oracle SIEM platforms), Terraform, Splunk platform and its components, Data streaming technologies, Agile methodologies and toolsets, Python, Bash, CI/CD, InfrastructureasCode, Automation tools, Collaboration and stakeholder engagement, Attention to detail, Architectural thinking, Problemsolving skills, Interpersonal skills, 6  10 years of experience, Familiarity with big data platform stack (Databricks Spark Hadoop Flink Airflow MPP Database), Cloud certification, Strong technical skills, Excellent communication skills, Solution architecture, Automation, Technology choices, Experimentation, Proof of concept, Analytical models',\n",
       " 'Data Engineering, Python, AWS, Airflow, Snowflake, Scala, Spark, Postgres, Angular JS, NoSQL, Hadoop, Hive, EMR, Kafka, Gurobi, MySQL, Mongo, Cassandra, Redshift, UNIX/Linux, Agile, MapReduce',\n",
       " 'Data Engineering Manager, Data Engineering, AWS, AWS Cost Explorer, Performance Insights, WellArchitected Tools, Databricks, Python, SQL, Terraform, GitHub Actions, JIRA, Confluence, GitHub, Agile development, Datadriven unit test suites, AWS Appflow, AWS Airflow, AWS Redshift, Starburst, ThoughtSpot, Tableau, SpotFire, Domain Driven Design, Data modelling',\n",
       " 'Data Loss Prevention (DLP), Cyber security, Software engineering, Agile frameworks, Customer engagement, Cloud computing, Troubleshooting, Web proxy, Email, Endpoint solution, Data protection, URL filtering, Network DLP, Operating systems, Databases, Virtualization, JIRA, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, CISSP, GIAC, CISM, CCSP, CISA, Security+',\n",
       " 'Analytical Methods, cGMP, HPLC, CE, qPCR, ELISA, Data Quality, Data Integrity, Accuracy, Completeness, Compliance, Calculations, Documentation, Deviation Investigations, GMP Requirements, Experiments, Assays, Scientific Literature, Regulatory Requirements, Change Management, B.S. degree, M.S. degree, Research Associate, Associate Scientist, COVID19 Vaccination, Equal Employment Opportunity, Affirmative Action',\n",
       " 'Data protection, Cybersecurity, Data Loss Prevention (DLP), Cloud computing, Security solutions, SaaS, IaaS, Symantec Data Loss Prevention (DLP), Agile frameworks, JIRA, AWS, CISSP, GIAC, CISM, CCSP, CISA, Security+, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional',\n",
       " 'EDI, ERP, SOAP, REST, GraphQL, X12, EDIFACT, SAP IDoc, GS1 EPCIS, AS2, sFTP, Contivo, XSLT, ETL, SAP Process Integrator, Sterling Integrator, SaaS, Cloud deployment, AWS, Azure, Supply Chain, ERP systems, Manufacturing, Contract Manufacturing, Wholesalers, Distributors, 3PL, Pharmaceutical Dispensers, Serialization, EPCIS, GS1, Track and trace, GOOD manufacturing practices, Enterprise software, PLM, Workflow Systems, Document Management Systems, Platform Systems, SAP, Oracle, MatrixOne, Enovia, Pegasystems, Force.com, SharePoint',\n",
       " 'Data Management, Housing, Lending, Data Validation, Data Analysis, Compliance, Quality Assurance, Statistics, Access, Excel, Advanced Excel, Pivot Tables, Charts, Graphs, Analytical Skills, Critical Thinking, Writing Skills, Logic, Creativity, Research, Statistics, Methodologies, Assumptions, Collaboration, Teamwork, Presentation Skills, Medical Insurance, Dental Insurance, Vision Insurance, Retirement Benefits',\n",
       " 'Network security, Encryption, Firewall management, Vulnerability assessment, Threat response, Network design, Network engineering, Computer science degree, Flexible work environment, Collaborative atmosphere, Professional development, Volunteer opportunities, Competitive salary and benefits, Equal opportunity employer',\n",
       " 'Software Engineering, C Programming, C++ Programming, Python Programming, Linux, Windows, NVML, NVIDIASMI, Data Center Monitoring, Data Center Management, PCI Express, NVLink, NVSwitch, API Development, CrossPlatform Development, Team Player, Problem Solving, Collaboration, Communication, SelfMotivation, Autonomy, B.S. in Computer Science Computer Engineering or Electrical Engineering, M.S. in Computer Science Computer Engineering or Electrical Engineering, 5+ years of experience in developing user space tools especially for Linux',\n",
       " 'Epic Clarity, Epic Clinical Data Model, SQL, Data analysis, Data visualization, Statistical methods, Clinical informatics, Data mining, Agile Development, Project management, Written and oral communication, Organizational skills, Analytical skills, Technical skills, Usability testing, Data dictionaries, Clinical experience, Inpatient setting, Outpatient setting, Python, R',\n",
       " 'Data Governance, Data Analysis, Root Cause Analysis, Process Auditing, SQL, ANSI SQL, Microsoft SQL Server (MSSQL), TSQL, Oracle (PL SQL), Postgres (PostgreSQL), Database Design, Data Structures, Normalization, Metamodel, Conceptual Data Model (CDM), Logical Data Model (LDM), Physical Data Model (PDM), Microsoft Visio, Microsoft PowerPoint, Story telling tools, Microsoft Excel, Visualization, Reporting, Qlik Sense, Qlikview, Tableau, Crystal Reports, SAP Web Intelligence (WebI), Data Governance Tools, Alation, Atlan, Collibra, Informatica, Data modeling tools, Idera ER/Studio, SAP PowerDesigner, ERwin, Agile, Scrum, Kanban, Privacy laws, CCPA, GDPR',\n",
       " 'Public/private cloud, Platform services and components, DevOps, Automation, Software acquisition, Systems administration, Operational support, Problem resolution, Cloud computing environment, Resource stacks, Configuration, Standard systems management processes, Change management, Incident management, Problem management, Deployable tested and documented automation design scripts, Procedures, Container management systems, CI/CD solutions, Code deployment models, Selfservice automation, Big data foundational concepts, Relational database, Data warehouse, Data lakes, Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Snowflake, Spark, Virtual networking, Docker, Kubernetes, Infrastructureascode technologies, Terraform, Azure, Event streaming, Data transformation, Azure Data Factory, Azure data lake, Networking concepts, DNS, DHCP, Firewalls, Subnetting, Shell, NodeJS, Python, PL/SQL, TSQL, NZ SQL, DB Optimization techniques, Data ingress and egress patterns, Spark and Snowflake platform Architecture, Function/feature, DevOps concepts, Azure DevOps framework, Monitoring concepts, Continuous delivery, Infrastructure as code, Strong problemsolving ability, Fastpaced environment, Developer tooling, Software development life cycle, Hadoop, Computer Science, Information Systems',\n",
       " \"AI, ML, Product Management, Retail Ecommerce, Agile Development, Data Analysis, Decision Making, Leadership, Communication, Business Requirements, Technical Specifications, Product Development, Product Roadmap, CrossFunctional Collaboration, Market Research, Customer Interviews, User Needs, Industry Trends, Performance Monitoring, DataDriven Optimization, AI Advancements, ML Advancements, Retail Technologies, Product Vision, Stakeholder Communication, Executive Leadership, External Partners, Bachelor's Degree, Master's Degree, Computer Science, Business, AI/ML, Ecommerce Technology, Analytical Skills, ProblemSolving Skills, DataDriven Decision Making, Leadership Skills, Communication Skills, Teamwork, Stakeholder Management, Competitive Salary, Equity Packages, Health Insurance, Dental Insurance, Vision Insurance, Flexible Work Hours, Remote Work, Career Growth Opportunities\",\n",
       " 'Maintenance, Reliability, SAP, NonSAP, FLOC, Equipment, PM, MM, Microsoft Office, SixSigma, Lean Black Belt, Plant Maintenance, Material Management, Centralized Maintenance Management System (CMMS)',\n",
       " 'Data Analytics, Data Management, Data Engineering, Data Visualization, Predictive Modeling, Advanced Analytics, Artificial Intelligence, Machine Learning, Data Governance, Data Privacy, Data Security, Regulatory Compliance, Strong Leadership, Strategic Thinking, Practical Execution, Effective Communication, Relationship Building, Collaboration, Domain Knowledge, Thought Leadership',\n",
       " 'Data Analysis, Data Management, Documentation, Windows, Anatomical Knowledge, Crosssectional Imaging, Computer Networks, Radiology Information Systems, PACS, Image Analysis Applications, CT Technology, Clinical Trials Research, Radiology, Database, Data Structures, Quality Controls, SQL, Python, R, SAS, Tableau, Power BI',\n",
       " 'Coding standards, Data quality, SQL, Python, SDLC, Scrum teams, Integration environment, Agile mindset, TDD, BDD, Apache Spark, Hadoop, Scala, Snowflake, GCP BigQuery, AWS Redshift, Airflow, Kafka, Kinesis, Spark Streaming, Flink, Machine Learning, Mentoring',\n",
       " 'Microsoft Distributed File Systems, Windows Server OS, MS Office 365, Active Directory, NTFS File permissions, Windows File ACLs, NFS shares, Azure, AWS, Project management, Data governance, Data migration, Data warehousing, Data analytics, Data visualization, File system management, Network administration, Server administration, Storage systems, Cloudbased storage, Communication skills, Troubleshooting skills, Organizational skills',\n",
       " 'SAS Enterprise Guide (EG), SQL, Excel, PowerPoint, Business Data Analyst Mindset, Data Visualization Software, Power BI, Microsoft Reporting Services, Spotfire, Tableau, Data Visualization, Communication Skills, Creative ProblemSolving, SelfMotivation, Ability to Work Under Pressure, Ability to SelfCheck Work, Ability to Plan Organize and Work on Multiple Tasks, Risk Management, Leadership, Data Analysis, Statistical Analysis, Analytical Thinking, ProblemSolving, Reporting, Dashboarding, Business Intelligence, Data Warehousing, Data Mining, Machine Learning, Artificial Intelligence, Cloud Computing, Big Data, Data Governance, Data Security, Data Privacy, Data Ethics, Data Quality, Data Integration, Data Architecture, Data Engineering, Data Science, Data Analytics, Business Intelligence, Data Visualization, Data Storytelling, Machine Learning, Artificial Intelligence, Natural Language Processing, Computer Vision, Robotics, Blockchain, Cybersecurity, Internet of Things',\n",
       " 'Linux, Unix, Java, Python, JavaScript, Spark, Kafka, Flink, Sqoop, EMR, Kinesis, Sagemaker, Splunk, Flume, Machine learning, Syslog, HEC, HEC ingestion, NoSQL',\n",
       " 'SQL, Tableau, Python, Azure, Microsoft SQL, Excel, Data governance, Data visualization, Data analysis, Data management, Architecture, Software development, Programming, Database administration, Data integration, Data warehousing, Data science, Data analytics, Data modeling, Data transformation, Data cleansing, Data mining, Data reporting, Performance management, Business intelligence, Strategic planning, Policy development, Risk management, Change management, Project management, Leadership, Communication, Problemsolving, Critical thinking, Creativity, Innovation, Collaboration, Teamwork, Adaptability, Flexibility, Time management, Organizational skills, Attention to detail, Accuracy, Precision, Clarity, Conciseness, Completeness, Consistency, Reliability, Validity, Utility, Significance, Fairness',\n",
       " 'Data Analytics, Business Data Analyst, DataDriven Insights, Product Analytics, Marketing Analytics, Operations Analytics, Statistics, Mathematics, Data Analytics, Finance, Advanced Analytics, SQL, Excel, Data Visualization, Qlik, Tableau, PowerBI, Experimentation Design, Experimentation Execution, Project Management, Communication Skills, Storytelling, DataDriven Decision Making, Organizational Skills, Documentation, Time Management, Portfolio Prioritization, Problem Solving, Collaboration, Critical Thinking, Business Needs, Data Exploration, Hypothesis Formulation, Data Generation, Data Cleansing, Testing, Insight Generation, Visualization, Action Planning, Business Insights, Strategic Decision Making, Entrepreneurial Guidance, Research, Findings, Recommendations, Growth Culture',\n",
       " 'Data Center Operations, Server Maintenance, Storage Management, Networking, Capacity Planning, Documentation, Backups, Technical Support, Scripting, Energy Efficiency, Environmental Compliance, Associate Degree, Information Technologies, Monitoring, Troubleshooting, Installation, Configuration, Assessment, Documentation, Backups, Technical Support, Automation, Energy Monitoring, Environmental Testing',\n",
       " 'MS Excel, MS Word, MS Access, MS Power Point, Relational databases, Data analysis, Information extraction, Communication skills, Attention to detail, Technical writing, Change management, Quality assurance, Team work, Leadership, Safety, Nuclear facilities, US citizenship',\n",
       " 'Data Analytics, Data Visualization, Statistical Analysis, Data Science, Machine Learning, Business Intelligence, Data Mining, Database Management, Python, R, SQL, Tableau, Apache Spark, ElasticSearch, Kibana, SAS, SPSS, D3, Visual Basic, MySQL',\n",
       " 'Machine Learning, LargeScale Platform Development, StateoftheArt Machine Learning Projects, Targeting Components, Product Vision, Computer Science, Computer Engineering, Machine Learning Concepts, Machine Learning Techniques, Team Leading, Programming, Debugging, Optimization, Go, C/C++, Python, Tensorflow, PyTorch, Critical Thinking, ProblemSolving',\n",
       " 'Machine Learning, Distributed Computing, AI, Python, Scala, Java, Scikitlearn, PyTorch, Dask, Spark, TensorFlow, Data Pipelines, Cloud Computing, AWS, Azure, Google Cloud Platform, Data Gathering, Data Preparation, Agile, People Leadership, Team Leadership, Presentations, Papers, Blog Posts, Open Source Contributions, Patents',\n",
       " 'Medical Technologist, Medical Laboratory Technician, MT/CLS (ASCP), MLT/CLS (ASCP), Blood Bank, Clinical Laboratory Tests, Data Analysis, Equipment Operation and Maintenance, StateoftheArt Equipment, FastPaced Lab System',\n",
       " 'ML Platform Design, Cloud Hosted AI Services, Foundational AI Models, LLMs, Edge AI, AI Research, Data Engineering, Automation, Visualization Tools, Analytics Solutions, Data Products, Data Feeds, AI/ML Models, Data Science, Analytical Techniques, Data Sources, Data Lineage, Business Challenges, DataDriven Insights, Frameworks, Standards, Prototypes, Python, SQL, Database Design, Master Data Strategies, Data Products, Software Development Tools, Practices, Distributed Systems, Streaming Systems, Kubernetes, Kafka, Airflow, Dagster, Communication Skills, CrossFunctional Teams, Export Licensing, Equal Opportunity Employer',\n",
       " 'Machine Learning, Medical Imaging, Deep Learning, Algorithm Development, TensorFlow, PyTorch, Python, Cloud Computing, AWS, Azure, Communication, Collaboration',\n",
       " 'Data science, Data engineering, Machine learning, Natural language processing, Data extraction, Data warehousing, ETL, Reporting, AWS technologies, Redshift, RDS, EMR, SQL, Data modeling, Python, APIs, Agile methodologies, Coding standards, Code review, Source management, Build processes, Testing, Operations, Jira, Confluence, Gitlab, Data integrity, Test design, Analysis, Validation, Documentation',\n",
       " 'Python, AWS, Airflow, Snowflake, Scala, Spark, Postgres, Angular JS, NoSQL, Hadoop, MapReduce, Hive, EMR, Kafka, Gurobi, MySQL, Mongo, Cassandra, Redshift, UNIX/Linux, Agile',\n",
       " 'Installation, Data Cables, International Travel, Autonomy',\n",
       " 'Data management, Data modelling, ETL software, Data analysis, SQL, Python, Bloomberg terminal, Bloomberg Data workflows, Semantic structures, Econometrics, Hypothesis driven data analysis, Data governance, Economics, Business management, Team leadership, Recruiting, Mentoring, Developing, Collaboration, Communication, Problem solving, Creativity, Flexibility',\n",
       " 'Datadriven marketing, Lead nurturing, Email marketing, Marketing automation, Campaign management, Audience segmentation, Marketing analytics, CRM, A/B testing, Marketing experimentation, B2B marketing, Lifecycle marketing, Demand generation, Data analysis, Written communication, Verbal communication, Time management, Problemsolving, Critical thinking, Leadership, Teamwork, Creativity, Innovation, Passion for marketing, Customer journey',\n",
       " 'Software Engineering, System Design, Application Development, Testing, Cloud Native, Computer Science, Computer Engineering, Mathematics, Data Media Types, Semantic Technologies, Modeling, Controlled Vocabularies, Ontologies, Taxonomies, Graph Databases, Graph Processing Languages, Snowflake, Communication Skills, ProblemSolving, Teamwork, Technical Guidance, Technical Expertise, Technical Operations, Technical Processes, Design, Functionality, Stability, Scalability, Security, Quality Assurance, Debugging',\n",
       " \"Agile, Containerization, Cloudgenix, Ansible, Automation, ITIL, Scripting, Python, PowerShell, SDWAN, SDN, ACI/NSX, Rest/SOAP, Expect, Rancid, CISCO, Juniper, Arista, Brocade, EMC, HP/Aruba, AWS, Azure, Google Cloud, Oracle Cloud, VMware, Hadoop, Spark, Kafka, Flink, NoSQL, SQL, Machine Learning, Artificial Intelligence, Data Science, DevOps, Scrum, Kanban, Leadership, Communication, Problem Solving, Decision Making, Risk Management, Analytical Skills, Execution, Delivery, Commitment, Strategy, Business Impact, Results, Ownership, Accountability, Detail Oriented, Technical Acumen, Partnering, Bachelor's degree, Engineering, Computer Science, MIS, Technical Training, Advanced Technical/Business Degree\",\n",
       " 'Databricks Platform, Data Pipelines, Scheduling, Spark SQL, Python, Scala, Cloud Native Tools, Azure, AWS, Realtime Data Pipeline',\n",
       " 'Software Development, Data Structures, Algorithms, Testing, Maintenance, Software Design, Software Architecture, ML/AI Algorithms, Deep Learning, Natural Language Processing, Computer Science, Technical Leadership, Accessible Technologies, Information Retrieval, Distributed Computing, Largescale System Design, Networking, Data Storage, Security, Artificial Intelligence, UI Design, Mobile, Project Management, Time Management, Deliverables Management, Software Solutions, Android, Mobile Operating Systems, Compensation, Benefits, Equal Opportunity, Affirmative Action, EEO Policy',\n",
       " 'Manager Cyber Risk & Analysis (Machine Learning), Process Management, Project Management, Risk Management, Cloud Risk Management, Compliance, Legal, Regulatory, Operations, Technology, Cybersecurity, PRIME system, Knowledge of Machine Learning, Knowledge of Generative AI, Business Process Management certification, Lean, Green Belt Certification, Change Management, Risk Guide, CRISC, CISM, CRCM, CIPP, ABA Risk Mgmt Certification, PMP',\n",
       " 'Data Management, System Design, Database Technologies, Data Storage, Data Access, Data Integration, Data Architecture, Data Structures, Data Systems, ETL, Data Analytics, Reliability, Availability, Compatibility, Transportability, Interoperability, Maintainability, Safety, Human Factors, Manpower Supportability, Logistics Supportability, Environmental Effects, System Documentation, Training, HighQuality Analysis, Data Collection, Data Processing, Data Evaluation, Data Reporting, Test Design, Test Planning',\n",
       " 'Software engineering, ML features storage and retrieval, Redis, DynamoDB, OpenSearch, Kubernetes, Docker, AWS/GCP/Azure, Cloudbased environments, Distributed systems, Scalable systems, Low latency, High throughput, Problemsolving, Project management, Crossfunctional collaboration, Custom edge cases, Technical solution design, System integration',\n",
       " 'DLP, Cyber Security, Cyber Technical, Agile, JIRA, AWS, Cloud Computing, AWS Solution Architect, Symantec, Data Loss Prevention, Linux, Windows, Budgeting, Excellent Communication, Network Security, Security+, CCSP, CISSP, CISM, Compliance, Software Development, Project Management, Software Engineering, Technical Writing, Python, Unit Testing, Object Oriented Programming, Data Structures, Communication, Stakeholder Management, CrossFunctional Teams, Public Speaking, Troubleshooting, Problem Solving, Analytical Thinking, Business Analysis, Data Protection, Information Security, Risk Management, Cloud Infrastructure, AWS Security, Data Privacy, Regulatory Compliance, Ethical Hacking, Penetration Testing, Incident Response, Threat Intelligence',\n",
       " 'Agile, Artificial Intelligence, AWS, Big Data, SQL, Java, Scala, Python, Open Source RDBMS, NoSQL, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, MongoDB, Cassandra, Redshift, Snowflake, UNIX/Linux, MapReduce, Cloud Computing, Microsoft Azure, Google Cloud, Data Modeling, Distributed Data/Computing, RealTime Data and Streaming Applications, Data Warehousing, Unit Testing, Code Reviews, Agile Engineering Practices, Machine Learning, Distributed Microservices, Full Stack Systems',\n",
       " 'Nursing, Patient care, Critical care, Interdisciplinary teamwork, Quality improvement, Evidencebased practice, Clinical competence, Research, Rotating shifts, Negativepressure isolation rooms, Aesthetics, Family support, Infection control',\n",
       " 'Data science, Machine learning, Artificial intelligence, Robotic process automation (RPA), Agile, Statistics, Mathematics, Operations research, Computer science, Information systems, Engineering, Economics, Data analytics, Python, R, SQL, Medicaid, Health insurance exchanges, Scripting language, Statistical analysis, Predictive modeling, Simulation, Business development, RFP/RFQ responses, White papers/concept papers, Prototype solutions, Compensation, Benefits package, Health insurance, Dental insurance, Vision insurance, Personal and family sick time, Company paid holidays, Variable incentive bonus, Parental leave, Adoption assistance, 401(k) retirement plan, Life insurance, Health savings account, Dental/vision & dependent care flexible spending accounts, Shortterm & longterm disability, Student loan pay down, Tuition reimbursement, Personal development, Learning opportunities, Skills development, Certifications, Employee referral program, Corporate sponsored events, Community outreach, Emergency backup childcare program, Mobility stipend',\n",
       " 'Case Report Form, Clinical Research, Clinical Trials Management System, Patient Registration, Data Management, Adverse Event Reporting, Clinical Research Nurse, Clinical Research Coordinator, Research Data Specialist, Data Queries, Monitor Audits, Study Supplies, Biologic Samples, Radiology Films, Lab Specimens, Hospital Records, Study Visit Calendars, Investigator Availability, Ancillary Services, Clinical Environment, Computer Systems, Microsoft Office Suite, Certified Clinical Research Professional, Society of Clinical Research Associates',\n",
       " \"Consumer Centricity, Consumer data management, Digital Transformation, GDPR & Privacy regulations, Data Governance, Data Architecture, Data Engineering, Data Partnerships, DX, eComm, Consumer Relations, Data monetization, Datacentric business strategy, Analytical skills, Strategic skills, Project management skills, Decisionmaking skills, Problemsolving skills, Communication skills, Interpersonal skills, Bachelor's Degree, MBA/Graduate degree\",\n",
       " 'AI/ML, Cloud infrastructure development, Machine learning frameworks, Data visualization tools, DevOps, Data Engineer, Backend machine learning tech stacks, Dataoriented workflow, TensorFlow, PyTorch, scikitlearn, JIRA Xray, SonarQube, JFrog, TeamCity, Data pipelines, ML models, CI/CD pipeline, GCP, MS in computer science, Statistics, Mathematics',\n",
       " 'Data Architect, SQL, Data Models, Data Warehousing, Data Pipelines, Data Migration, Data Governance, Data Quality, SDLC, DevOps, CI/CD, Python, Databricks/Spark, Azure Data Factory, Data Lake Analytics, Databricks, Stream Analytics, Azure Functions, SSRS, Tableau, Microsoft Power BI, MDM, Mortgage, Financial Industry, Data Processing, ODS, Data Analytics, Visualization Tools, ETL, DDL, Data Standards, Realtime Data, Batch Data, Data Integration, Data Architecture, Data Management, Data Modeling, Data Migration, Data Quality, Data Security, Data Governance, Business Intelligence, Enterprise Data Architecture, Database Design, Database Development, Data Warehousing, Data Analytics, Data Visualization, Data Governance, Cloud Computing, Azure Cloud Technologies, Big Data Platforms, Data Platforms, Data Scalability, Data Performance, Data Transformation, Data Modernization, Enterprise Data Solutions, Storage Solutions, Data Processing, Realtime Data Processing, Batch Data Processing, Data Processing at Scale, Data Movement, Data Transformation',\n",
       " 'Master Data Management, Data Cleansing, Data Ingestion Pipelines, Business Process Workflow, Data Matching & Deduplication, Standardization, Semiagile Delivery Framework, Master Data Hierarchy Management, Deterministic Matching Methodologies, Probabilistic Matching Methodologies, Data Profiling, Data Quality Analysis, Data Scorecard Development, Exception Management, Data Governance, Data Transformation, ETL Flows, SQL, Informatica MDM/DG, SAP MDM, Agile, SDLC, Collaborative Approach, Analytical Skills, ProblemSolving Skills, Business Communication Skills',\n",
       " 'Machine Learning, Deep Reinforcement Learning, Federated Learning, Generative AI, PhD in Telecoms, Computer Science, Engineering, Electrical Engineering, Electronic Engineering',\n",
       " 'Data integration, Data modeling, ETL (Extract Transform Load), Data warehousing, SQL, Redshift, Python, Agile Scrum, BI systems, Distributed systems, Performance tuning, Debugging, Data pipelines, Data storage',\n",
       " \"Ontology, Data Modeling, Data Standardization, Machine Learning, Semantics, Semantic Technology, Knowledge Organization, RDF, RDFS, OWL, SKOS, SHACL, SQL, SPARQL, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, JSON, XML, OpenAPI/YAML, AVRO, Python, R, Agile, Amazon Web Services, Bachelor's degree in information science computer science engineering library science ontology semantics or computational linguistics, Master's degree or PhD in information science computer science engineering library science ontology semantics or computational linguistics, Experience translating business strategy and analysis into enterprise semantic solutions, Familiarity with graph databases and technologies, Familiarity with JSON OpenAPI/YAML AVRO, Familiarity with Agile principles processes and methodologies, Familiarity with Amazon Web Services, Detailoriented and an ability to problemsolve independently, Strong reading and writing skills, Strong project management experience, Excellent communication skills and the ability to present ideas clearly and with confidence., Ability to work individually and with a team to meet deadlines, At least 6 years’ experience in a metadata field of work (ontology taxonomy semantics or computational linguistics), At least 6 years’ experience or training in using W3C standards including linked and canonical data and ontologies ( JSON XML RDF RDFS OWL and SKOS), At least 4 years’ experience or training in ontology and linked data tools (Protégé TopQuadrant PoolParty Stardog AnzoGraph Neptune or Data.World), At least 4 years’ experience or training with SQL or SPARQL\",\n",
       " \"Early Childhood Education, Education, Psychology, Social Work, Standardized Child Assessments, Child Observation, Math Assessments, Literacy Assessments, Bilingual (Spanish and English), Research Experience, Experience with Young Children, Working in Classrooms, InPerson Work, Reliable Car, Valid Driver's License, Criminal Background Check, TB Test, Ability to comply with sitelevel health and safety regulations, COVID19 Vaccination, PPE (e.g. wearing masks)\",\n",
       " 'SAP S/4HANA, Data Design, ERP, ECC, Business Data Requirements, S/4 Enterprise Data Standards, Data Governance, Master Data, Reference Data, Metadata, Data Models, Business Glossary, Ontologies, Process Design, Analytics, Change Impact Assessments, Quality Core, Data Migration, Data Quality Strategy, Data Cleanse Strategy, Data Enrichment, Lifecycle Management, Network Planning, Supply Chain Planning, SAP Architecture, Data Integration, Business/IT Partnering, Diversity and Inclusion, Lifelong Learning, Growth and Development',\n",
       " 'Data Science, Leadership, Coaching, Mentoring, SQL, Python, Advanced Analytics, Machine Learning, Forecasting, Recommendation Engines, Clustering, Natural Language Processing, Business Judgement, Strategic Thinking, Collaboration, Coordination, Marketing, Stakeholder Management, Communication, Team Management, Data Culture, Predictive Analytics',\n",
       " 'Revenue Cycle Management, Data Analysis, Data Visualization, Epic Clarity, SQL, Python, R, Statistical Methods, Project Management, Agile Development, Epic Revenue Cycle, Healthcare Data, Clinical Experience, Written Communication, Oral Communication, Organizational Skills, Analytical Skills, Technical Skills',\n",
       " 'Data Analysis, Data Reporting, Process Improvement, Data Structures, Metadata, Data Integrity, Data Modeling, Business Intelligence (BI), Database Analytics, Regulatory Reporting (FRY9C FFIEC 031 FR 2052a FR 2028D FRY14), Excel, Database Query, SOX Compliance, Record to Report (R2R), Management Reporting, OFSAA, AXIOM',\n",
       " 'Analytical Thinking, Big Data, Data Modeling, Statistics, Research, Evaluation, Data Analysis, Planning, Design, Computer Systems, PreDeveloped Application Packages, System Principles, Procedures, Design Solutions, Integration, Computer Systems, Sensor Data, Communications, BA/BS Degree, TopSecret Clearance, 401K with company match, Comprehensive health and wellness packages, Internal mobility team, Professional growth opportunities, Cuttingedge technology, TopSecret Clearance',\n",
       " 'Data Science, Business Analysis, Data Wrangling, Data Cleaning, Data Verification, Data Enrichment, DataInformed Business Strategy, Data Product Requirements, Technological Evaluation, Data Innovation, Statistical Modeling, Machine Learning, Data Visualization, Dashboard Development, Process Playbooks, Model Development, Computer Science, Statistics, Econometrics, Physics, Python, SQL, Spark, PyTorch, Relational Databases, NOSQL Databases, CloudNative Technology, Data Mining, Statistical Modeling, Cloud Computing, Data Engineering, MLOps, R',\n",
       " 'C++, CUDA, Python, Pytorch, TensorFlow, Machine learning, Deep learning, Model training, Model serving, Model Parallel framework, Megatron, DeepSpeed, Open source machine learning framework, Algorithms, Systems, Resource optimization, Communication, Teamwork, Large Language Model',\n",
       " 'Machine Learning, Software Development, Cloud Computing, Distributed Systems, Data Engineering, Python, Scala, Java, Scikitlearn, PyTorch, Dask, Spark, TensorFlow, AWS, Azure, Google Cloud Platform, Open Source ML Software, Data Gathering, Data Preparation, Data Pipelines, ML Systems, Responsible and Explainable AI',\n",
       " 'Statistics, Finance, Engineering, Computer Science, Math, Advanced research, Problem solving, Analytical, Critical thinking, Business operations, Value chain, Market conditions, Written, Verbal, Interpersonal communication, Persuading, Informing, Influencing, Collaboration, Project management, Business process changes, Data sources, Strengths, Shortcomings, Microsoft Office (Word Excel Access PowerPoint), Adobe, Tableau, Power BI, SQL, Programming languages, Data manipulation, Business intelligence tools',\n",
       " \"Data Management, Data Analysis, Data Manipulation, Data Collection, Reporting, Metrics, Charts, Graphs, MS Office, MS Project, GUI Tools, Statistical Analysis, Databases, Relational Databases, Data Repositories, Integrated Master Schedule, Customer Service, Communication, Decision Making, Shipbuilding, Program Management, SECRET Clearance, Bachelor's Degree, Military Experience, DoD Contractor Experience, Database Management Training, Personal Development, Professional Development\",\n",
       " 'Data Science, Machine Learning, AI, R&D, Research, Grant Writing, Patent Development, MA or Ph.D. in Data Science, NSF Protocol',\n",
       " \"Configuration Management, Data Management, Data Analysis, Change Control, Engineering Change Proposals, Program Acquisition Documentation, Governmentinstituted Processes, Documentation, Data Management, Program Acquisition, Acquisition Plans (AP), Acquisition Strategy Reports (ASR), Procurement Initiation Document (PID), Statement of Work (SOW), Contract Data Requirements Lists (CDRLs), Acquisition Program Baseline Agreements (APBA), Program Compliance, Briefings, Reports, Correspondence, Meetings, Conferences, Review Boards, 4130 Packages, Routings in ECM, ECM for ECPs, Certification, CCB Chair Approval, RFVs in ECM, GSEG and UA CSA, CAR and HSCM Reviews, Configuration Audits, TD Development, NATECH, Bachelor's Degree, Associate's Degree, Work Experience, Secret Security Clearance, U.S. Citizenship, Affirmative Action/Equal Opportunity Employer, EVerify, Workplace Diversity\",\n",
       " \"Identity Data Management (IdDM), Course development/design, Instructional delivery, Mentorship, Advisory, Review requirements, IdDM applications, IdDM Support, Community outreach, Advanced Planner's Course, Due Diligence course, DoD IdDM, Instructor Training, Travel Support, Travel Risk Management, Exceptional Communication, Valid US Passport, TopSecret Level Clearance, SCI Access, Counterintelligence Polygraph\",\n",
       " 'Data Science, Data Science leadership, Cloud Computing, Python, R, SQL, Machine Learning, Statistical Analysis, Experimental Design, System Design, Software Development, Communication, Collaboration, Problem Solving, Critical Thinking, Forecasting, Payments, Financial Analysis, Statistics, Quantitative Analysis, Business Strategy, Leadership, Mentorship',\n",
       " 'AML/BSA Specialist, AntiMoneyLaundering, Bank Secrecy Act, Suspicious Activity, Suspicious Activity Report (SARs), Customer Account Transactions, Money Laundering, Federal Government, Internal Departments, Financial Institutions, Local Authorities, State Authorities, Federal Authorities, Analytical Ability, Communication Skills, WellOrganized, SelfStarter, Relevant Experience, Relevant Education, Relevant Training',\n",
       " 'Data Analysis, Data Management, Statistical Analysis, Database Management, Query Design, Report Creation, Graphic Representation of Data, Business, Computer Science, Complex Databases, Hazardous Chemical, Biological Agents, Radioactive Agents',\n",
       " \"Clinical Data Management, Data Coordination Center, Data Operations, Infectious Disease Clinical Research Program (IDRCP), Electronic Data Capture (EDC), REDCap, Programming Languages: SAS Oracle SQL PHP, Microsoft Teams, Database Management, Supervisory Responsibilities, Statistical Analysis, Data Quality Control, Data Security, Clinical Trials Management, Good Clinical Practices (GCP), Certified Clinical Data Manager (CDM), Bachelor's Degree, Master's Degree, 10+ years of experience, 5+ years of supervisory experience\",\n",
       " \"Medical Laboratory Scientist, Blood Bank Experience, Lab Lead or Supervisor Experience, ASCP or AMT Certification, Flu Vaccine Requirement, Specimen Acceptability Determination, Standard Operating Procedures Adherence, Instrumentation Monitoring and Troubleshooting, Technical Decision Making, Reagent and Control Preparation Testing and Evaluation, Accurate and Timely Test Result Reporting, Preventive Maintenance and Quality Control Procedures, Testing Bench Supply Identification and Replenishment, Specimen Processing Assistance, Safe Work Environment Maintenance, Personal Protective Equipment Usage, Independent and Team Work Ability, Computer Proficiency, Laboratory Information Systems Familiarity, Attention to Detail, Communication and Organizational Skills, Standardized Color Vision Screen, Overtime or Shift Work Flexibility, Bachelor's in Biology Chemistry Clinical Laboratory Science or Medical Technology, 24 Years Clinical Laboratory Testing Experience\",\n",
       " 'AWS Cloud, Hadoop, Spark, Scala, Presto, Flink, Air flow, Physical data modeling, Logical data modeling, Data modeling, File formats, Data positioning, Data tuning',\n",
       " 'DeFi, NFTs, DAOs, Web 3.0, Crypto, A/B testing, Causal inference, Machine learning, SQL, Python, Data engineering, Statistical concepts, Analytical projects, Data analysis, Data pipelines, BA/BS in quantitative field, PhD in related field, 5+ years of relevant experience, 2+ years of experience, Project planning, Data pipelines, Communication skills, Learning, Execution, Energy, Understand statistical concepts, Understand programming/modeling, Leadership, Innovation, BA/BS in a technical field, MA/MS in a technical field, PhD in a technical field, 5+ years of relevant experience, 10+ years of relevant experience, Data engineering skills, Lower level work',\n",
       " 'Data Analytics, Data Mining, SQL, Tableau, R (Programming language), Python (Programming language), Data Visualization, Data Representation, Databases, Data Science, Analytical Skills, Business Analysis, Business Intelligence (BI), Machine Learning, Statistics, Java, MS Power BI, Data Analyst, MS SQL Server',\n",
       " 'Product Management, Artificial Intelligence, Machine Learning, Generative AI, Large Language Model, Retail, Ecommerce, Product Roadmap, User Stories, Development Teams, Retail Technology, PointofSale (POS), Store Mobile Solutions, SelfService Kiosks, Automated Teller Machines, Check Processing Systems, Barcode Scanner, Jira, Confluence, Vendor Interactions, Collaboration',\n",
       " 'Quantitative Research, Machine Learning Engineering, Statistical Analysis, Econometrics, Financial Modeling, Machine Learning Development, Data Preprocessing, Feature Engineering, Data Cleaning, Algorithmic Trading Strategies, Portfolio Management, Risk Management, Collaborative Research, Continuous Learning, Quantitative Finance, Statistics, Economics, Computer Science, Python, R, Data Engineering, Financial Markets, Trading Strategies, Risk Management, Communication, Collaboration, Dynamic Environment, FastPaced Environment, Project Requirements',\n",
       " 'Data Management, EDC system, eCRF, ePRO, Data Validation, System Validation, Database Design, Data Transfer Agreements, Data Cleaning, Statistical Analysis, Data Analysis, Data Interpretation, Programming (SAS Business Objects IX), Communication, Presentation skills, Attention to detail, HighCharacter, Collaborative culture, Integrity, Team spirit, Accountability, Mutual trust, Honesty',\n",
       " 'Securities Data Management, Investment Data Quality, Investment Data Ecosystem, Investment Types, Investment Life Cycle, Data Flows, Data Processes, Business Process Development, Continuous Improvement, Efficiencies, Employee Development, Training, Planning, Departmental Policies, Internal Control Standards, Eagle PACE, Data Management Tool, Eagle STAR, Investment Accounting Platform, Excel, SQL, Oracle Query Tools, Microstrategy, Reporting Tools, Business Acumen, Perseverance, Accountability, Resourcefulness, Business Challenges, Decision Making, Ownership, Collaboration, Teamwork, Networking, Degreed, Learning Platforms, Ethics, Integrity',\n",
       " 'Python, C#.NET, Go, Java, Angular, React, PostgreSQL, SQL Server, Top Secret or TS/SCI clearance, Full stack development, Software systems design, Testing and infrastructure, Profiling and application performance improvement, Data, Software engineering, Product development, JavaScript, Objectoriented programming, Single page web applications',\n",
       " \"Machine Learning (ML) Engineering, Dataintensive solutions, Distributed computing, ML modeling techniques, Model training, Hyperparameter tuning, Dimensionality reduction, Bias/variance tradeoff, Validation, Data gathering, Data preparation, Unit testing, Integration testing, Deployment testing, Responsible AI, Explainable AI, Python, Scala, Java, scikitlearn, PyTorch, Dask, Spark, TensorFlow, AWS, Azure, Google Cloud Platform, Data pipelines, ML frameworks, Performance engineering, Resilient coding, Maintainable coding, People leadership, Agile development, Scrum, Kanban, Team management, ML industry impact, Conference presentations, Papers, Blog posts, Open source contributions, Patents, Bachelor's degree, Master's degree, Doctoral degree, Computer science, Electrical engineering, Mathematics\",\n",
       " \"Data management, Databased planning, Data production, Compliance measurement, Training improvement, Economics, Efficiency, Internal mission goals, Program goals, Databased product creation, Navy databases, CEODD historical records, CEODD processes, Data display, Data dissemination, Data explanation, CEODD staff, Navy commands, DOD commands, ISO of EOD, ND SCWG, EOD and ND rate RRL requirements, CEODD database management, Database requirements, Microsoft Office Suite, Microsoft 365 applications, Bachelor's degree, Information technology, Web design, Graphic design, Graphic design experience, Web content creation experience, Government organizations, DOD organizations\",\n",
       " \"Sustainability Data, ESG reporting, Carbon management, Data architecture, Data management, Data analytics, Data integration, Data visualization, Data quality, Data warehousing, Business intelligence, Project management, Communication, Collaboration, SAP data, ETL, Data frameworks, Data workflows, Data pipelines, Data governance, Data stewardship, Database, Data blending, Master's Degree, Bachelor's Degree, Data Science, Information Systems, Environmental Management, Sustainability Leadership, Business Administration (MBA), ESG Corporate Sustainability Disclosure Management, Product Sustainability, Circularity Performance, Environmental Management, Sustainable Supply Chain Management, Risk Management\",\n",
       " \"SAP EHS, S/4 HANA, SAP, Data Migration, Data Validation, Product Safety, Project management, Reporting, Data Transformation, Data Extraction, Analysis, Rule sets creation, Specifications review, Documentation review, Standards compliance, Collaboration, Problemsolving, Attention to detail, Accuracy, Bachelor's degree in science\",\n",
       " 'Master Data Analyst, Master Data Management (MDM), Data Quality, Data Governance, Data Harmonization, Agile, Traditional SDLC, SQL, Informatica MDM/DG, SAP MDM, Data Enrichment, Dun & Bradstreet, ETL, Data Profiling, Data Scorecard Development, Exception Management, Data Cleansing, Data Ingestion Pipelines, Business Process Workflow, Data Matching & Deduplication, Standardization, Data Modeling, HighLevel Design, Business Communication, Analytical Skills, ProblemSolving Skills',\n",
       " 'Peoplesoft, Oracle database, Software development, Testing, Release management, Operating system, Linux, Windows, DevOps, Ansible, Puppet, Jenkins, GitLab, Vagrant, Network packet, Hardware performance, Storage analyzers, Coding style guides, Naming specifications, Technical documentation standards, Regression testing, ERP software, Installation, Implementation, Administration, SQL, MySQl, MS SQL, Prostgres, Replication, Clustering',\n",
       " 'Software engineering, Agile methodologies, CI/CD, Applicant Resiliency, Security, Financial services, IT system, Cloud native, Java, PL/SQL, AWS, Automation, Continuous delivery, Software Development Life Cycle, Diversity, Equity, Inclusion, Respect, Continuous delivery',\n",
       " 'Distributed Systems, SRE (Site Reliability Engineering), Production Engineering, Monitoring, Alerting, Error Budgets, Fault Analysis, Reliability Engineering, Kubernetes, Docker, Containerization, Automation, Java, Python, Go, Communication Skills, Leadership, Kafka, Service Lifecycle Management, Troubleshooting, Computer Science',\n",
       " 'Data Management, Financial Crimes, Fraud, Audit, Risk Assessment, Data Analysis, AWS, CAMS CIA CPA CRCM CFE CDMP CISM CISP, Business Intelligence, Reporting, Communication, Investigation, Problem Solving, Teamwork, Leadership, Analytical Thinking, Regulatory Compliance',\n",
       " 'Machine Learning, Software Engineering, Data Engineering, Data Science, Java, Scala, Python, Kotlin, MLOps, Kubeflow, MLFlow, Scikitlearn, LGBM, TensorFlow, PyTorch, Spark, S3, Hadoop, Docker, Kubernetes, DevOps, CI/CD, Containerization, Container Orchestration, APIs, Model Serving, Data Pipelines, Monitoring Tools, Scalability, Performance, Collaboration, Communication, Troubleshooting, Debugging',\n",
       " 'Medical Laboratory Technician, NAACLS, MLT(ASCP), MLT(AMT), Chemistry, Hematology, Microscopic evaluations, Microbiology, Laboratory Quality Control, Preventive Maintenance, Technical Procedures, Customer Service, Communication skills, Problemsolving skills, Leadership, Interpersonal skills, Teamwork, Organization, Prioritization, Accuracy, Attention to detail, Data analysis, Quality control, Safety protocols, Bloodborne pathogen exposure, Lifting, Climbing, Squatting, Kneeling, Twisting, Standing, Sitting',\n",
       " \"Machine Learning, Data Preparation, Feature Engineering, Algorithm Selection, Model Evaluation, Deployment, Monitoring and Maintenance, Bachelor's Degree, 5 Years Experience, Programming Languages, TensorFlow, PyTorch, Deep Learning, Data Manipulation, Preprocessing, Model Deployment, Docker, Kubernetes\",\n",
       " 'Data security, Data governance, Data platform, Cloud computing, Distributed systems, Databases, Security, Privacy, Identity and access management, Cryptography, Secret management, Network security, SAML, SCIM, OAuth, RBAC, Engineering management, Team leadership, System design, Implementation, Software development, AWS, Azure, GCP, Collaboration, Communication',\n",
       " 'SAP, Data Structure, Data Governance, Data Objects, Functional Design Documentation, Migration Strategy, Business Process, Test Scenarios, Root Cause Analysis, Data Material Training, Data Maintenance, Supply Chain Management, ERP Operations, Data Migration, Data Integration, Data Analysis, Production Planning, Change Control Process, Oral Communication, Written Communication, Interpersonal Communication, Investigation Skills, Problem Solving Skills, Project Management, Stakeholder Management, French, English',\n",
       " \"Python, AWS, Airflow, Snowflake, Hadoop, SQL, Scala, Spark, Postgres, Angular JS, NoSQL, UNIX/Linux, Agile, MapReduce, Hive, EMR, Kafka, Gurobi, MySQL, MongoDB, Cassandra, Redshift, Bash scripting, Data warehousing, Data modeling, Data integration, Data governance, Data quality, Data processing, Data visualization, Data analytics, Machine learning, Artificial intelligence, Realtime data analytics, Data security, Cloud computing, Distributed computing, People management, Team leadership, Mentoring, Crossteam collaboration, Problem solving, Communication, Critical thinking, Analytical skills, Attention to detail, Time management, Bachelor's Degree, Master's Degree, 6+ years of experience in application development, 2+ years of experience in big data technologies, 1+ year experience with cloud computing, 2+ years of people management experience, 7+ years of experience in application development, 4+ years of experience with a public cloud, 4+ years experience with Distributed data/computing tools, 4+ year experience working on realtime data and streaming applications, 4+ years of experience with NoSQL implementation, 4+ years of data warehousing experience, 4+ years of experience with UNIX/Linux including basic commands and shell scripting, 4+ years of experience with Agile engineering practices\",\n",
       " 'Configuration and Data Management, Business Unit Operations, Analytics, Finance, Economics, Engineering, Market Research, Tools and Analysis Development, Strategic Planning, Leadership, Communication, Team Collaboration, Adaptability, Problem Solving, Quantitative Skills, Attention to Detail, Data Management, Industry Landscape Analysis, Market Analysis, Financial Planning, Investment Analysis, Software, Systems Development, Data Quality, Data Traceability, Tools and Analysis Development, Data Visualization, Business Unit Policies, Programmatic Objectives, Software Development, Data Analytics, Market Intelligence, Financial Strategy, Market Trends',\n",
       " 'Data science, Statistical methods, Regression analysis, Stata, SAS, SPSS, Geographic Information Systems (GIS), Alteryx, SQL, Python, AI/Machine learning, Retail location analytics',\n",
       " 'SOC, SIEM, Splunk, EPP/EDR, NGFW/IPS, WAF, IDM/IAM, SWG/SASE, CASB, DLP, Zero Trust security architecture, Defense In Depth (DiD) model, Threat Intelligence (TI) systems, OSINT, Recorded Future, CPRA, CCPA, GDPR, NIST 80053, NIST 800171, Computer Science, Information Security/Data Systems Management, CISSP, CRISC, CISM, CISA, Excellent interpersonal skills, Fluent in English',\n",
       " \"MEP Commissioning Manager, Hyperscale Data Center Experience, MEP Systems, Construction Practices, Data Center Industry, QA/QC Processes, Commissioning Activities, Project Management, Level 15 Commissioning, Mechanical Engineering, Electrical Engineering, Construction Management, Safety Protocols, Communication, Collaboration, Flexibility, Time Management, Microsoft Office Suite, Bachelor's Degree\",\n",
       " 'Structural Engineering, Civil Engineering, Architecture, Expert Witness',\n",
       " 'Bayesian machine learning, Psychoacoustic modelling, TensorFlow, PyTorch, Programming skills, Data analytics, Preprocessing, Feature engineering, Computer science, Engineering, Deep learning',\n",
       " 'Biology, Microbiology, Geology, Laboratory work experience, Mold microscopy analysis, Asbestos microscopy analysis, LIMS (Laboratory Information Management System), Project management, Data analysis and interpretation, Data entry, Quality assurance and quality control, 401(k) with company match, Paid holidays and vacation, Personal days, Dental and vision options',\n",
       " 'Oracle Primavera P6 scheduling software, BIM models, CPM schedules, Lean (pull planning) methodologies, Linear schedules, Takt time analytics, Project Managers, Superintendents, Project Executives, CPM schedules, Historical production data, Engineering or Construction Management, AACE PSP certified certification, Ability to analyze and synthesize information, Communication skills, Customer service orientation, Interpersonal skills, Ability to travel',\n",
       " 'Data Management Systems, Data Architectures, Data Structures, Data Pipelines, Data Throughput Optimization, Query Performance Optimization, Database Technologies, SQL, NoSQL, HPC, Data Analysis, Operational Suitability Testing, Logistics, Maintenance, Space Systems, Ground Systems, Manufacturing, Mission Planning, Test Design, Test Planning, Test Analysis, Data Collection, Data Processing, Data Evaluation, Data Reporting, TS/SCI Clearance, Masters Degree, Bachelors Degree, 12 Years of Experience, 17 Years of Experience',\n",
       " 'Retail Sales, Customer Service, Hospitality, Technology, Communication, Sales, Learning, Adaptability, Flexibility, Second language, French, Learning, Inclusive work environment',\n",
       " 'Postgres SQL, GraphQL, Python, XML, Apache BigQuery, ACFC standards, Data Modeling, SSIS, Azure, Healthcare, Erwin, DDL scripts, Data flow, Data impact analysis, Project deliverables',\n",
       " 'Agile, AWS, Apache Spark, Big data, Cassandra, Cloud computing, Data warehousing, EMR, Gurobi, Hadoop, Hive, Java, Kafka, Linux, MapReduce, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, SQL, UNIX',\n",
       " \"Kinesis Data Streams, Realtime analytics, Cloud computing, Software engineering, Code review, Documentation, AWS, Agile development, Scrum, DevOps, Troubleshooting, Communication skills, Teamwork, Problemsolving, Analytical skills, Creativity, Initiative, Leadership, Flexibility, Adaptability, Bachelor's degree in computer science or equivalent, 5+ years of noninternship professional software development experience, 5+ years of programming experience, 5+ years of leading design or architecture experience, 5+ years of full software development life cycle experience\",\n",
       " 'School Psychologist, School Psychology, National credential, Weekly Direct Deposit, Weekly taxfree stipends, Medical benefits, Referral bonuses',\n",
       " 'Medical technology, Chemistry, Hematology, Microbiology, Laboratory practices, Clinical laboratory, CMS2226F, 42 CFR 493, Medicare, Medicaid, CLIA, Interpretive Guidelines for Laboratories, Appendix C, Subpart M, NAACLS, ACSP, MLS certification, American Society for Clinical Pathology',\n",
       " 'Customer Service, Data Entry, Data Capture, Data Management, Document Scanning, Document Archiving, Document Retrieval, Document Distribution, Kofax, Paper Flow, Google Docs, HighSpeed Scanner, Large Format Scanner, Booking Scanner Machine, Custom Program Tools',\n",
       " \"Business analysis, Strategic leadership, Business ownership, Product development, Marketing, Credit risk, Execution, Partnership, Analytic orientation, People leadership, Communication skills, Results orientation, Quantitative analysis, Qualitative analysis, Project management, Financial modeling, Economic forecasting, Bachelor's degree, Master's degree, Data strategy, Machine learning, AI, Technology platforms, Data management services, Data use, Horizontal services, Data privacy, External data sharing\",\n",
       " \"Enterprise Storage Engineer, SDS, SAN, Software Defined Storage Services, Ceph, MinIO, Swift, OpenStack, GlusterFS, Container Orchestration services, Docker, Kubernetes, Azure tools and services, Python, C#, SQL, NoSQL, Linux operating systems, Bash, Perl, Amazon S3, Fibre Channel, iSCSI, NFS, SMB/CIFS, VMware, HyperV, Active Directory, Windows Authentication, SAML, OAuth, C/C++, GO, AWS, GCP, Azure, Cloud Computing, Virtualization, Data Management, Data Storage Protocols, Data Protection, Disaster Recovery, Security, Linux, Scripting, Troubleshooting, Problem Solving, Analytical Skills, Communication Skills, Collaboration, Bachelor's degree in Computer Science or Information Systems, Certifications in cloud services softwaredefined storage technologies or related areas, Experience in complex storage solution environments, Experience in designing implementing and maintaining highavailability storage systems, Experience in implementing and managing cloud storage solutions, Experience with successful data migration projects, Experience in direct involvement in disaster recovery drills and actual recovery operations, Experience in managing and scaling out storage in a multitenant environment, Experience working with security protocols and products, Experience in a collaborative development environment, Scripting/coding experience in C/C++ Bash Python or GO, Experience in developing and maintaining large scale highavailability systems, Experience engaging with OpenSource communities, Technical leadership experience\",\n",
       " \"Data loss prevention, Data protection, Data security, Cybersecurity, Risk assessment, Cloud computing, Networking, Virtualization, Agile methodology, JIRA, Symantec Data Loss Prevention (DLP), Web proxy, Email security, Endpoint security, Security+, CISSP, GIAC, CISM, CCSP, CISA, AWS Cloud Practitioner, AWS Solution Architect  Associate, AWS Developer  Associate, AWS Security  Specialty, AWS Solution Architect  Professional, High School Diploma or equivalent, 4+ years in cybersecurity or IT, 3+ years in data protection, 1+ years in Symantec DLP infrastructure engineering, Bachelor's Degree in Cybersecurity Systems Engineering or Computer Science, 3+ years in scripting and solving cyber technical challenges, 3+ years in Agile delivery model, 3+ years in public cloud security and multicloud environments, 2+ years in IT Delivery projects and technical writing, 2+ years of handson JIRA experience, 2+ professional cybersecurity certifications, 1+ professional cloud certifications\",\n",
       " 'Data Analysis, Tableau, Metabase, Financial Crime Risk, Fraud Detection, Financial Crime Reporting, SQL, Python, ETL, Cloud Environment, dbt, Statistics, Business Intelligence, Leadership, Teamwork, Communication, Critical Thinking, Problem Solving, BSc in Math or Computer Science',\n",
       " 'Machine Learning, Data Analysis, Data Science, Big Data Technologies (AWS Hadoop Spark Pig Hive), Statistical Analysis, Optimization Methodologies, Discrete Optimization, Continuous Optimization, Algorithms, A/B Experimentation, Reporting, Dashboarding, Data Warehousing, Conversion Events, Campaign Dimension Ingestion, Attribution Processing',\n",
       " 'DataStage Architect, Data Modelling, ETL, Data Warehouse, Dimension Modeling, Star Schema, Snowflake Schema, Chat Data Modeling, ETL Design, ETL Development, IBM Infosphere DataStage, SQL, PLSQL, Unix, Linux, JIRA, Confluence, OnsiteOffshore Model, Offshore Team Leadership, Client Requirements Gathering, Data Model Creation, Source Target Mapping, ETL Architecture, Offshore Development Team Management, Legacy ETL Platform Understanding, Project Management, Stakeholder Engagement, Performance Management, Quality Management, Best Practices Implementation',\n",
       " 'Project management, Data mapping, GDPR compliance, Data privacy governance, Legal and regulatory knowledge, Risk assessment, Vendor management, Contract negotiation, Businesswide data mapping experience, Data transfer agreements, Policy drafting and maintenance, Breach reporting, Retention policy, Employee privacy notice, Security policy, Publicfacing privacy notices, Cookies notices, Standard/template processor and controller clauses, Commercial contract clauses, Proportionate view of requirements, Leadership and project management skills, Deliverables management, Internal steering group collaboration, Diversity and inclusion commitment, LGBTQ+ and disability considerations, Covering letter required',\n",
       " \"Data Engineering, SQL, ETL Solutions, Python, SAS, R, TensorFlow, Pytorch, OpenAI Technology, AI/ML Tools and Techniques, Government Contracting, Bachelor's Degree, Consulting, DataDriven Projects, Data Collation and Cleaning, Task Delegation, Performance Monitoring, Advanced Statistical Procedures, Model CrossValidation, NonTechnical Reporting, Business Strategy Input, Industry Knowledge and Research\",\n",
       " 'MLOps frameworks and concepts, Quality assurance with APIs, SQL, Python, Azure Portal, Function Apps, Databricks, Automation framework, Windows, Web Applications, Web Service environments, JIRA, HP ALM, HP LoadRunner, Selenium, ReadyAPI, SOAP UI, Azure SQL, HP Unified Functional Testing (UFT), Cypress, REST Assured frameworks, Agile, Computer science, IT software testing certification, IT Agile certification',\n",
       " 'Revenue cycle management, Data analysis, Data visualization, SQL, Python, R, Statistics, Epic Clarity reporting, Epic Revenue Cycle modules, Project management, Agile Development, Communication, Organizational skills, Analytical skills, Epic Revenue Cycle certification, Certified Health Data Analyst (CHDA) certification',\n",
       " 'Data engineering, Data pipeline, Data API, Data delivery, Python, SQL, Scala, Java, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, NoSQL, Mongo, Cassandra, UNIX/Linux, Agile',\n",
       " \"Data governance, Project management, Data governance frameworks, Written communication, Oral communication, Policy development, Contract management, Project leadership, Client management, Strategic thinking, Problem solving, Team leadership, Time management, Requirements analysis, Data quality management, Data security, Bachelor's degree, 8+ years of project management experience, 5+ years in data governance or related field\",\n",
       " 'Data Governance, Data Integration, Data Quality, Master Data Management, Data Models, Data Structures, Business Process Integration, Project Management, IT & Business Integration, Data Sourcing, Data Migration, Issue Resolution, Process Integration, Data Workflows, Data Ownership, Data Modelling, SAP ERP, SD/MM, MIS, STEM, Medical Dental and Vision Plans, 401k Match, Family Leave, PTO & Paid Holiday Schedule, Pet Legal and Life Insurance, Tuition Reimbursement',\n",
       " 'Data Engineering, Data Architecture, Data Pipelines, Data Visualization, Data Warehousing, Data Lakes, Data Mesh, Cloud Scale Analytics, Azure Data Bricks, Azure Synapse, Azure Data Services, Software Development Methodologies, Team Management, Project Management, Data Profiling, Data Cataloging, Data Mapping, Business Processing Mapping, Data Flows, Data Integration, System Integration, Machine Learning, AI, Microsoft Data and Analytics Technologies, Microsoft Azure',\n",
       " 'Customer Service, Wireless Technology, Microsoft Word, Microsoft Excel, QuickBase, Tier 2 Data Specialist, High School Education, Call Center Environment, 1year previous point of sale order entry system, 1 to 3 years supervisory experience, Excellent verbal and written communication, Interpersonal skills, Troubleshooting, ProblemSolving',\n",
       " 'MS Excel, Data analysis, Reporting, Target setting, School performance statistics, Timetable process, Data integrity, Data accuracy, Robust processes, Logical thinking, Analytical thinking, Attention to detail, Interpersonal skills, Flexibility',\n",
       " 'ML Ops, ML Systems, Workflow Orchestration, Data Engineering, ETL Pipelines, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Team Management, Mentorship, Product Ownership, Financial Workflows, Corporate Financial Data, Product EndtoEnd Ownership',\n",
       " 'Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Tableau, SQL, UNIX/Linux, Agile, Data visualizations',\n",
       " \"Data Engineering, Apache Spark, Production Level Data Pipelines, Data Lake Technology, Big Data Streaming, Big Data Ingestion and Workflows, Custom Proof of Concept Content, Load Performance Testing and Optimization, REST API, BI Tools, SQL Interfaces, Cloud Infrastructure and Services, AWS, Azure, GCP, Cloud Security and Networking, Industry Specific Data Analytics Use Cases, Production Programming Experience, Python, Scala, Hadoop, Kafka, CI/CD, Unit and Integration Testing, Automation and Orchestration, SQL, Jenkins, Bachelor's Degree in Computer Science, Information Systems, Engineering, Ability to Travel\",\n",
       " 'Windows, Microsoft Office, CERNER, eMR Scheduling systems, Time management, Problemsolving, Interpersonal skills, Communication skills, Professional development, Teamwork, Data management, Data integrity, Data analysis, Data reporting, Data visualization, Renal data, Health informatics',\n",
       " 'Geospatial Data & Analysis (GDA), Geographic Information Systems (GIS), GIS Professionals, Planning, Geography, Economics, Environmental Science, Environmental Studies, Mathematics, Computer Science, Statistics, Information Technology, SQL, Spatial Queries, Land Use Model Applications, Spatial Analysis, Cartography, Geospatial Data Management, ArcGIS Software, Jupyter Notebook, ArcGIS Online, SQL Server, Spatial Database Engine (SDE), ArcSDE, Geospatial Technology',\n",
       " 'Data extraction, Data transformation, Data analysis, Epic Clarity reporting, SQL, Statistical methods, Python, R, Agile development, Project management, Clinical experience, Observation, Problemsolving, Communication, Organization, Analytical skills, Technical skills',\n",
       " 'SQL, P21 ERP, Latitude Wireless Warehouse, WebQuery, EDI, SAGE, Company Intranet, SQL Server, HTML, CSS, PHP, Javascript, BI, Associates Degree in Computer/Information Technology, Knowledge of SQL Databases, Experience Supporting P21 or Other ERP Distribution Systems, Experience with SQL Server HTML CSS PHP and Javascript, Experience in BI',\n",
       " \"Computer Systems Analyst, Database Manager, Data Analytics, Power Apps, Business Dynamics, PowerBI, SQL, Data Science, Data Warehousing, Business Intelligence, EndtoEnd Data Architecture, Data Design Documentation, Data Models, Data Management Systems, Geographic Information System (GIS), Integrated Databases, Legacy Applications, Data Verification, Data Integration, Industry Standard Software, Mentoring, Field Readiness, Field Investigation, Remediation Activities, Bachelor's Degree in Information Technology, Information Systems, Database Development and Administration, MS Access, US Citizenship\",\n",
       " 'Data Security, Application Security, Software Development Lifecycle, Data Protection Laws, Security Architectures, Cloud Computing Security, Compliance Frameworks, Vulnerability Management, Data Encryption, Data Masking, Data Classification, Penetration Testing, CRISC, CISA, CISM, CISSP, SANS GIAC Security Certifications, ITIL, ISO27001, SQL, Python',\n",
       " 'Master Data Analyst, Master Data Management (MDM), MDM program functionality, Data Cleansing, Data Ingestion Pipelines, Business Process Workflow, Data Matching & Deduplication, Standardization, Technical Design, Functional Design, Process Design, Application Integration, Data Security, MDM Strategies, Deterministic Matching, Probabilistic Matching, Master Data Hierarchy Management, Data Enrichment, Data Profiling, Data Scorecard Development, Exception Management, Informatica MDM/DG, SAP MDM, Data Governance, Data Harmonization, Agile SDLC, Traditional SDLC, Business Communication, Analytical Skills, ProblemSolving Skills, SQL, Informatica, ETL Flows, Dun & Bradstreet, Data Quality Analysis, Data Transformation',\n",
       " 'Project Management, Process Management, Risk Management, Compliance, Legal, Regulatory, Operations, Business Process Management, Cloud Risk Management, PRIME system, Data model, Machine Learning, Generative AI, SaaS, CRISC, CISM, CRCM, CIPP, ABA Risk Management Certification, Change Management',\n",
       " \"Machine Learning, Model Development, Data Preparation, Feature Engineering, Algorithm Selection, Model Evaluation, Model Deployment, Python, TensorFlow, PyTorch, Deep Learning, Data Manipulation, Model Deployment, Docker, Kubernetes, Bachelor's Degree, 5+ Years Experience\",\n",
       " 'MLS Inventory, Auditing, Reporting, Internet, Email, Sales, Contact Management, Database Management',\n",
       " 'Data Modeling, Azure, Databricks, Data Factory, SQL, Python, Power BI, R, Microsoft Office Suite, Agile, Waterfall, Data Governance, Data Quality, Data Management, Data Policies, Business Process Management, Risk Management, Coaching, Mentoring, Communication, Teamwork, Problem Solving, Analytical Thinking, Organization, Customer Service, Software Development Best Practices, Cloud Technologies',\n",
       " 'Material management, Inventory management, Customer service, Assembly, Installation, Servers, Electronic components, Hand tools, Power tools, Measuring devices, Loading, Unloading, Inbound freight, Outbound freight, Vehicles, Containers, Shipping documents, Records, Quality checks, Part numbers, Quantities, Inventory cycle counts, Replenishment orders, Spare parts, Warehouse management systems, Server racks, Datacenter floor, Laser measuring devices, Electronic components, Cabling, Diagrams, Schematics, Rack hardware, Material handling equipment, Straddle stackers, Rack movers, Electric pallet jacks, Cleanup activities, Work area, Safety, Cleanliness, 5s standards, Critical thinking, Analytical skills, Decision making, Information analysis, Data analysis, Production goals, Computer experience, Electrical safety, Static discharge hazards, Protocols, Work experience',\n",
       " 'Terraform, Azure, Data Factory, Data Lake, Databricks, Microsoft Purview, Power BI, Hashicorp Terraform, GitOps, DevOps, Bash, Python, Java, Cloud logging, Monitoring, Observability, Computer Science, Leadership, Communication, Collaboration, Cloud Data Engineer, Financial services, Data governance, Compliance, Data controls, IaaS, PaaS, Monitoring, Logging, Observability, Cloud security, Subnets, Routing, Load balancing, Firewalls, IaC, CI/CD',\n",
       " 'Data Engineering, Big Data, Scalability, Cloud Computing, Python, SQL, Pandas, NumPy, Jupyter, IPython, Apache Spark, Hadoop, Hive, Pig, Cassandra, Kafka, Git, Docker, Kubernetes, Anodot, Machine Learning, Data Science, Data Analysis, Business Intelligence, Communication Skills, API Integration, Data Architecture, ETL, Data Warehousing, Data Lake, Data Mining, Data Visualization',\n",
       " \"Data Product Management, Agile Methodology, Personalization, Data Architecture, Data Engineering, Data Transformation, Cloud Computing, Artificial Intelligence, Data Management, Software Development, Visualization, Business Acumen, Data Warehousing, Data Analytics, Communication Skills, Strategic Thinking, Leadership, CrossFunctional Collaboration, Change Management, Business Partnership, Analytical Skills, Master's Degree, Fluent English, Work Experience (810 years), Global Distribution, Definition of Product Roadmaps, Release Management, KPI Definition, Data Privacy, Cybersecurity, Market Research, Customer Data, User Research, Competitive Analysis\",\n",
       " 'Data Vault, Data modeling, Data Storage, Relational databases, Snowflake, ODS, Data marts, Data lakes, RDBMS, Metadata management, Data architecture, Conceptual data modeling, Logical data modeling, Physical data modeling, Handson modeling, Design, Configuration, Installation, Performance tuning, Leadership skills, Stakeholder management, Communication, Collaboration',\n",
       " 'Data Stewardship, Data Management, Project Management, Team Building, Conflict Management, Interpersonal Skills, Multitasking, Attention to Detail, Data Warehousing, Institutional Data Strategy, Accreditation Compliance, Strategic Planning, Data Governance, Data Quality Management, Data Security, Data Privacy, Data Analysis, Data Visualization, Data Mining, Data Modeling, Data Integration, Data Extraction, Data Transformation, Data Cleansing, Data Validation, Data Warehousing, Data Lakes, Data Pipelines, Data Governance Tools, Data Quality Tools, Data Security Tools, Data Privacy Tools, Data Analysis Tools, Data Visualization Tools, Data Mining Tools, Data Modeling Tools, Data Integration Tools, Data Extraction Tools, Data Transformation Tools, Data Cleansing Tools, Data Validation Tools',\n",
       " 'AI/ML Product Leader, Engineering, Software Product, Product Management, Applied Science, Multilevel team leadership, Microservices, Cloud platforms, Computer Science, Data Science, Consumer Algorithms, Content Discovery, Experimentation, Retention and Growth, Leadership',\n",
       " 'Machine Learning, Natural Language Processing, Document Entity Extraction, Reading Comprehension, Computer Vision, Robotic Process Automation, Object Detection, Optical Character Recognition, Data Science, Data Analysis, Data Mining, Data Visualization, Software Development, Python, Tensorflow, Kubeflow, PyTorch, FastAPI, Kubernetes, Cloud Computing, Google Cloud Platform, Microservices, Model Deployment, Model Maintenance, Model Scaling, Business Requirements Translation, Machine Learning Model Specifications, Model Prototyping, Opensourced Code, Model Architecture Design, Model Training, Realworld Data Experience, Structured Data, Unstructured Data, Numpy, Pandas, Scikitlearn, Deep Learning Package, Production Model Deployment',\n",
       " 'Scheduling, Troubleshooting, Project Management, Data Collection, Microsoft Excel, Technical Support, Resource Allocation, Resolving Issues, Interpersonal Communication, Data Entry, Critical Thinking, Multitasking, Process Improvement, Documentation, Customer Support, Crossfunctional Collaboration, User Experience, Outreach, Work Ethic, Organization, Attention to Detail, Time Management, Task Management, Lab Space Utilization, Device Setup, Data Collection, Issue Documentation, Study Area Setup/Breakdown, Device Inventory Management, Participant Engagement, Protocol Compliance, Device Troubleshooting, Participant Scheduling/Recruitment, Instructional Material Development, Crossfunctional Team Communication, Data Fidelity Validation, Study Issue Raising, User Study Data Collection Support',\n",
       " 'Prototyping, Build, Test, Implementation, Application Support, Data Management tooling, Atacama (Data Quality), ILM (Archiving), Collibra (Meta data management), Google Dataplex, Azure Purview, Metadata Solutions, DevOps processes and tooling, GitHub, Jenkins, Spinnaker, Leadership, Line management, Agile, DevOps, JIRA, Confluence, SAFe, Programming languages, Objectoriented programming, Solution management tooling, Cloud Native solutions, Google Cloud Platform (GCP), Azure',\n",
       " \"MySQL, Oracle, SQL Server, Database design, Data modeling, Performance tuning, Database security, Data encryption, Backup and recovery, Database monitoring, Optimization, Analytical and problemsolving skills, Troubleshooting skills, Communication skills, Interpersonal skills, Continuous improvement, Bachelor's degree in relevant major, MySQL Database Administration or Developer, Oracle Certified Professional, 9 years of experience as a Database Engineer or Administrator\",\n",
       " 'Database Schema Design, SQL Development, Oracle Database Tuning, Oracle Database Administration, Oracle Data Masking, Oracle DataGuard Replication, Cúram Software, Golden Gate, AWS Services (EC2 RDS S3), Shell Scripting, PL/SQL, System Security Maintenance, Performance Monitoring, Backup/Restore Services, Disaster Recovery (DR) Solutions, Database Software Installation and Upgrades, Analytical Skills, Technical Skills, Time Management Skills, Communication Skills, OnCall Rotation and Weekend Work, Advanced Security',\n",
       " 'Data Architecture, Data Engineering, Analytics, Cloud Native Computing, Hybrid MultiCloud, Data Governance, Data Domain Model, APICentric Design, DataOps, Data Automation, Artificial Intelligence, Machine Learning, Natural Language Processing, Computer Vision, Zero Trust, Domain Driven Data Architecture, Scalable Data Pipelines, CI/CD, SQL/NoSQL, FAIR Data Principles, Microsoft Azure, Google Cloud, Azure Storage, Azure Search, Azure Catalog, API Management, Data Processing, Data Analytics, Azure Data Factory, Data Bricks, Azure Synapse, PowerBI, Data Mesh, Data Fabric, Lakehouse, Vendor Management, Negotiation, Life Sciences',\n",
       " 'Medical Lab Technician, Medical Technologist, Clinical Laboratory Assays, Patient Data Entry and Verification, Reporting Assay Results, Quality Control Activities, Proficiency Testing Activities, Test Results Evaluation, Independent Judgment, Technical Communication, Integrity, Good Judgment, Physical and Emotional WellBeing, Salary of $65000, Benefits Package, FullTime Position',\n",
       " 'Data Analysis, SQL, Microsoft Access, Relational Databases, Data Reporting Tools, Statistics, Mathematical Models, Medical and Pharmacy Claims Data, Health Insurance Business, Claims Payment Procedures, Strategies and Trends in Health Care Government Programs, Business Analytics, Data Warehouse, Coding, ETL Experience, Encounters Experience',\n",
       " \"Medical Laboratory Technician, Medical Technologist I, Medical Technologist II, MLT ASCP certification, MT or MLS (ASCP) certification, Clinical experience, Associate degree, Bachelor's degree, Compassion, Professionalism, Positive outlook, Strong sense of advocacy, Collaborative, Ability to work in a constant state of alertness and in a safe manner\",\n",
       " 'Data Analysis, Tableau, Splunk, SQL, R, Python, Java, TS/SCI Clearance, Polygraph, Data Support, Dashboards, Metrics, Reports, Queries, Customer Metrics',\n",
       " 'Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Machine learning, Distributed microservices, Cloudbased data warehousing, Unit testing, Agile engineering practices, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Shell scripting, SQL',\n",
       " 'Software engineering, Data engineering, Data modeling, Python, PySpark, Pandas, NumPy, Looker, Thoughtspot, Tableau, Quik Sense, Agile methodologies, AWS cloud native, Big data technologies, Hive, Dremio, Impala, AI, Large Language Models, SQL, Linux, Docker, Jenkins, Git, Jira, Confluence, AWS, Azure, GCP, Hadoop, Spark, Kafka, Flink, Data analysis, Data visualization, Data warehousing, Data mining, Machine learning, Deep learning, Natural language processing, Computer vision, Recommendation systems, Fraud detection, Risk management, Cybersecurity, Business intelligence, Data governance, Data quality, Data security, Data privacy',\n",
       " 'Greenplum, Data warehousing, ETL processes, Data migration, Cloudbased data platforms, Database administration, Data transformation, SQL, Python, Project management, Data assessment, Data modeling, Data retrieval, Communication, Scalability, Performance optimization, Relational database management, Snowflake',\n",
       " \"Ontology, Data Modeling, Business Strategy, Data Standards, Semantic Technology, Knowledge Organization, RDF, RDFS, OWL, SKOS, SHACL, JSON, XML, SPARQL, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, SQL, Graph Databases, Python, R, OpenAPI/YAML, AVRO, Agile, Amazon Web Services, ProblemSolving, Project Management, Communication Skills, Teamwork, Bachelor's Degree, Master's Degree, PhD, 6+ Years Experience in Metadata Field, 6+ Years Experience with W3C Standards, 4+ Years Experience with Ontology and Linked Data Tools, 4+ Years Experience with SQL or SPARQL\",\n",
       " 'Data Interpretation, Statistical Analysis, Data Scrubbing, Testing Scripts, Data Quality Control, Project Management, Procurement, Construction, DOE Orders Compliance, Safety Programs, Environmental Law, Legal Concepts, Business Concepts, Power BI, Primavera (P6), Deltek Cobra, Associate Degree, Bachelor of Science, U.S. Citizenship, Q Clearance, Clear Understanding of Work Hazards, DOE Security Clearance',\n",
       " 'data analysis, data analytics, data representation, data mining, data science, data visualization, databases, business intelligence (BI), business analysis, SQL, Python, R programming, Tableau, statistical analysis, data warehousing, machine learning, computer science, statistics, MS Power BI, BI Tools, R studio, data modeling, analytical skills',\n",
       " 'Data analysis, Data cleansing, Data structuring, Predictive modeling, Machine learning, Clustering, Classification, Python, C, C++, Java, SQL, Cassandra, Hadoop, Spark, Tableau, Statistics, Mathematics, Computer science, Engineering',\n",
       " \"LIS, Medical Technology, CAP, CLSI, Clinical Laboratory Standards, Laboratory Information Systems, Phlebotomy, CPR, Specimen Processing, ASCP Certification, Mississippi Driver's License, Communication Skills, Customer Service Skills, Team Work, Regulatory Compliance, Quality Control, Data Entry, Documentation, Recordkeeping, Laboratory Procedures, Biohazardous Materials Handling, Medical Terminology, Safety Protocols\",\n",
       " 'Data Governance, Data Management, Data Security, Data Quality, Master Data Management, Data Retention, Data Glossary, SelfService Reporting, Communication Skills, Interpersonal Skills, Analytical Skills, Project Management, RACI, Data Catalog, Data Privacy, Data Warehousing, Data Modeling, SQL, Python, Tableau, Power BI, Microsoft Office Suite, Data Visualization, Data Analytics, Data Integration, Data Mining, Machine Learning, Artificial Intelligence, Cloud Computing, Agile Methodology, Scrum, Kanban, Jira, Confluence, Trello, Asana, Slack, Microsoft Teams',\n",
       " 'Data Steward, Data Quality, Data Integrity, Data Accuracy, Master Data, Data Governance, Data Audits, Data Change Management, Process Improvement, Data Reporting, Troubleshooting, Risk Management, Compliance, Data Visualization, PowerBI, Qlik Sense, SAP, LIMS, Radiopharmaceutical, Aseptic Manufacturing, D365, Information Technology, Computer Science, Management Information',\n",
       " 'API Testing, Postman, RESTful APIs, Response Validation, Status Code Validation, API Performance Testing, Test Suite Development, Database Testing, MongoDB, SQL Server, Data Integrity Validation, Data Format Validation, Query Output Validation, SQL Query Writing, Kafka Monitoring, Kafka Testing, KPow, Kafka Cluster Management, Topic Inspection, Message Queuing, Streaming, eCommerce Architecture, eCommerce Platform Testing, Scalability, Security, Agile/Scrum, Test Plan Creation, Crossfunctional Collaboration, ProblemSolving, Analytical Skills, Communication, Collaboration, Computer Science, Information Technology, Automation Testing, Agile/Scrum Methodologies',\n",
       " 'Data Architecture, AWS Cloud, Data Products, Enterprise Data Governance, Software Architecture, Data Modeling, Data Quality, Data Privacy, Data Security, Database Technology, Cloud Native Data Solutions, Data Engineering, Agile Practices, Target Architectures, AWS Solution Architect, AWS Certified Data Analytics, Python, Java, Scala, Node.js',\n",
       " 'IBM InfoSphere/DataStage, Data analysis, Data modeling, Data Warehousing, Oracle PL/SQL, MS SQL server development, Business applications integration, Agile/Scrum process, Web service development, IIB, ADF, Communication skills, Clear writing skills, Clear speaking skills',\n",
       " 'Laboratory testing, Quality control, LIS procedures, Specimen acquisition, Critical values, Delta checks, Reagents, Calibration, CAP proficiency testing, Biochemistry, Biology, Chemistry, Medical technology, Microbiology, Molecular biology, Genetics, Healthrelated science, MT (ASCP) certification, CLS (NCA) certification, CLIA 493.1489 standards, Interpersonal skills, Communication skills, MLT (ASCP) certification, CLT (NCA) certification',\n",
       " 'Talent acquisition, Engineering sourcing, Data science sourcing, Sourcing strategy, Boolean search, Recruitment, Candidate screening, Candidate relationship management, Market research, Candidate pipeline management, Hiring management, Software engineering concepts, Data science concepts, Industry trends, Proficient sourcing techniques, Datadriven decision making, Crossfunctional collaboration, Growth mindset, Open and collaborative culture, Learning and development budget, Competitive financial package, Equity, Health insurance, Mental health support platforms',\n",
       " 'Computer Science, Information Technology, Cyber Security, Security Operations Center, DLP, CrowdStrike, MDE, EDR, Mobile device policy governance, Endpoint DLP, Classification, M365 Endpoint DLP, Data detection DLP, Labeling policies, M365 DLP solution, False positive analysis, DLP policy refinement, Hybrid model, Security Operations Centre, Cybersecurity, Analytical skills, Problemsolving skills, Oral and written communication skills, French, English, Analytical skills, Positive attitude, Team spirit, Eagerness to learn, Collaboration, Stakeholder influence, Customeroriented approach, Endpoint security, DLP policies, RegEx patterns, Keywords, Rule sets, Policy violations, Enduser management notification, Reports, Subject matter experts, Escalated issues, Problem management, Oncall support, 24x7x365 coverage, Core infrastructure, Customer infrastructure, Support cases, Internal audits, External audits, Metrics, Reports, Dashboards, Information Technology, Information security management principles, Information security practices, Ethical principles, Business security ethics, Information security ethics, Azure Certification, SC100, SC900, AZ400, CrowdStrike, Security+, CEH, GISP, McAfee, Symantec, Palo Alto, MIP, Titus, Forcepoint, Confluence / JIRA, ServiceNow, Microsoft PowerApps, PowerBi, Power Automate',\n",
       " 'Epic Clarity, Clinical modules, SQL, Analytic methods, Statistical methods, Data analysis, Data visualization, Healthcare data, Clinical operations, Clinical outcomes, Epic Clarity Data Model, Epic Clinical Data Model, Certified Health Data Analyst, Project management, Agile Development, Python, R, Data manipulation, Data analysis, Clinical experience, EHR, Observation, Experimentation, Usability testing, Strategy documents, Data dictionaries',\n",
       " 'Medical Technologist (MT MLS MLT CLS), Baccalaureate Degree, Microbiology, Biology, Chemistry, Molecular Sciences, Collaborative Team Environment, Cepheid Infinity, Hologic Panther, Cobas 6800, Biofire Film Array, Genmark ePlex, BD MAX, Meridian Alethia, Copan WASPLAB, Artificial Intelligence (AI), Interpretive Algorithmic Software, Bruker Malditof, Beckman Coulter MicroScan Walkaway, Biomerieux Vitek, NAACLS Approved Medical Technology Program, ASCP Certification, CLIA ’88 Equivalent, HEW/HHS Certification, Registry Eligible, Section Certification, Minimum FiveYears Technical Experience',\n",
       " 'Kubernetes, Istio, Kong, Kafka, Flink, Spark, Infrastructureascode (IAC), VictoriaMetrics, Prometheus, Grafana, ElasticSearch, Kibana, Fluentd, Linux, Networking, Production systems, Terraform, Packer, Chef, Ansible, Scripting, Programming, US citizenship (preferred), Federal customer infrastructure experience (nice to have)',\n",
       " 'Electrical Engineering, Construction Management, LEED Certification, OSHA 30 Certification, First Aid / CPR / AED, Project Planning, Subcontractor Management, Scheduling, Safety Management, Quality Control, Cost Management, Risk Management, Submittal Process, Closeout Documentation, Work Procurement, Electrical Trade/Technical Experience, Private and Government Documentation Requirements, Data Center Experience, Basic Understanding of LEEDS, Project Plans Specifications',\n",
       " \"Risk Management, Compliance, Audit, Leadership, Management, Problem Solving, Negotiation, Communication, Microsoft Office, Applicable Professional Certifications, Bachelor's or Advanced Degree, Applicable Laws and Regulations, Financial Services, Regulatory Trends, Thorough knowledge of Risk/Compliance/Audit, Strong Leadership and Management, Effective written and verbal communication, Analytical and problemsolving skills, Proficient Computer Skills, Stress management, Staffing, Performance Management, Guidance and Training\",\n",
       " \"Gameday Compliance, Basic Agreement, Major League Rules, Major League Baseball Regulations, Commissioner's Office, Sign Stealing, Electronic Devices, Baseball, Equipment, Video Hardware, Replay Software, Clubhouse Atmosphere, Major League Baseball Personnel\",\n",
       " 'Medical Technologist, CLS, MLT, Laboratory Services, Diagnostic Testing Procedures, Moderate and High Complexity Laboratory Health Care, Outpatient and Inpatient Services, Emergency Room Services, Obstetrical Services, Behavioral Health, Surgery, Mobile Units, Outlying Clinics, Health Stations, Levey Jennings Criteria, Quality Control System, Medicare/Medicaid, HIPAA, JCAHO Standards, Blood Bank and Transfusion Medicine, ABO/RH, Antibody Screen, Crossmatch, Blood Release Protocols, Beckman Coulter DXH 900, Ortho Vitros 5600, Siemens EXL 200, ACL Top 350, Beckman Coulter IRIS, Abbott ID Now, Sysmex XN450, ID Microtyping System Inc [MTS Gel for blood bank], ABL 90 Blood Gas, ISTAT, Waived Serology, Miscellaneous Equipment, CPR, BLS, ASCP, Computer Operations, Word Processing, Email, Electronic Medical Records, Laboratory Information Systems, Cultural Sensitivity',\n",
       " \"Data Engineering, ETL/ELT processes and pipelines, SQL, Python, PowerShell, Database systems, Data structures, Algorithms, API integration, Tableau Desktop, Tableau Server, Data warehousing, Data integration, Data lake, Reporting, Analytics, MSSQL Server/Azure, Snowflake, AWS, Business intelligent implementation methodologies, Analytical & troubleshooting and remediation capabilities, Complex problem solving, Technical specifications translation, Bachelor's degree in Economics Business Mathematics or Statistics, 4+ years of experience in data solutions, Software development experience\",\n",
       " 'Software Development, Autonomous Driving, Artificial Intelligence, Computer Science, Distributed Systems, Embedded Systems, Operating Systems, Data Engineering, Big Data, Machine Learning, Cloud Computing, Apache Spark, Compliers, Core Competency, Systems Engineering, Faulttolerant Runtime Systems, Distributed Databases, DomainSpecific Languages, Model Training Systems, Performance Optimization, Device Drivers, Secure Data Transfer, BS/MS/PhD in Computer Science',\n",
       " 'Software Development, Data Engineering, Python, AWS, Lambda, S3, Glue, Athena, Data Modeling, ETL, Data Warehousing, Git, Agile Development, Problem Solving, Collaboration, Financial Data, Industry Regulations, Serverless Architecture, Infrastructure as Code (IaC), SaaS, Data Pipeline',\n",
       " 'Data Science, Data Analytics, Informatics, Statistics, TS/SCI Clearance, SAS, R, Java, C, MATLAB, ScaLa, Python, GPU architectures, SQL, Tradecraft, Publication, Pandas, Scikit, TensorFlow, Gensim, Army structure, Defense level intelligence, Intelligence collection, Fusion, Analysis, Production, Dissemination, Intelligence databases, Intelligence products, National System for GEOINT (NSG), Intelligence Community, Private sector data science/analytics, Machine learning, Data visualization',\n",
       " 'Business Development, Project Management, BIM, Commercial, Fabrication, Operations, M&E Solutions, Prefabrication, Mechanical and Electrical Services, PreConstruction, Building Services, Mission Critical, Client Engagement, Tendering, Negotiation, Sales, Client Meetings, Site Visits, Proposal Preparation, Scope Management, Team Collaboration, Communication, Leadership, Data Centre Expertise, Industry Knowledge, Competitor Analysis, Pipeline Management, Pricing, Quoting, Negotiation, Handover, Training, Development, Pension Scheme, Health Insurance, Annual Leave, Social Events, Parking, Refreshments',\n",
       " 'Data Architecture, Data Strategy, Structured Query Language (SQL), Reporting Technologies (Excel SQL SSRS Tableau), Project Management, Database Software, Spreadsheets, Word Processors, Internet Software, Report Development, Integration Technologies, Reporting Technologies, Clinical Care, Finance, Human Capital, Project/Program Planning, Domainrelated Information Technology Solutions, Application Architecture, Infrastructure Architecture, Operations Architecture, Communication, Presentation Skills, Meditech 6x, Revenue Cycle Management',\n",
       " \"Python, Scala, Java, Airflow, Hive, Spark, Kafka, EMR, Data Engineering, DataOps, Machine Learning, Analytics, Data pipelines, Data validation, Testing, Agile, Collaboration, Interpersonal skills, Mentoring, Coaching, Onboarding, Leadership, Problemsolving, Communication, Bachelor's degree, Master's degree, Computer Science, Software Engineering\",\n",
       " 'Mechanical, Electrical, Plumbing, HVAC, Low Voltage, Fire Sprinkler, Microsoft Office Suite, Project management software, Scheduling software, LEED accreditation, Leadership and interpersonal skills, Strong oral and written communication skills, Knowledge of project management and scheduling software, Ability to implement leading edge technologies, Construction, Engineering, Design, Cost, Sequence, Logistics, Field installations, Contract documents, Plans, Specifications, Applicable codes, Procurement, Budgets, Quality, Safety, Coordination, Scheduling, Installation, Commissioning, Closeout processes, Physical abilities, Vision abilities, Exposure to moving mechanical parts, Exposure to airborne particles or caustic chemicals',\n",
       " 'Data Science, AntiMoney Laundering, Data Analytics, Machine Learning, Risk Management, Data Quality, Data Governance, Reporting Design, Architecture, Implementation, Communication, Client Service, Teamwork, Problem Solving, Analytical Thinking, Critical Thinking, SQL, Hadoop, Spark, R, Python, Scala, AI/Machine Learning Techniques, Algorithms, Data Processing, ETL, Data Quality, Data Management, Data Integration, Data Warehousing, Master Data Management, Data Rationalization, Data Migration, Regulatory Requirements, Guidelines, AML Software, Other DB Technology, ETL Packages, Microsoft Azure, Azure DevOps, Fraud, Business Intelligence, Solution Architecture, Business Analysis, Systems Analysis',\n",
       " 'School Nursing, Healthcare Program, Data Analysis, OSHA Compliance, 504 Coordinator, Records Management, Medication Monitoring, Nursing Services, First Aid, Health Education, Communication, Confidentiality, Special Education, Multidisciplinary Teams, Individual Education Plans, Nursing Interventions',\n",
       " 'Software Development, Data Acquisition, Web Crawling, Data Ingestion, Data Processing, Distributed Systems, Data Indexing, Data Search, KeyValue Databases, Data Synchronization, Kubernetes, InfrastructureasCode, System Checks, Data Analysis, Computer Science, Software Engineering, Large Web Crawlers, Stateful Distributed Systems, Communication Skills',\n",
       " 'Machine Learning, Data Science, Python, Scala, Java, Distributed Computing, Data Pipelines, Cloud Platforms, Model Selection, Data Gathering, Feature Selection, Model Deployment, Continuous Integration, Model Governance, Responsible AI, Data Analysis, Software Development, Automation, Public Cloud, Data Engineering',\n",
       " 'Medical Laboratory Technology, Blood testing, Tissue testing, Body specimens testing, Patient safety, Clinical laboratory equipment, Quality control, Equipment checks, Temperature monitoring, Troubleshooting, Associates of Science, College or university accreditation, Medical Lab Technician Program, U.S. military medical laboratory training, MLT Medical Laboratory Technician Certification, ASCP certification, Technical experience, Analytical skills, Attention to detail, Problemsolving skills, Communication skills, Teamwork skills, Professionalism',\n",
       " \"SQL, Tableau, Data projects, GovCon, Bachelor's Degree, File manipulation, Database indexing, Quality checks, Methodologies, Procedures, COTS, Litigation support, Case information, Records, Productions, Interagency, Thirdparty, Databases, Business requirements, Interviews, Group workshops, Business process review, Surveys, Functional requirements, Enduser requirements, Data Analytics, Graduate degree, Computer Science, Statistics, Informatics, Information Systems, Mathematics, Quantitative field, Database, BigQuery, MySQL, SQL Server, PostgreSQL, Data pipeline, Workflow management tools, Airflow, Finance, Accounting, Data warehousing, Data mining, Cloud computing, GCP, UNIX, Crossfunctional teams, Dynamic environment, Communication skills\",\n",
       " \"Master's degree, 10+ years work experience, Secret security clearance, Computer systems, Windowsbased personal computers, Microsoft Office, Electrooptical/infrared (EO/IR) jammers, Expendables, Threat analysis, Airborne EW computer software, Avionics, Systems integration, Electronics engineering, RF integration, EM spectrum management, Antenna design, Digital signal processing, EW weapons systems development, Test and evaluation, Systems engineering, Mission Data (MD), EW system programming/reprogramming, Validation and verification test plans, Configuration control boards, Data analysis, Posttest analysis\",\n",
       " 'Medical Laboratory Technician, Blood Bank, Chemistry, Hematology, Serology, Urinalysis, Coagulation, Phlebotomy, Specimen collection, ASCP certification, AMT certification, Federal regulations, State regulations, Local regulations, HIPAA, CLIA, CAP',\n",
       " 'Data Management Analyst, Big Data Engineering, Azure Big Data technologies, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Data Lake Storage, Python, Scala, Java, Data Modeling, ETL processes, Data Warehousing, Financial Data, Financial Services, ProblemSolving Skills, FastPaced Environment, Collaborative Environment, Azure Certifications, Microsoft Certified: Azure Data Engineer Associate',\n",
       " 'Python, AWS, Airflow, Snowflake, Big data technologies, Public cloud (AWS Microsoft Azure Google Cloud), Distributed data/computing tools (MapReduce Hadoop Hive EMR Kafka Spark Gurobi MySQL), Realtime data and streaming applications, NoSQL databases (Mongo Cassandra), Data warehousing experience (Redshift or Snowflake), UNIX/Linux and shell scripting, Agile engineering practices, Technical leadership, Mentoring, Collaboration, People management',\n",
       " \"Data Center Operations Management, Team Leadership, Strategic Planning, Risk Management, Security and Compliance, Vendor Management, Budget Management, Performance Monitoring, Disaster Recovery Planning, Environmental Sustainability, Leadership, Communication, Analytical Thinking, Problem Solving, Server Technologies, Networking, Devops, Hybrid Cloud Architecture, Observability, Monitoring, Virtualization, Decision Making, Certifcations, Bachelor's or Master's degree\",\n",
       " \"DQLabs, Data quality tools, Pilot testing, Data quality processes, Data quality framework, Financial datasets, Scripting languages, Coding languages, Data quality automation, Data quality checks, Data quality validations, Data quality requirements, Data quality concepts, Data quality regulations, Financial data concepts, Regulations, Industry best practices, Python, SQL, Analytical skills, Problemsolving skills, Attention to detail, Communication skills, Collaboration skills, Financial context, Bachelor's degree, Computer science, Information systems, Finance, Data quality engineer\",\n",
       " \"Product Management, Human Centered, Business Focused, Technology Driven, Transformational Leadership, Data Science, Software Engineering, Bachelor's Degree, Master's Degree, MBA, Agile, Scrum, UX/UI, Technical Writing, Communication, Leadership, Teamwork, Problem Solving, Decision Making, Critical Thinking, Analytical Skills, Research, Innovation, Execution, Customer Focus, Data Analysis, Metrics, Performance, Security, Resilience, Scalability, Extensibility, Maintainability, Reliability, Availability, Documentation, Testing, Deployment, Monitoring, Maintenance, Support, Continuous Delivery, Continuous Integration\",\n",
       " 'Data Analytics, Data Reporting, Data Analysis, Internal Audits, Project Management, Data Mining, SQL, OBIEE, Microsoft Vizio, R, Advanced Microsoft Excel, Microsoft Office Suite, Written and Verbal Communication Skills, Quantitative Analysis, Qualitative Analysis, Trend Analysis, Data Quality Assurance, Data Integrity, Business Intelligence, Data Visualization, Program Evaluation, Fraud Detection, Internal Controls, Research Methods, Statistical Analysis, Risk Management, Reporting, Presentations',\n",
       " 'Specialist Solutions Architect, Data Warehousing, Data Governance, Databricks, Unity Catalog, Delta Sharing, Apache Spark, Data Warehousing workloads, Performance tuning, Data modeling, Testing, Optimization, Data management, Cloud platforms, Architecture, Proof of concept, Workload sizing, Custom architectures, Tutorials, Training, Hackathons, Conference presentations, Query tuning, Troubleshooting, Debugging, MPP data warehouses, NoSQL, MPP, OLTP, OLAP, Delta Lake, AWS, Azure, GCP, SQL, Python, Scala, Big Data technologies, Spark, Hadoop, Kafka, Presales, Postsales, Technical training, Computer Science, Information Systems, Engineering, Travel, Medical, Dental, Vision, 401(k) Plan, FSA, HSA, Commuter Benefit Plans, Equity Awards, Flexible Time Off, Paid Parental Leave, Family Planning, Fitness Reimbursement, Annual Career Development Fund, Home Office/Work Headphones Reimbursement, Employee Assistance Program (EAP), Business Travel Accident Insurance, Mental Wellness Resources',\n",
       " 'Data engineering, Platform engineering, SQL, Python, Airflow, Tableau, Statistical analysis, Statistics, Data pipelines, Machine learning, Predictive modeling, Data quality, Data visualization, Software design, Database design, Business needs translation, Communication, Collaboration, Teamwork, Root cause analysis, Manufacturing Execution Systems',\n",
       " 'SAP SD Pricing/Billing, SAP centric solutions, SAP Sales and Logistics support, Business process analysis and design, Solution architecture, Stakeholder relationship management, Business requirements gathering and documentation, Test plan development and execution, Linux commands, Linux OS installation, Network troubleshooting, Cabling, Lab support experience, Hardware prototype support/debug, Liquid cooling experience, Attention to detail, Independent thinking, Problemsolving, Data center experience, Power Space Cooling Equipment, Server hardware debugging, Server/desktop build, Data Center rack and stack, Break fix on system, System parts knowledge, Troubleshooting, Soldering, Fixing, Testing DC components, Inventory experience',\n",
       " 'SDA, XMD, USSF, UDL, Space Systems, Cross Mission Data, Space Domain Awareness, Program Management, Data Sharing, Agile Systems, Confluence, JIRA, DoD, Intelligence Community, CrossDomain Solutions, Cloud Architectures, CyberSecurity, Data Lake Architectures, PMP, DAU, DAWIA, DoD Adaptive Acquisition Framework, RFI, RFP, PWS, SOW, MultiSource Data Integration, International Acquisition, Contractual Instruments, Interagency Agreements, Foreign Military Sales',\n",
       " 'Test Automation, Test Design, Test Development, Jenkins, Ansible, Docker, Kubernetes, YAML, Unit Testing, Integration Testing, Functional Testing, Microservices, Software Development Lifecycle (SDLC), Microsoft Office Suite (Word Excel PowerPoint), Communication Skills, Collaboration Skills, Problem Solving Skills, ISTQB Certified Foundation Level, Accessibility Testing, Mobile Testing',\n",
       " 'IT Project Management, Data Center Build, Network Infrastructure, Cisco Technologies, Arista Technologies, Project Lifecycle Management, Requirements Gathering, Deployment Planning, Network Refresh, Leadership, Change Management, Hardware Management, Vendor Management, Microsoft Word, PowerPoint, SharePoint, Visio, Project, Excel, ServiceNow, Dark Fiber, WAN, LAN, Data Center Builds, Consolidations, Decommissions, Branch Restacks, Renovations, Office Builds',\n",
       " \"Senior Project Manager, Project management, Data Center construction, Cost control, Primavera, Microsoft Project, Engineering, Construction, Architecture, Bachelor's degree, Estimating, Microsoft Office, Communication skills, Attention to detail, Safety controls, Quality control\",\n",
       " 'Financial Intelligence Unit (FIU), Bank Secrecy Act (BSA), AntiMoney Laundering (AML), Office of Foreign Assets Control (OFAC), Financial crimes, Regulatory compliance, Suspicious Activity Reports (SAR), Know Your Customer (KYC), Enhanced Due Diligence (EDD), Case investigation, Financial analysis, Risk assessment, Money laundering, Financial sanctions, Customer account management, Data analysis, Report writing, Communication, Teamwork, Computer proficiency, Microsoft Office, G Suite, Certified AntiMoney Laundering Specialist (CAMS), FinTech',\n",
       " 'Data Analysis, Data Interpretation, Data Mining, Predictive Modeling, Statistical Techniques, Business Analytics, Data Visualization, Data Reporting, Data Management, Data Quality Assurance, Data Governance, Data Compliance, KPI Development, Performance Measurement, Process Improvement, Process Optimization, Stakeholder Collaboration, Python, R, SQL, Tableau, Power BI, Data Science, Business Intelligence, Machine Learning, Artificial Intelligence, Augmented Reality, Sensor Science, Signal Processing, Data Fusion',\n",
       " 'Data Center Network Management, Network Design and Architecture, Network Implementation and Deployment, Network Security, Network Monitoring and Maintenance, Capacity Planning, Network Documentation, Team Leadership, Network Protocols, Hardware and Software, Cisco and Juniper Technologies, Network Security Practices and Tools, Data Center Operations and Infrastructure, Leadership Communication and Interpersonal Skills, Cisco Certified Network Professional (CCNP), Cisco Certified Internetwork Expert (CCIE)',\n",
       " 'Data Science, Analytics, Data Structures, Machine Learning, Python, SQL, Natural Language Processing, Jupyter Notebooks, MLflow, Databricks, AWS, Spark, Hive, Hadoop',\n",
       " 'Image Data Science, Machine Learning, Computer Vision, R, Python, Matlab, Java, C/C++, Image Analysis, Biomedical Imaging, Data Visualization, Software Development, Bioinformatics, Data Science, Computer Engineering, Scientific Computing, Statistics, Scientific Software, HPC Optimization, Algorithm Development, Image Processing, Deep Learning, Medical Imaging, Biological Imaging, Personnel Management, ImageJ/Fiji, CellProfiler, Cellpose',\n",
       " 'Data Pipelines, Data Infrastructure, Data Science, Distributed Data Systems, Event Driven Architectures, Data Pipeline Tooling, BigQuery, Elasticsearch, Dagster, DBT, Python, SQL, GraphQL, Machine Learning Models, Legal Technology, Medical Records, Unstructured Data',\n",
       " 'Data Analysis, Data Visualization, Tableau, SQL, Python, R, Statistics, Machine Learning, Hypothesis Testing, Regression, Logistic Regression, Random Forest, Stakeholder Management, Communication, Presentation, Automation, Programming, Prototyping, Decision Making, Resource Management, MBA, Program Management, Asian Market Experience, Travel Industry, Ecommerce, Tech, Consulting',\n",
       " \"Project management, Program management, Network infrastructure delivery, Change management, Service delivery, Jira, Confluence, LeanAgile, Scrum, Kanban, SAFe Agile methodologies, Leadership, Execution, Delivery, Commitment, PMP, ITIL, CCNA, Risk management, Decision making, Bachelor's degree, Engineering, Computer science, Business, Finance, Analytical skills, Technical acumen, Partnership, Automation, Process improvements, IT service delivery, Financial services, Insurance, Banking, Investment banking, Nimble, Flexible, Prioritization, Shifting deadlines\",\n",
       " 'Data Integration, Cloud Computing, ETL Processes, Data Modeling, Data Mapping, SQL, PL/SQL, Python, Data Lakes, Cloud Processing, BI Tools, Data Quality, Data Governance, Regulatory Compliance, Project Management, Leadership, Communication, Teamwork, Problem Solving, Analytical Skills, Data Security, Data Architecture, Data Integration Tools, Data Design, Oracle, SQL Server',\n",
       " 'Data Stewardship, Program Evaluation, Data Management, Data Analysis, Statistics, GIS, Environmental Science, Engineering, Computer Science, Natural Science, Spatial Research, Database Querying, Project Management, Communication, Tableau, SQL Developer, MS Access, Python, R, Statistical Analysis Software, Data Visualization, Data Processing, Data Collection, Data Evaluation, Scientific Data Interpretation, Presentation, Physical Requirements, Travel',\n",
       " 'DeFi, NFTs, DAOs, Web 3.0, Direct communication, Candid feedback, Growth strategies, Experimentation, Rigorous decision making, Data science, Product management, Data analysis, Modeling, Communication, Storytelling, Business intelligence, Data visualization, SQL, R, Python, Data models, Experimental design, Business metrics, Systems, Automation, BA / BS degree, Handson experience, Business insights, Internal storytelling, Executive presentations, Analytics tools, Looker, Tableau, Domain experience, Structured data, Unstructured data',\n",
       " 'Machine Learning, Natural Language Processing, Deep Learning, Voice Analysis, Model Training, Model Deployment, Data Processing, Cloud Computing, Software Development, Python, NumPy, Pandas, Numba, Torch, Tensorflow, Jupyter, DataBricks, MLFlow, WandB, Agile Methodologies, Sprint Management, FDA Approvals, Empathetic Leader, Mentor, Strategic Planner, Agile Planner, MVP, Efficient Communication, Medical Knowledge, Dental Knowledge, Vision Knowledge',\n",
       " 'Technical Architecture, Consulting, Data warehousing, Databricks, AWS, Azure, Data Lake, Data classification, Data profiling, MLOPS, Machine Learning (ML), Artificial Intelligence (AI), Banking domain, Card and payment systems, Offshore onsite coordination, Business needs translation',\n",
       " 'Analytics, Analytical Programs, SQL, Data mining, UDS Quality Measures, EHR systems, Medical Terminology, CPRBLS, Data validation, Data layouts, Visual display, KPI’s, Quality Improvement Measures, UDS/HEDIS/ACO, Care Gap Analysis, Reporting, Business Intelligence, Data Accuracy and Integrity, Research Collaboration, Project Management, MOU’s and Contracts, Survey Data Collection, Cultural Diversity, Research Proposals, Clinical Intervention Activities, CHI Culture, Research Dissemination Policies, HIPAA Regulations, Site Visits, Employee Surveys, Data Sharing Agreements',\n",
       " \"Data Pipeline Frameworks, Data APIs, Analytical Applications, Scala, Spark, AngularJS, Product Owners, Bachelor's Degree, Application Development, Big Data Technologies, Cloud Computing (AWS Azure Google Cloud), People Management, Master's Degree, Python, SQL, Java, Distributed Data/Computing Tools (MapReduce Hadoop Hive EMR Kafka Gurobi MySQL), Realtime Data, Streaming Applications, NoSQL Implementation (Mongo Cassandra), Data Warehousing (Redshift Snowflake), UNIX/Linux, Agile Engineering Practices, AWS, Azure, Google Cloud\",\n",
       " \"Data Science, Marketing, Statistics, Experimental design, Web analytics, Data modeling, SQL, Attribution, Mediamix models, A/B testing, Bayesian inference, Machine learning, Python, R, SAS, Big data, Cloud computing, Data visualization, Communication, Problemsolving, Critical thinking, Analytical skills, Industry knowledge, Team player, Experience, Education, Skills, Bachelor's degree, Master's degree, Experience as a Data Scientist, Business and organizational needs\",\n",
       " \"Medical Technologist, Medical Laboratory Technician, Clinical laboratory tests, Chemistry, Hematology, Microbiology, Immunology, Laboratory equipment, Laboratory instruments, Quality control, Quality assurance, Healthcare professionals, Bachelor's degree, Medical Technology, Associate's degree, MLT program, Medical Laboratory Technicians, Salary and benefits, Signon bonus, Fulltime, Travel nursing, Travel allied health, Permanent placement, Healthcare recruiters, Career path\",\n",
       " 'Java, C/C++, Apache Spark, Kafka, Airflow, Kubernetes, Cassandra, Druid, Snowflake, Cloud (AWS), Generative AI, Looker, Tableau, ReactiveJS, Communication, Presentation, Opensource, Data platform, Analytics, Cloud, Distributed systems, Software engineering, AI/ML, Data science, Data analytics, Data processing, Data engineering, Data quality, Data governance, Data security, Data architecture, Data modeling, Data visualization, Data mining, Machine learning, Deep learning, Natural language processing, Computer vision, Speech recognition, Natural language generation, Reinforcement learning, Generative adversarial networks, Data mining, Business intelligence, Data warehousing, Data lakes, Data pipelines, Data integration, Data cleansing, Data preparation, Data modeling, Data visualization, Data storytelling, Data ethics, Data privacy, Data security, Data governance, Data management, Data architecture, Data infrastructure, Data engineering, Data analysis, Data science, Machine learning, Artificial intelligence',\n",
       " 'Artificial Intelligence, Machine Learning, Computer Science, Translational Research, Research, Teaching, Mentoring, Electrical Engineering, Physics',\n",
       " 'Databricks, Data Architect, SQL, Python, Spark, Cloud Computing, AWS, Azure, GCP, Data Engineering, Data Warehousing, Data Pipeline, Data Security, Data Governance, Unit Testing, Integration Testing, DevOps, CI/CD, Microservices, Software Development Life Cycle, Agile, Waterfall, Machine Learning, AI, Tableau, Power BI',\n",
       " 'Data Science, Analytics, Business Intelligence, Machine Learning, Data Mining, Statistical Modeling, Data Extraction, Data Transformation, Data Loading, Data Visualization, Dashboards, SQL, Python, R, SAS, Matlab, AWS QuickSight, Tableau, R Shiny, ETL, Forecasting, Predictive Modeling, Classification, Clustering, Data Pipelines, AI, ML, Sales Metrics, Reporting Tools, Data Structures, A/B Experiments, Multinomial Logistic Regression',\n",
       " 'Solution Architecture, Application Design, Development, Systems Life Cycle Management, Architecture, Cloud Computing, Hybrid Ecosystems, Public Cloud Technologies, Ecommerce Platforms, Mobile Development, Marketing Technology, Integration, Restaurant Technology, API, Microservices, Agile, DevOps, CI/CD, Application Architecture, Infrastructure Architecture, Security Architecture, Performance, Scalability, Reliability, Availability, Cloud Computing, Data Technologies, Business Drivers, Emerging Computing Trends, Deployment Options, Programming Languages, Scripting Languages, .NET, Java, Python, PHP, Ruby, PERL, C++, JavaScript',\n",
       " 'Agile, Machine learning, Distributed microservices, Fullstack systems, Java, Scala, Python, Open Source Relational Database Management System, NoSQL databases, Cloudbased data warehousing, Redshift, Snowflake, Big data technologies, Cloud computing, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Realtime data, Streaming applications, NoSQL databases, Mongo, Cassandra, Data warehousing, UNIX, Linux, Shell scripting',\n",
       " 'Machine Learning Operations (ML Ops), Engineering, Production ML Systems, Complex Workflows, Workflow Orchestration, Monitoring, Visibility, Experimentation, A/B Testing, Data Engineering, ETL Pipelines, Systems Ownership, LLMs, NLP, Reinforcement Learning, Probabilistic Graphs, Deep Learning, Vision, Scrappy, Autonomy, Flexibility, Teamfirst Mentality, Mentoring, Passion for Vision, NYCbased, Inperson Environment, Hiring, Financial Workflows, Product Ownership',\n",
       " 'Social media, Project management, Digital marketing, Marketing strategies, Content writing, Graphic design, Web development, Video editing, Photo editing, Microsoft Office Suite, Adobe Creative Suite, Communications, Collaboration, Teamwork, Interpersonal skills, Problem solving, Analytical skills, Organizational skills, Leadership skills, Budget management, Time management, Strategic thinking, Creativity, Adaptability, Initiative, Attention to detail, Multitasking, Customer service, Public speaking, Presentation skills, Research skills, Writing skills, Editing skills',\n",
       " 'Data Warehousing, Data Modeling, ETL, Data Architecture, SQL, Cloudbased data technology, Data Management, Database Technologies, Data Integration, Business Objects, Data Services, RStudio, Azure',\n",
       " 'Machine Learning, Data Science, Python, Java, Scala, TensorFlow, PyTorch, JAX, Scikitlean, Recommendations & Search, Causal Machine Learning, Interpretable ML, MultiArmed Bandits, Reinforcement Learning, Constrained Optimization, Datadriven research, Experimentation, Written and verbal communication, Selfmotivated, Problemsolving, Team work, Adaptability',\n",
       " 'Data Science, Advanced Analytics, Machine Learning, Statistical Techniques, Python, SQL, Tableau, Power BI, Consumer Lending, Communication Skills, Large Datasets, Data Processing Frameworks, Data Pipelines, Supervised Learning, Unsupervised Learning, Data Wrangling, NoSQL Databases, MongoDB, Git, JSON, XML',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(datascience_skills['job_skills'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dbc6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_skills = scan_for_new_skills(datascience_skills, 2, unique_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "974f7985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98125\n",
      "1985\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_skills)) # We should see an increase in the number of tabulated jobs\n",
    "print(unique_skills['business analyst']) # This number should not have shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9b554",
   "metadata": {},
   "source": [
    "Now make a new column into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "239845ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience_skills['job_skills_tabulated'] = datascience_skills['job_skills'].apply(convert_skill_to_tabulated_form) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db86d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_skills_tabulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-mach...</td>\n",
       "      <td>Machine Learning, Programming, Python, Scala, ...</td>\n",
       "      <td>[372, 256, 380, 2514, 390, 1404, 19171, 2971, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/principal-s...</td>\n",
       "      <td>C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...</td>\n",
       "      <td>[893, 380, 1517, 1516, 46551, 46552, 46553, 46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-etl-...</td>\n",
       "      <td>ETL, Data Integration, Data Transformation, Da...</td>\n",
       "      <td>[226, 106, 1153, 101, 103, 100, 1490, 847, 107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-data...</td>\n",
       "      <td>Data Lakes, Data Bricks, Azure Data Factory Pi...</td>\n",
       "      <td>[6455, 8845, 46568, 514, 380, 103, 1257, 503, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-e...</td>\n",
       "      <td>Java, Scala, Python, RDBMS, NoSQL, Redshift, S...</td>\n",
       "      <td>[390, 2514, 380, 1307, 515, 842, 686, 3941, 46...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_link  \\\n",
       "0  https://www.linkedin.com/jobs/view/senior-mach...   \n",
       "1  https://www.linkedin.com/jobs/view/principal-s...   \n",
       "2  https://www.linkedin.com/jobs/view/senior-etl-...   \n",
       "3  https://www.linkedin.com/jobs/view/senior-data...   \n",
       "4  https://www.linkedin.com/jobs/view/lead-data-e...   \n",
       "\n",
       "                                          job_skills  \\\n",
       "0  Machine Learning, Programming, Python, Scala, ...   \n",
       "1  C++, Python, PyTorch, TensorFlow, MXNet, CUDA,...   \n",
       "2  ETL, Data Integration, Data Transformation, Da...   \n",
       "3  Data Lakes, Data Bricks, Azure Data Factory Pi...   \n",
       "4  Java, Scala, Python, RDBMS, NoSQL, Redshift, S...   \n",
       "\n",
       "                                job_skills_tabulated  \n",
       "0  [372, 256, 380, 2514, 390, 1404, 19171, 2971, ...  \n",
       "1  [893, 380, 1517, 1516, 46551, 46552, 46553, 46...  \n",
       "2  [226, 106, 1153, 101, 103, 100, 1490, 847, 107...  \n",
       "3  [6455, 8845, 46568, 514, 380, 103, 1257, 503, ...  \n",
       "4  [390, 2514, 380, 1307, 515, 842, 686, 3941, 46...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datascience_skills.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7f1c6",
   "metadata": {},
   "source": [
    "## Luke Barousse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d35a2d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61953 entries, 0 to 61952\n",
      "Data columns (total 27 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           61953 non-null  int64  \n",
      " 1   index                61953 non-null  int64  \n",
      " 2   title                61953 non-null  object \n",
      " 3   company_name         61953 non-null  object \n",
      " 4   location             61916 non-null  object \n",
      " 5   via                  61944 non-null  object \n",
      " 6   description          61953 non-null  object \n",
      " 7   extensions           61953 non-null  object \n",
      " 8   job_id               61953 non-null  object \n",
      " 9   thumbnail            38194 non-null  object \n",
      " 10  posted_at            61763 non-null  object \n",
      " 11  schedule_type        61707 non-null  object \n",
      " 12  work_from_home       27980 non-null  object \n",
      " 13  salary               10088 non-null  object \n",
      " 14  search_term          61953 non-null  object \n",
      " 15  date_time            61953 non-null  object \n",
      " 16  search_location      61953 non-null  object \n",
      " 17  commute_time         0 non-null      float64\n",
      " 18  salary_pay           10088 non-null  object \n",
      " 19  salary_rate          10088 non-null  object \n",
      " 20  salary_avg           10088 non-null  float64\n",
      " 21  salary_min           9512 non-null   float64\n",
      " 22  salary_max           9512 non-null   float64\n",
      " 23  salary_hourly        5900 non-null   float64\n",
      " 24  salary_yearly        4069 non-null   float64\n",
      " 25  salary_standardized  10088 non-null  float64\n",
      " 26  description_tokens   61953 non-null  object \n",
      "dtypes: float64(7), int64(2), object(18)\n",
      "memory usage: 12.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>In the intersection of compliance and analytic...</td>\n",
       "      <td>['15 hours ago', '101K–143K a year', 'Work fro...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101K–143K</td>\n",
       "      <td>a year</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>['tableau', 'r', 'python', 'sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ATC</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Job Title: Entry Level Business Analyst / Prod...</td>\n",
       "      <td>['12 hours ago', 'Full-time', 'Health insurance']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aeronautical Data Analyst</td>\n",
       "      <td>Garmin International, Inc.</td>\n",
       "      <td>Olathe, KS</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Overview:\\n\\nWe are seeking a full-time...\\nAe...</td>\n",
       "      <td>['18 hours ago', 'Full-time']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Consumer Goods - Contract to Hire</td>\n",
       "      <td>Upwork</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Upwork</td>\n",
       "      <td>Enthusiastic Data Analyst for processing sales...</td>\n",
       "      <td>['12 hours ago', '15–25 an hour', 'Work from h...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15–25</td>\n",
       "      <td>an hour</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>['powerpoint', 'excel', 'power_bi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst | Workforce Management</td>\n",
       "      <td>Krispy Kreme</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Overview of Position\\n\\nThis position will be ...</td>\n",
       "      <td>['7 hours ago', '90K–110K a year', 'Contractor']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90K–110K</td>\n",
       "      <td>a year</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>['powerpoint', 'excel', 'outlook', 'word']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                             title  \\\n",
       "0           0      0                                      Data Analyst   \n",
       "1           1      1                                      Data Analyst   \n",
       "2           2      2                         Aeronautical Data Analyst   \n",
       "3           3      3  Data Analyst - Consumer Goods - Contract to Hire   \n",
       "4           4      4               Data Analyst | Workforce Management   \n",
       "\n",
       "                 company_name            location           via  \\\n",
       "0                        Meta           Anywhere   via LinkedIn   \n",
       "1                         ATC    United States     via LinkedIn   \n",
       "2  Garmin International, Inc.       Olathe, KS       via Indeed   \n",
       "3                      Upwork           Anywhere     via Upwork   \n",
       "4                Krispy Kreme    United States     via LinkedIn   \n",
       "\n",
       "                                         description  \\\n",
       "0  In the intersection of compliance and analytic...   \n",
       "1  Job Title: Entry Level Business Analyst / Prod...   \n",
       "2  Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
       "3  Enthusiastic Data Analyst for processing sales...   \n",
       "4  Overview of Position\\n\\nThis position will be ...   \n",
       "\n",
       "                                          extensions  \\\n",
       "0  ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
       "1  ['12 hours ago', 'Full-time', 'Health insurance']   \n",
       "2                      ['18 hours ago', 'Full-time']   \n",
       "3  ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
       "4   ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
       "\n",
       "                                              job_id  \\\n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "1  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "2  eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
       "4  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
       "\n",
       "                                           thumbnail  ... commute_time  \\\n",
       "0  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "1  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "2                                                NaN  ...          NaN   \n",
       "3                                                NaN  ...          NaN   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "\n",
       "  salary_pay salary_rate salary_avg salary_min salary_max salary_hourly  \\\n",
       "0  101K–143K      a year   122000.0   101000.0   143000.0           NaN   \n",
       "1        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "2        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "3      15–25     an hour       20.0       15.0       25.0          20.0   \n",
       "4   90K–110K      a year   100000.0    90000.0   110000.0           NaN   \n",
       "\n",
       "   salary_yearly salary_standardized  \\\n",
       "0       122000.0            122000.0   \n",
       "1            NaN                 NaN   \n",
       "2            NaN                 NaN   \n",
       "3            NaN             41600.0   \n",
       "4       100000.0            100000.0   \n",
       "\n",
       "                           description_tokens  \n",
       "0           ['tableau', 'r', 'python', 'sql']  \n",
       "1                                          []  \n",
       "2                                     ['sql']  \n",
       "3         ['powerpoint', 'excel', 'power_bi']  \n",
       "4  ['powerpoint', 'excel', 'outlook', 'word']  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luke_barousse = pd.read_csv('../../datasets/luke_barousse/gsearch_jobs.csv')\n",
    "luke_barousse.info()\n",
    "luke_barousse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071dde4",
   "metadata": {},
   "source": [
    "For this dataset, the skills are under `description_tokens`.This is an attempt to format it into a list datatype already. However, the column is of `object` datatype, meaning its probably a string.\n",
    "\n",
    "So we will need to format it. Strip the \"[]\" rectangular brackets && the single quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fbbecd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['tableau', 'r', 'python', 'sql']\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luke_barousse.at[0,'description_tokens']\n",
    "\n",
    "# Just from this example, we verify that the datatype is of string format. \n",
    "# For us to parse through with our function `scan_for_new_skills()`, we \n",
    "# must format it into a list of skills naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea53784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_skills_column(x):\n",
    "    return x.strip(\"[]\").replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7fa39",
   "metadata": {},
   "source": [
    "We are going to make a 'permanent' change to the dataset by applying this change throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d35ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "luke_barousse['description_tokens'] = luke_barousse['description_tokens'].apply(format_skills_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2eff7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>In the intersection of compliance and analytic...</td>\n",
       "      <td>['15 hours ago', '101K–143K a year', 'Work fro...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101K–143K</td>\n",
       "      <td>a year</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>tableau, r, python, sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ATC</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Job Title: Entry Level Business Analyst / Prod...</td>\n",
       "      <td>['12 hours ago', 'Full-time', 'Health insurance']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aeronautical Data Analyst</td>\n",
       "      <td>Garmin International, Inc.</td>\n",
       "      <td>Olathe, KS</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Overview:\\n\\nWe are seeking a full-time...\\nAe...</td>\n",
       "      <td>['18 hours ago', 'Full-time']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Consumer Goods - Contract to Hire</td>\n",
       "      <td>Upwork</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Upwork</td>\n",
       "      <td>Enthusiastic Data Analyst for processing sales...</td>\n",
       "      <td>['12 hours ago', '15–25 an hour', 'Work from h...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15–25</td>\n",
       "      <td>an hour</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>powerpoint, excel, power_bi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst | Workforce Management</td>\n",
       "      <td>Krispy Kreme</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Overview of Position\\n\\nThis position will be ...</td>\n",
       "      <td>['7 hours ago', '90K–110K a year', 'Contractor']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90K–110K</td>\n",
       "      <td>a year</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>powerpoint, excel, outlook, word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                             title  \\\n",
       "0           0      0                                      Data Analyst   \n",
       "1           1      1                                      Data Analyst   \n",
       "2           2      2                         Aeronautical Data Analyst   \n",
       "3           3      3  Data Analyst - Consumer Goods - Contract to Hire   \n",
       "4           4      4               Data Analyst | Workforce Management   \n",
       "\n",
       "                 company_name            location           via  \\\n",
       "0                        Meta           Anywhere   via LinkedIn   \n",
       "1                         ATC    United States     via LinkedIn   \n",
       "2  Garmin International, Inc.       Olathe, KS       via Indeed   \n",
       "3                      Upwork           Anywhere     via Upwork   \n",
       "4                Krispy Kreme    United States     via LinkedIn   \n",
       "\n",
       "                                         description  \\\n",
       "0  In the intersection of compliance and analytic...   \n",
       "1  Job Title: Entry Level Business Analyst / Prod...   \n",
       "2  Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
       "3  Enthusiastic Data Analyst for processing sales...   \n",
       "4  Overview of Position\\n\\nThis position will be ...   \n",
       "\n",
       "                                          extensions  \\\n",
       "0  ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
       "1  ['12 hours ago', 'Full-time', 'Health insurance']   \n",
       "2                      ['18 hours ago', 'Full-time']   \n",
       "3  ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
       "4   ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
       "\n",
       "                                              job_id  \\\n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "1  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "2  eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
       "4  eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
       "\n",
       "                                           thumbnail  ... commute_time  \\\n",
       "0  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "1  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "2                                                NaN  ...          NaN   \n",
       "3                                                NaN  ...          NaN   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
       "\n",
       "  salary_pay salary_rate salary_avg salary_min salary_max salary_hourly  \\\n",
       "0  101K–143K      a year   122000.0   101000.0   143000.0           NaN   \n",
       "1        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "2        NaN         NaN        NaN        NaN        NaN           NaN   \n",
       "3      15–25     an hour       20.0       15.0       25.0          20.0   \n",
       "4   90K–110K      a year   100000.0    90000.0   110000.0           NaN   \n",
       "\n",
       "   salary_yearly salary_standardized                description_tokens  \n",
       "0       122000.0            122000.0           tableau, r, python, sql  \n",
       "1            NaN                 NaN                                    \n",
       "2            NaN                 NaN                               sql  \n",
       "3            NaN             41600.0       powerpoint, excel, power_bi  \n",
       "4       100000.0            100000.0  powerpoint, excel, outlook, word  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luke_barousse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a584398",
   "metadata": {},
   "source": [
    "We need to verify datatypes in the blank cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7d08a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(luke_barousse.at[1,'description_tokens'])\n",
    "print(len(luke_barousse.at[1,'description_tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38b09454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to tabulate the skills\n",
    "\n",
    "unique_skills = scan_for_new_skills(luke_barousse, 27, unique_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e31b0",
   "metadata": {},
   "source": [
    "Because blank values are interpretted as an empty string, we will have to modify `scan_for_new_skills()` function to handle that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7541c2",
   "metadata": {},
   "source": [
    "Let's just verify that \n",
    "1. the function worked on this dataset \n",
    "2. Existing records weren't altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dfc7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98139\n",
      "1985\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_skills))\n",
    "print(unique_skills['business analyst'])\n",
    "# Ensure that blank values weren't recorded!\n",
    "# print(unique_skills['']) # Should throw ERROR!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0621930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n",
      "Failed on: \n"
     ]
    }
   ],
   "source": [
    "luke_barousse['description_tokens_tabulated'] = luke_barousse['description_tokens'].apply(convert_skill_to_tabulated_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fe6f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>description_tokens</th>\n",
       "      <th>description_tokens_tabulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>In the intersection of compliance and analytic...</td>\n",
       "      <td>['15 hours ago', '101K–143K a year', 'Work fro...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>101K–143K</td>\n",
       "      <td>a year</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>tableau, r, python, sql</td>\n",
       "      <td>[474, 379, 380, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ATC</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Job Title: Entry Level Business Analyst / Prod...</td>\n",
       "      <td>['12 hours ago', 'Full-time', 'Health insurance']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aeronautical Data Analyst</td>\n",
       "      <td>Garmin International, Inc.</td>\n",
       "      <td>Olathe, KS</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Overview:\\n\\nWe are seeking a full-time...\\nAe...</td>\n",
       "      <td>['18 hours ago', 'Full-time']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sql</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Consumer Goods - Contract to Hire</td>\n",
       "      <td>Upwork</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Upwork</td>\n",
       "      <td>Enthusiastic Data Analyst for processing sales...</td>\n",
       "      <td>['12 hours ago', '15–25 an hour', 'Work from h...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15–25</td>\n",
       "      <td>an hour</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>powerpoint, excel, power_bi</td>\n",
       "      <td>[18, 16, 98126]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst | Workforce Management</td>\n",
       "      <td>Krispy Kreme</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Overview of Position\\n\\nThis position will be ...</td>\n",
       "      <td>['7 hours ago', '90K–110K a year', 'Contractor']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>90K–110K</td>\n",
       "      <td>a year</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>powerpoint, excel, outlook, word</td>\n",
       "      <td>[18, 16, 1664, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61948</th>\n",
       "      <td>61948</td>\n",
       "      <td>955</td>\n",
       "      <td>Marketing Data &amp; BI Analyst II</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Houstonia, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '76,798–130,764 a year', 'Ful...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJNYXJrZXRpbmcgRGF0YSBcdTAwMj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>76798–130764</td>\n",
       "      <td>a year</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>76798.0</td>\n",
       "      <td>130764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>power_bi, tableau, excel, snowflake, sql, r, p...</td>\n",
       "      <td>[98126, 474, 16, 686, 98, 379, 380]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61949</th>\n",
       "      <td>61949</td>\n",
       "      <td>956</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Marshfield, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61950</th>\n",
       "      <td>61950</td>\n",
       "      <td>957</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>High Point, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61951</th>\n",
       "      <td>61951</td>\n",
       "      <td>958</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Calhoun, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61952</th>\n",
       "      <td>61952</td>\n",
       "      <td>959</td>\n",
       "      <td>Institutional Credit Management - Lending Data...</td>\n",
       "      <td>Citi</td>\n",
       "      <td>United States</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>The Institutional Credit Management (ICM) grou...</td>\n",
       "      <td>['24 hours ago', '105,850–158,780 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJJbnN0aXR1dGlvbmFsIENyZWRpdC...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>105850–158780</td>\n",
       "      <td>a year</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>105850.0</td>\n",
       "      <td>158780.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>tableau, cognos</td>\n",
       "      <td>[474, 1319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61953 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  index                                              title  \\\n",
       "0               0      0                                       Data Analyst   \n",
       "1               1      1                                       Data Analyst   \n",
       "2               2      2                          Aeronautical Data Analyst   \n",
       "3               3      3   Data Analyst - Consumer Goods - Contract to Hire   \n",
       "4               4      4                Data Analyst | Workforce Management   \n",
       "...           ...    ...                                                ...   \n",
       "61948       61948    955                     Marketing Data & BI Analyst II   \n",
       "61949       61949    956                                  Lead-Data Analyst   \n",
       "61950       61950    957                                  Lead-Data Analyst   \n",
       "61951       61951    958                                  Lead-Data Analyst   \n",
       "61952       61952    959  Institutional Credit Management - Lending Data...   \n",
       "\n",
       "                     company_name            location                    via  \\\n",
       "0                            Meta           Anywhere            via LinkedIn   \n",
       "1                             ATC    United States              via LinkedIn   \n",
       "2      Garmin International, Inc.       Olathe, KS                via Indeed   \n",
       "3                          Upwork           Anywhere              via Upwork   \n",
       "4                    Krispy Kreme    United States              via LinkedIn   \n",
       "...                           ...                 ...                    ...   \n",
       "61948                EDWARD JONES       Houstonia, MO  via My ArkLaMiss Jobs   \n",
       "61949                EDWARD JONES      Marshfield, MO  via My ArkLaMiss Jobs   \n",
       "61950                EDWARD JONES      High Point, MO  via My ArkLaMiss Jobs   \n",
       "61951                EDWARD JONES         Calhoun, MO  via My ArkLaMiss Jobs   \n",
       "61952                        Citi       United States  via My ArkLaMiss Jobs   \n",
       "\n",
       "                                             description  \\\n",
       "0      In the intersection of compliance and analytic...   \n",
       "1      Job Title: Entry Level Business Analyst / Prod...   \n",
       "2      Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
       "3      Enthusiastic Data Analyst for processing sales...   \n",
       "4      Overview of Position\\n\\nThis position will be ...   \n",
       "...                                                  ...   \n",
       "61948  At Edward Jones, we help clients achieve their...   \n",
       "61949  At Edward Jones, we help clients achieve their...   \n",
       "61950  At Edward Jones, we help clients achieve their...   \n",
       "61951  At Edward Jones, we help clients achieve their...   \n",
       "61952  The Institutional Credit Management (ICM) grou...   \n",
       "\n",
       "                                              extensions  \\\n",
       "0      ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
       "1      ['12 hours ago', 'Full-time', 'Health insurance']   \n",
       "2                          ['18 hours ago', 'Full-time']   \n",
       "3      ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
       "4       ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
       "...                                                  ...   \n",
       "61948  ['23 hours ago', '76,798–130,764 a year', 'Ful...   \n",
       "61949  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61950  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61951  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61952  ['24 hours ago', '105,850–158,780 a year', 'Fu...   \n",
       "\n",
       "                                                  job_id  \\\n",
       "0      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "1      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "2      eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
       "3      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
       "4      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
       "...                                                  ...   \n",
       "61948  eyJqb2JfdGl0bGUiOiJNYXJrZXRpbmcgRGF0YSBcdTAwMj...   \n",
       "61949  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61950  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61951  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61952  eyJqb2JfdGl0bGUiOiJJbnN0aXR1dGlvbmFsIENyZWRpdC...   \n",
       "\n",
       "                                               thumbnail  ...     salary_pay  \\\n",
       "0      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...      101K–143K   \n",
       "1      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...            NaN   \n",
       "2                                                    NaN  ...            NaN   \n",
       "3                                                    NaN  ...          15–25   \n",
       "4      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...       90K–110K   \n",
       "...                                                  ...  ...            ...   \n",
       "61948                                                NaN  ...   76798–130764   \n",
       "61949                                                NaN  ...  106916–182047   \n",
       "61950                                                NaN  ...  106916–182047   \n",
       "61951                                                NaN  ...  106916–182047   \n",
       "61952  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...  105850–158780   \n",
       "\n",
       "      salary_rate salary_avg salary_min salary_max salary_hourly  \\\n",
       "0          a year   122000.0   101000.0   143000.0           NaN   \n",
       "1             NaN        NaN        NaN        NaN           NaN   \n",
       "2             NaN        NaN        NaN        NaN           NaN   \n",
       "3         an hour       20.0       15.0       25.0          20.0   \n",
       "4          a year   100000.0    90000.0   110000.0           NaN   \n",
       "...           ...        ...        ...        ...           ...   \n",
       "61948      a year   103781.0    76798.0   130764.0           NaN   \n",
       "61949      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61950      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61951      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61952      a year   132315.0   105850.0   158780.0           NaN   \n",
       "\n",
       "      salary_yearly  salary_standardized  \\\n",
       "0          122000.0             122000.0   \n",
       "1               NaN                  NaN   \n",
       "2               NaN                  NaN   \n",
       "3               NaN              41600.0   \n",
       "4          100000.0             100000.0   \n",
       "...             ...                  ...   \n",
       "61948      103781.0             103781.0   \n",
       "61949      144481.5             144481.5   \n",
       "61950      144481.5             144481.5   \n",
       "61951      144481.5             144481.5   \n",
       "61952      132315.0             132315.0   \n",
       "\n",
       "                                      description_tokens  \\\n",
       "0                                tableau, r, python, sql   \n",
       "1                                                          \n",
       "2                                                    sql   \n",
       "3                            powerpoint, excel, power_bi   \n",
       "4                       powerpoint, excel, outlook, word   \n",
       "...                                                  ...   \n",
       "61948  power_bi, tableau, excel, snowflake, sql, r, p...   \n",
       "61949                                                      \n",
       "61950                                                      \n",
       "61951                                                      \n",
       "61952                                    tableau, cognos   \n",
       "\n",
       "              description_tokens_tabulated  \n",
       "0                      [474, 379, 380, 98]  \n",
       "1                                     None  \n",
       "2                                       98  \n",
       "3                          [18, 16, 98126]  \n",
       "4                       [18, 16, 1664, 17]  \n",
       "...                                    ...  \n",
       "61948  [98126, 474, 16, 686, 98, 379, 380]  \n",
       "61949                                 None  \n",
       "61950                                 None  \n",
       "61951                                 None  \n",
       "61952                          [474, 1319]  \n",
       "\n",
       "[61953 rows x 28 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luke_barousse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f3974",
   "metadata": {},
   "source": [
    "We should also rename this `description_tokens` into `job_skills`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d507922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>...</th>\n",
       "      <th>salary_pay</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_hourly</th>\n",
       "      <th>salary_yearly</th>\n",
       "      <th>salary_standardized</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_skills_tabulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>In the intersection of compliance and analytic...</td>\n",
       "      <td>['15 hours ago', '101K–143K a year', 'Work fro...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>101K–143K</td>\n",
       "      <td>a year</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>143000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>122000.0</td>\n",
       "      <td>tableau, r, python, sql</td>\n",
       "      <td>[474, 379, 380, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ATC</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Job Title: Entry Level Business Analyst / Prod...</td>\n",
       "      <td>['12 hours ago', 'Full-time', 'Health insurance']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aeronautical Data Analyst</td>\n",
       "      <td>Garmin International, Inc.</td>\n",
       "      <td>Olathe, KS</td>\n",
       "      <td>via Indeed</td>\n",
       "      <td>Overview:\\n\\nWe are seeking a full-time...\\nAe...</td>\n",
       "      <td>['18 hours ago', 'Full-time']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sql</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst - Consumer Goods - Contract to Hire</td>\n",
       "      <td>Upwork</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via Upwork</td>\n",
       "      <td>Enthusiastic Data Analyst for processing sales...</td>\n",
       "      <td>['12 hours ago', '15–25 an hour', 'Work from h...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15–25</td>\n",
       "      <td>an hour</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41600.0</td>\n",
       "      <td>powerpoint, excel, power_bi</td>\n",
       "      <td>[18, 16, 98126]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst | Workforce Management</td>\n",
       "      <td>Krispy Kreme</td>\n",
       "      <td>United States</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Overview of Position\\n\\nThis position will be ...</td>\n",
       "      <td>['7 hours ago', '90K–110K a year', 'Contractor']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>90K–110K</td>\n",
       "      <td>a year</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>powerpoint, excel, outlook, word</td>\n",
       "      <td>[18, 16, 1664, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61948</th>\n",
       "      <td>61948</td>\n",
       "      <td>955</td>\n",
       "      <td>Marketing Data &amp; BI Analyst II</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Houstonia, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '76,798–130,764 a year', 'Ful...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJNYXJrZXRpbmcgRGF0YSBcdTAwMj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>76798–130764</td>\n",
       "      <td>a year</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>76798.0</td>\n",
       "      <td>130764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>103781.0</td>\n",
       "      <td>power_bi, tableau, excel, snowflake, sql, r, p...</td>\n",
       "      <td>[98126, 474, 16, 686, 98, 379, 380]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61949</th>\n",
       "      <td>61949</td>\n",
       "      <td>956</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Marshfield, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61950</th>\n",
       "      <td>61950</td>\n",
       "      <td>957</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>High Point, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61951</th>\n",
       "      <td>61951</td>\n",
       "      <td>958</td>\n",
       "      <td>Lead-Data Analyst</td>\n",
       "      <td>EDWARD JONES</td>\n",
       "      <td>Calhoun, MO</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>At Edward Jones, we help clients achieve their...</td>\n",
       "      <td>['23 hours ago', '106,916–182,047 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106916–182047</td>\n",
       "      <td>a year</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>106916.0</td>\n",
       "      <td>182047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144481.5</td>\n",
       "      <td>144481.5</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61952</th>\n",
       "      <td>61952</td>\n",
       "      <td>959</td>\n",
       "      <td>Institutional Credit Management - Lending Data...</td>\n",
       "      <td>Citi</td>\n",
       "      <td>United States</td>\n",
       "      <td>via My ArkLaMiss Jobs</td>\n",
       "      <td>The Institutional Credit Management (ICM) grou...</td>\n",
       "      <td>['24 hours ago', '105,850–158,780 a year', 'Fu...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJJbnN0aXR1dGlvbmFsIENyZWRpdC...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>...</td>\n",
       "      <td>105850–158780</td>\n",
       "      <td>a year</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>105850.0</td>\n",
       "      <td>158780.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>132315.0</td>\n",
       "      <td>tableau, cognos</td>\n",
       "      <td>[474, 1319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61953 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  index                                              title  \\\n",
       "0               0      0                                       Data Analyst   \n",
       "1               1      1                                       Data Analyst   \n",
       "2               2      2                          Aeronautical Data Analyst   \n",
       "3               3      3   Data Analyst - Consumer Goods - Contract to Hire   \n",
       "4               4      4                Data Analyst | Workforce Management   \n",
       "...           ...    ...                                                ...   \n",
       "61948       61948    955                     Marketing Data & BI Analyst II   \n",
       "61949       61949    956                                  Lead-Data Analyst   \n",
       "61950       61950    957                                  Lead-Data Analyst   \n",
       "61951       61951    958                                  Lead-Data Analyst   \n",
       "61952       61952    959  Institutional Credit Management - Lending Data...   \n",
       "\n",
       "                     company_name            location                    via  \\\n",
       "0                            Meta           Anywhere            via LinkedIn   \n",
       "1                             ATC    United States              via LinkedIn   \n",
       "2      Garmin International, Inc.       Olathe, KS                via Indeed   \n",
       "3                          Upwork           Anywhere              via Upwork   \n",
       "4                    Krispy Kreme    United States              via LinkedIn   \n",
       "...                           ...                 ...                    ...   \n",
       "61948                EDWARD JONES       Houstonia, MO  via My ArkLaMiss Jobs   \n",
       "61949                EDWARD JONES      Marshfield, MO  via My ArkLaMiss Jobs   \n",
       "61950                EDWARD JONES      High Point, MO  via My ArkLaMiss Jobs   \n",
       "61951                EDWARD JONES         Calhoun, MO  via My ArkLaMiss Jobs   \n",
       "61952                        Citi       United States  via My ArkLaMiss Jobs   \n",
       "\n",
       "                                             description  \\\n",
       "0      In the intersection of compliance and analytic...   \n",
       "1      Job Title: Entry Level Business Analyst / Prod...   \n",
       "2      Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
       "3      Enthusiastic Data Analyst for processing sales...   \n",
       "4      Overview of Position\\n\\nThis position will be ...   \n",
       "...                                                  ...   \n",
       "61948  At Edward Jones, we help clients achieve their...   \n",
       "61949  At Edward Jones, we help clients achieve their...   \n",
       "61950  At Edward Jones, we help clients achieve their...   \n",
       "61951  At Edward Jones, we help clients achieve their...   \n",
       "61952  The Institutional Credit Management (ICM) grou...   \n",
       "\n",
       "                                              extensions  \\\n",
       "0      ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
       "1      ['12 hours ago', 'Full-time', 'Health insurance']   \n",
       "2                          ['18 hours ago', 'Full-time']   \n",
       "3      ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
       "4       ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
       "...                                                  ...   \n",
       "61948  ['23 hours ago', '76,798–130,764 a year', 'Ful...   \n",
       "61949  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61950  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61951  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
       "61952  ['24 hours ago', '105,850–158,780 a year', 'Fu...   \n",
       "\n",
       "                                                  job_id  \\\n",
       "0      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "1      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
       "2      eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
       "3      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
       "4      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
       "...                                                  ...   \n",
       "61948  eyJqb2JfdGl0bGUiOiJNYXJrZXRpbmcgRGF0YSBcdTAwMj...   \n",
       "61949  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61950  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61951  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
       "61952  eyJqb2JfdGl0bGUiOiJJbnN0aXR1dGlvbmFsIENyZWRpdC...   \n",
       "\n",
       "                                               thumbnail  ...     salary_pay  \\\n",
       "0      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...      101K–143K   \n",
       "1      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...            NaN   \n",
       "2                                                    NaN  ...            NaN   \n",
       "3                                                    NaN  ...          15–25   \n",
       "4      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...       90K–110K   \n",
       "...                                                  ...  ...            ...   \n",
       "61948                                                NaN  ...   76798–130764   \n",
       "61949                                                NaN  ...  106916–182047   \n",
       "61950                                                NaN  ...  106916–182047   \n",
       "61951                                                NaN  ...  106916–182047   \n",
       "61952  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...  105850–158780   \n",
       "\n",
       "      salary_rate salary_avg salary_min salary_max salary_hourly  \\\n",
       "0          a year   122000.0   101000.0   143000.0           NaN   \n",
       "1             NaN        NaN        NaN        NaN           NaN   \n",
       "2             NaN        NaN        NaN        NaN           NaN   \n",
       "3         an hour       20.0       15.0       25.0          20.0   \n",
       "4          a year   100000.0    90000.0   110000.0           NaN   \n",
       "...           ...        ...        ...        ...           ...   \n",
       "61948      a year   103781.0    76798.0   130764.0           NaN   \n",
       "61949      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61950      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61951      a year   144481.5   106916.0   182047.0           NaN   \n",
       "61952      a year   132315.0   105850.0   158780.0           NaN   \n",
       "\n",
       "      salary_yearly  salary_standardized  \\\n",
       "0          122000.0             122000.0   \n",
       "1               NaN                  NaN   \n",
       "2               NaN                  NaN   \n",
       "3               NaN              41600.0   \n",
       "4          100000.0             100000.0   \n",
       "...             ...                  ...   \n",
       "61948      103781.0             103781.0   \n",
       "61949      144481.5             144481.5   \n",
       "61950      144481.5             144481.5   \n",
       "61951      144481.5             144481.5   \n",
       "61952      132315.0             132315.0   \n",
       "\n",
       "                                              job_skills  \\\n",
       "0                                tableau, r, python, sql   \n",
       "1                                                          \n",
       "2                                                    sql   \n",
       "3                            powerpoint, excel, power_bi   \n",
       "4                       powerpoint, excel, outlook, word   \n",
       "...                                                  ...   \n",
       "61948  power_bi, tableau, excel, snowflake, sql, r, p...   \n",
       "61949                                                      \n",
       "61950                                                      \n",
       "61951                                                      \n",
       "61952                                    tableau, cognos   \n",
       "\n",
       "                      job_skills_tabulated  \n",
       "0                      [474, 379, 380, 98]  \n",
       "1                                     None  \n",
       "2                                       98  \n",
       "3                          [18, 16, 98126]  \n",
       "4                       [18, 16, 1664, 17]  \n",
       "...                                    ...  \n",
       "61948  [98126, 474, 16, 686, 98, 379, 380]  \n",
       "61949                                 None  \n",
       "61950                                 None  \n",
       "61951                                 None  \n",
       "61952                          [474, 1319]  \n",
       "\n",
       "[61953 rows x 28 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luke_barousse.rename({'description_tokens' : 'job_skills', 'description_tokens_tabulated' : 'job_skills_tabulated'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008671b",
   "metadata": {},
   "source": [
    "Assume everything is posted at 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d9bb8",
   "metadata": {},
   "source": [
    "## Ending: Saving Tabulated Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba994ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save_tabulated_jobs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
